***REMOVED***
    from functools import lru_cache

except ImportError:
    # backport of Python's 3.3 lru_cache, written by Raymond Hettinger and
    # licensed under MIT license, from:
    # <http://code.activestate.com/recipes/578078-py26-and-py30-backport-of-python-33s-lru-cache/>
    # Should be removed when Django only supports Python 3.2 and above.

    from collections import namedtuple
    from functools import update_wrapper
    from threading import RLock

    _CacheInfo = namedtuple("CacheInfo", ["hits", "misses", "maxsize", "currsize"***REMOVED******REMOVED***

    class _HashedSeq(list***REMOVED***:
        __slots__ = 'hashvalue'

        def __init__(self, tup, hash=hash***REMOVED***:
            self[:***REMOVED*** = tup
            self.hashvalue = hash(tup***REMOVED***

        def __hash__(self***REMOVED***:
            return self.hashvalue

    def _make_key(args, kwds, typed,
                 kwd_mark = (object(***REMOVED***,***REMOVED***,
                 fasttypes = {int, str, frozenset, type(None***REMOVED******REMOVED***,
                 sorted=sorted, tuple=tuple, type=type, len=len***REMOVED***:
        'Make a cache key from optionally typed positional and keyword arguments'
        key = args
        if kwds:
            sorted_items = sorted(kwds.items(***REMOVED******REMOVED***
            key += kwd_mark
            for item in sorted_items:
                key += item
        if typed:
            key += tuple(type(v***REMOVED*** for v in args***REMOVED***
            if kwds:
                key += tuple(type(v***REMOVED*** for k, v in sorted_items***REMOVED***
        elif len(key***REMOVED*** == 1 and type(key[0***REMOVED******REMOVED*** in fasttypes:
            return key[0***REMOVED***
        return _HashedSeq(key***REMOVED***

    def lru_cache(maxsize=100, typed=False***REMOVED***:
        ***REMOVED***Least-recently-used cache decorator.

        If *maxsize* is set to None, the LRU features are disabled and the cache
        can grow without bound.

        If *typed* is True, arguments of different types will be cached separately.
        For example, f(3.0***REMOVED*** and f(3***REMOVED*** will be treated as distinct calls with
        distinct results.

        Arguments to the cached function must be hashable.

        View the cache statistics named tuple (hits, misses, maxsize, currsize***REMOVED*** with
        f.cache_info(***REMOVED***.  Clear the cache and statistics with f.cache_clear(***REMOVED***.
        Access the underlying function with f.__wrapped__.

        See:  https://en.wikipedia.org/wiki/Cache_algorithms#Least_Recently_Used
        ***REMOVED***

        # Users should only access the lru_cache through its public API:
        #       cache_info, cache_clear, and f.__wrapped__
        # The internals of the lru_cache are encapsulated for thread safety and
        # to allow the implementation to change (including a possible C version***REMOVED***.

        def decorating_function(user_function***REMOVED***:

            cache = dict(***REMOVED***
            stats = [0, 0***REMOVED***                  # make statistics updateable non-locally
            HITS, MISSES = 0, 1             # names for the stats fields
            make_key = _make_key
            cache_get = cache.get           # bound method to lookup key or return None
            _len = len                      # localize the global len(***REMOVED*** function
            lock = RLock(***REMOVED***                  # because linkedlist updates aren't threadsafe
            root = [***REMOVED***                       # root of the circular doubly linked list
            root[:***REMOVED*** = [root, root, None, None***REMOVED***      # initialize by pointing to self
            nonlocal_root = [root***REMOVED***                  # make updateable non-locally
            PREV, NEXT, KEY, RESULT = 0, 1, 2, 3    # names for the link fields

            if maxsize == 0:

                def wrapper(*args, **kwds***REMOVED***:
                    # no caching, just do a statistics update after a successful call
                    result = user_function(*args, **kwds***REMOVED***
                    stats[MISSES***REMOVED*** += 1
                    return result

            elif maxsize is None:

                def wrapper(*args, **kwds***REMOVED***:
                    # simple caching without ordering or size limit
                    key = make_key(args, kwds, typed***REMOVED***
                    result = cache_get(key, root***REMOVED***   # root used here as a unique not-found sentinel
                    if result is not root:
                        stats[HITS***REMOVED*** += 1
                        return result
                    result = user_function(*args, **kwds***REMOVED***
                    cache[key***REMOVED*** = result
                    stats[MISSES***REMOVED*** += 1
                    return result

            else:

                def wrapper(*args, **kwds***REMOVED***:
                    # size limited caching that tracks accesses by recency
                    key = make_key(args, kwds, typed***REMOVED*** if kwds or typed else args
                    with lock:
                        link = cache_get(key***REMOVED***
                        if link is not None:
                            # record recent use of the key by moving it to the front of the list
                            root, = nonlocal_root
                            link_prev, link_next, key, result = link
                            link_prev[NEXT***REMOVED*** = link_next
                            link_next[PREV***REMOVED*** = link_prev
                            last = root[PREV***REMOVED***
                            last[NEXT***REMOVED*** = root[PREV***REMOVED*** = link
                            link[PREV***REMOVED*** = last
                            link[NEXT***REMOVED*** = root
                            stats[HITS***REMOVED*** += 1
                            return result
                    result = user_function(*args, **kwds***REMOVED***
                    with lock:
                        root, = nonlocal_root
                        if key in cache:
                            # getting here means that this same key was added to the
                            # cache while the lock was released.  since the link
                            # update is already done, we need only return the
                            # computed result and update the count of misses.
                            pass
                        elif _len(cache***REMOVED*** >= maxsize:
                            # use the old root to store the new key and result
                            oldroot = root
                            oldroot[KEY***REMOVED*** = key
                            oldroot[RESULT***REMOVED*** = result
                            # empty the oldest link and make it the new root
                            root = nonlocal_root[0***REMOVED*** = oldroot[NEXT***REMOVED***
                            oldkey = root[KEY***REMOVED***
                            oldvalue = root[RESULT***REMOVED***
                            root[KEY***REMOVED*** = root[RESULT***REMOVED*** = None
                            # now update the cache dictionary for the new links
                            del cache[oldkey***REMOVED***
                            cache[key***REMOVED*** = oldroot
                        else:
                            # put result in a new link at the front of the list
                            last = root[PREV***REMOVED***
                            link = [last, root, key, result***REMOVED***
                            last[NEXT***REMOVED*** = root[PREV***REMOVED*** = cache[key***REMOVED*** = link
                        stats[MISSES***REMOVED*** += 1
                    return result

            def cache_info(***REMOVED***:
                ***REMOVED***Report cache statistics***REMOVED***
                with lock:
                    return _CacheInfo(stats[HITS***REMOVED***, stats[MISSES***REMOVED***, maxsize, len(cache***REMOVED******REMOVED***

            def cache_clear(***REMOVED***:
                ***REMOVED***Clear the cache and cache statistics***REMOVED***
                with lock:
                    cache.clear(***REMOVED***
                    root = nonlocal_root[0***REMOVED***
                    root[:***REMOVED*** = [root, root, None, None***REMOVED***
                    stats[:***REMOVED*** = [0, 0***REMOVED***

            wrapper.__wrapped__ = user_function
            wrapper.cache_info = cache_info
            wrapper.cache_clear = cache_clear
            return update_wrapper(wrapper, user_function***REMOVED***

        return decorating_function
