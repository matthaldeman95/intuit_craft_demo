from __future__ import unicode_literals

import hashlib
import json
***REMOVED***
import posixpath
import re
from collections import OrderedDict

from django.conf import settings
from django.contrib.staticfiles.utils import check_settings, matches_patterns
from django.core.cache import (
    InvalidCacheBackendError, cache as default_cache, caches,
***REMOVED***
from django.core.exceptions import ImproperlyConfigured
from django.core.files.base import ContentFile
from django.core.files.storage import FileSystemStorage, get_storage_class
from django.utils.encoding import force_bytes, force_text
from django.utils.functional import LazyObject
from django.utils.six import iteritems
from django.utils.six.moves.urllib.parse import (
    unquote, urldefrag, urlsplit, urlunsplit,
***REMOVED***


class StaticFilesStorage(FileSystemStorage***REMOVED***:
    ***REMOVED***
    Standard file system storage for static files.

    The defaults for ``location`` and ``base_url`` are
    ``STATIC_ROOT`` and ``STATIC_URL``.
    ***REMOVED***
    def __init__(self, location=None, base_url=None, *args, **kwargs***REMOVED***:
        if location is None:
            location = settings.STATIC_ROOT
        if base_url is None:
            base_url = settings.STATIC_URL
        check_settings(base_url***REMOVED***
        super(StaticFilesStorage, self***REMOVED***.__init__(location, base_url,
                                                 *args, **kwargs***REMOVED***
        # FileSystemStorage fallbacks to MEDIA_ROOT when location
        # is empty, so we restore the empty value.
        if not location:
            self.base_location = None
            self.location = None

    def path(self, name***REMOVED***:
        if not self.location:
            raise ImproperlyConfigured("You're using the staticfiles app "
                                       "without having set the STATIC_ROOT "
                                       "setting to a filesystem path."***REMOVED***
        return super(StaticFilesStorage, self***REMOVED***.path(name***REMOVED***


class HashedFilesMixin(object***REMOVED***:
    default_template = ***REMOVED***url("%s"***REMOVED******REMOVED***
    patterns = (
        ("*.css", (
            r***REMOVED***(url\(['"***REMOVED***{0,1***REMOVED***\s*(.*?***REMOVED***["'***REMOVED***{0,1***REMOVED***\***REMOVED******REMOVED******REMOVED***,
            (r***REMOVED***(@import\s*["'***REMOVED***\s*(.*?***REMOVED***["'***REMOVED******REMOVED******REMOVED***, ***REMOVED***@import url("%s"***REMOVED******REMOVED******REMOVED***,
        ***REMOVED******REMOVED***,
    ***REMOVED***

    def __init__(self, *args, **kwargs***REMOVED***:
        super(HashedFilesMixin, self***REMOVED***.__init__(*args, **kwargs***REMOVED***
        self._patterns = OrderedDict(***REMOVED***
        self.hashed_files = {***REMOVED***
        for extension, patterns in self.patterns:
            for pattern in patterns:
                if isinstance(pattern, (tuple, list***REMOVED******REMOVED***:
                    pattern, template = pattern
                else:
                    template = self.default_template
                compiled = re.compile(pattern, re.IGNORECASE***REMOVED***
                self._patterns.setdefault(extension, [***REMOVED******REMOVED***.append((compiled, template***REMOVED******REMOVED***

    def file_hash(self, name, content=None***REMOVED***:
        ***REMOVED***
        Return a hash of the file with the given name and optional content.
        ***REMOVED***
        if content is None:
            return None
        md5 = hashlib.md5(***REMOVED***
        for chunk in content.chunks(***REMOVED***:
            md5.update(chunk***REMOVED***
        return md5.hexdigest(***REMOVED***[:12***REMOVED***

    def hashed_name(self, name, content=None***REMOVED***:
        parsed_name = urlsplit(unquote(name***REMOVED******REMOVED***
        clean_name = parsed_name.path.strip(***REMOVED***
        opened = False
        if content is None:
            if not self.exists(clean_name***REMOVED***:
                raise ValueError("The file '%s' could not be found with %r." %
                                 (clean_name, self***REMOVED******REMOVED***
            ***REMOVED***
                content = self.open(clean_name***REMOVED***
            except IOError:
                # Handle directory paths and fragments
                return name
            opened = True
        ***REMOVED***
            file_hash = self.file_hash(clean_name, content***REMOVED***
        finally:
            if opened:
                content.close(***REMOVED***
        path, filename = os.path.split(clean_name***REMOVED***
        root, ext = os.path.splitext(filename***REMOVED***
        if file_hash is not None:
            file_hash = ".%s" % file_hash
        hashed_name = os.path.join(path, "%s%s%s" %
                                   (root, file_hash, ext***REMOVED******REMOVED***
        unparsed_name = list(parsed_name***REMOVED***
        unparsed_name[2***REMOVED*** = hashed_name
        # Special casing for a @font-face hack, like url(myfont.eot?#iefix"***REMOVED***
        # http://www.fontspring.com/blog/the-new-bulletproof-font-face-syntax
        if '?#' in name and not unparsed_name[3***REMOVED***:
            unparsed_name[2***REMOVED*** += '?'
        return urlunsplit(unparsed_name***REMOVED***

    def url(self, name, force=False***REMOVED***:
        ***REMOVED***
        Return the real URL in DEBUG mode.
        ***REMOVED***
        if settings.DEBUG and not force:
            hashed_name, fragment = name, ''
        else:
            clean_name, fragment = urldefrag(name***REMOVED***
            if urlsplit(clean_name***REMOVED***.path.endswith('/'***REMOVED***:  # don't hash paths
                hashed_name = name
            else:
                hashed_name = self.stored_name(clean_name***REMOVED***

        final_url = super(HashedFilesMixin, self***REMOVED***.url(hashed_name***REMOVED***

        # Special casing for a @font-face hack, like url(myfont.eot?#iefix"***REMOVED***
        # http://www.fontspring.com/blog/the-new-bulletproof-font-face-syntax
        query_fragment = '?#' in name  # [sic!***REMOVED***
        if fragment or query_fragment:
            urlparts = list(urlsplit(final_url***REMOVED******REMOVED***
            if fragment and not urlparts[4***REMOVED***:
                urlparts[4***REMOVED*** = fragment
            if query_fragment and not urlparts[3***REMOVED***:
                urlparts[2***REMOVED*** += '?'
            final_url = urlunsplit(urlparts***REMOVED***

        return unquote(final_url***REMOVED***

    def url_converter(self, name, template=None***REMOVED***:
        ***REMOVED***
        Return the custom URL converter for the given file name.
        ***REMOVED***
        if template is None:
            template = self.default_template

        def converter(matchobj***REMOVED***:
            ***REMOVED***
            Convert the matched URL to a normalized and hashed URL.

            This requires figuring out which files the matched URL resolves
            to and calling the url(***REMOVED*** method of the storage.
            ***REMOVED***
            matched, url = matchobj.groups(***REMOVED***

            # Ignore absolute/protocol-relative, fragments and data-uri URLs.
            if url.startswith(('http:', 'https:', '//', '#', 'data:'***REMOVED******REMOVED***:
                return matched

            # Ignore absolute URLs that don't point to a static file (dynamic
            # CSS / JS?***REMOVED***. Note that STATIC_URL cannot be empty.
            if url.startswith('/'***REMOVED*** and not url.startswith(settings.STATIC_URL***REMOVED***:
                return matched

            # Strip off the fragment so a path-like fragment won't interfere.
            url_path, fragment = urldefrag(url***REMOVED***

            if url_path.startswith('/'***REMOVED***:
                # Otherwise the condition above would have returned prematurely.
                assert url_path.startswith(settings.STATIC_URL***REMOVED***
                target_name = url_path[len(settings.STATIC_URL***REMOVED***:***REMOVED***
            else:
                # We're using the posixpath module to mix paths and URLs conveniently.
                source_name = name if os.sep == '/' else name.replace(os.sep, '/'***REMOVED***
                target_name = posixpath.join(posixpath.dirname(source_name***REMOVED***, url_path***REMOVED***

            # Determine the hashed name of the target file with the storage backend.
            hashed_url = self.url(unquote(target_name***REMOVED***, force=True***REMOVED***

            transformed_url = '/'.join(url_path.split('/'***REMOVED***[:-1***REMOVED*** + hashed_url.split('/'***REMOVED***[-1:***REMOVED******REMOVED***

            # Restore the fragment that was stripped off earlier.
            if fragment:
                transformed_url += ('?#' if '?#' in url else '#'***REMOVED*** + fragment

            # Return the hashed version to the file
            return template % unquote(transformed_url***REMOVED***

        return converter

    def post_process(self, paths, dry_run=False, **options***REMOVED***:
        ***REMOVED***
        Post process the given OrderedDict of files (called from collectstatic***REMOVED***.

        Processing is actually two separate operations:

        1. renaming files to include a hash of their content for cache-busting,
           and copying those files to the target storage.
        2. adjusting files which contain references to other files so they
           refer to the cache-busting filenames.

        If either of these are performed on a file, then that file is considered
        post-processed.
        ***REMOVED***
        # don't even dare to process the files if we're in dry run mode
        if dry_run:
            return

        # where to store the new paths
        hashed_files = OrderedDict(***REMOVED***

        # build a list of adjustable files
        adjustable_paths = [
            path for path in paths
            if matches_patterns(path, self._patterns.keys(***REMOVED******REMOVED***
        ***REMOVED***

        # then sort the files by the directory level
        def path_level(name***REMOVED***:
            return len(name.split(os.sep***REMOVED******REMOVED***

        for name in sorted(paths.keys(***REMOVED***, key=path_level, reverse=True***REMOVED***:

            # use the original, local file, not the copied-but-unprocessed
            # file, which might be somewhere far away, like S3
            storage, path = paths[name***REMOVED***
            with storage.open(path***REMOVED*** as original_file:

                # generate the hash with the original content, even for
                # adjustable files.
                hashed_name = self.hashed_name(name, original_file***REMOVED***

                # then get the original's file content..
                if hasattr(original_file, 'seek'***REMOVED***:
                    original_file.seek(0***REMOVED***

                hashed_file_exists = self.exists(hashed_name***REMOVED***
                processed = False

                # ..to apply each replacement pattern to the content
                if name in adjustable_paths:
                    content = original_file.read(***REMOVED***.decode(settings.FILE_CHARSET***REMOVED***
                    for extension, patterns in iteritems(self._patterns***REMOVED***:
                        if matches_patterns(path, (extension,***REMOVED******REMOVED***:
                            for pattern, template in patterns:
                                converter = self.url_converter(name, template***REMOVED***
                                ***REMOVED***
                                    content = pattern.sub(converter, content***REMOVED***
                                except ValueError as exc:
                                    yield name, None, exc
                    if hashed_file_exists:
                        self.delete(hashed_name***REMOVED***
                    # then save the processed result
                    content_file = ContentFile(force_bytes(content***REMOVED******REMOVED***
                    saved_name = self._save(hashed_name, content_file***REMOVED***
                    hashed_name = force_text(self.clean_name(saved_name***REMOVED******REMOVED***
                    processed = True
                else:
                    # or handle the case in which neither processing nor
                    # a change to the original file happened
                    if not hashed_file_exists:
                        processed = True
                        saved_name = self._save(hashed_name, original_file***REMOVED***
                        hashed_name = force_text(self.clean_name(saved_name***REMOVED******REMOVED***

                # and then set the cache accordingly
                hashed_files[self.hash_key(name***REMOVED******REMOVED*** = hashed_name
                yield name, hashed_name, processed

        # Finally store the processed paths
        self.hashed_files.update(hashed_files***REMOVED***

    def clean_name(self, name***REMOVED***:
        return name.replace('\\', '/'***REMOVED***

    def hash_key(self, name***REMOVED***:
        return name

    def stored_name(self, name***REMOVED***:
        hash_key = self.hash_key(name***REMOVED***
        cache_name = self.hashed_files.get(hash_key***REMOVED***
        if cache_name is None:
            cache_name = self.clean_name(self.hashed_name(name***REMOVED******REMOVED***
            # store the hashed name if there was a miss, e.g.
            # when the files are still processed
            self.hashed_files[hash_key***REMOVED*** = cache_name
        return cache_name


class ManifestFilesMixin(HashedFilesMixin***REMOVED***:
    manifest_version = '1.0'  # the manifest format standard
    manifest_name = 'staticfiles.json'

    def __init__(self, *args, **kwargs***REMOVED***:
        super(ManifestFilesMixin, self***REMOVED***.__init__(*args, **kwargs***REMOVED***
        self.hashed_files = self.load_manifest(***REMOVED***

    def read_manifest(self***REMOVED***:
        ***REMOVED***
            with self.open(self.manifest_name***REMOVED*** as manifest:
                return manifest.read(***REMOVED***.decode('utf-8'***REMOVED***
        except IOError:
            return None

    def load_manifest(self***REMOVED***:
        content = self.read_manifest(***REMOVED***
        if content is None:
            return OrderedDict(***REMOVED***
        ***REMOVED***
            stored = json.loads(content, object_pairs_hook=OrderedDict***REMOVED***
        except ValueError:
            pass
        else:
            version = stored.get('version'***REMOVED***
            if version == '1.0':
                return stored.get('paths', OrderedDict(***REMOVED******REMOVED***
        raise ValueError("Couldn't load manifest '%s' (version %s***REMOVED***" %
                         (self.manifest_name, self.manifest_version***REMOVED******REMOVED***

    def post_process(self, *args, **kwargs***REMOVED***:
        self.hashed_files = OrderedDict(***REMOVED***
        all_post_processed = super(ManifestFilesMixin,
                                   self***REMOVED***.post_process(*args, **kwargs***REMOVED***
        for post_processed in all_post_processed:
            yield post_processed
        self.save_manifest(***REMOVED***

    def save_manifest(self***REMOVED***:
        payload = {'paths': self.hashed_files, 'version': self.manifest_version***REMOVED***
        if self.exists(self.manifest_name***REMOVED***:
            self.delete(self.manifest_name***REMOVED***
        contents = json.dumps(payload***REMOVED***.encode('utf-8'***REMOVED***
        self._save(self.manifest_name, ContentFile(contents***REMOVED******REMOVED***


class _MappingCache(object***REMOVED***:
    ***REMOVED***
    A small dict-like wrapper for a given cache backend instance.
    ***REMOVED***
    def __init__(self, cache***REMOVED***:
        self.cache = cache

    def __setitem__(self, key, value***REMOVED***:
        self.cache.set(key, value***REMOVED***

    def __getitem__(self, key***REMOVED***:
        value = self.cache.get(key***REMOVED***
        if value is None:
            raise KeyError("Couldn't find a file name '%s'" % key***REMOVED***
        return value

    def clear(self***REMOVED***:
        self.cache.clear(***REMOVED***

    def update(self, data***REMOVED***:
        self.cache.set_many(data***REMOVED***

    def get(self, key, default=None***REMOVED***:
        ***REMOVED***
            return self[key***REMOVED***
        except KeyError:
            return default


class CachedFilesMixin(HashedFilesMixin***REMOVED***:
    def __init__(self, *args, **kwargs***REMOVED***:
        super(CachedFilesMixin, self***REMOVED***.__init__(*args, **kwargs***REMOVED***
        ***REMOVED***
            self.hashed_files = _MappingCache(caches['staticfiles'***REMOVED******REMOVED***
        except InvalidCacheBackendError:
            # Use the default backend
            self.hashed_files = _MappingCache(default_cache***REMOVED***

    def hash_key(self, name***REMOVED***:
        key = hashlib.md5(force_bytes(self.clean_name(name***REMOVED******REMOVED******REMOVED***.hexdigest(***REMOVED***
        return 'staticfiles:%s' % key


class CachedStaticFilesStorage(CachedFilesMixin, StaticFilesStorage***REMOVED***:
    ***REMOVED***
    A static file system storage backend which also saves
    hashed copies of the files it saves.
    ***REMOVED***
    pass


class ManifestStaticFilesStorage(ManifestFilesMixin, StaticFilesStorage***REMOVED***:
    ***REMOVED***
    A static file system storage backend which also saves
    hashed copies of the files it saves.
    ***REMOVED***
    pass


class ConfiguredStorage(LazyObject***REMOVED***:
    def _setup(self***REMOVED***:
        self._wrapped = get_storage_class(settings.STATICFILES_STORAGE***REMOVED***(***REMOVED***

staticfiles_storage = ConfiguredStorage(***REMOVED***
