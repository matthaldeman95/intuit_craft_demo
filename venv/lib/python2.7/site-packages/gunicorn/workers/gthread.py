# -*- coding: utf-8 -
#
# This file is part of gunicorn released under the MIT license.
# See the NOTICE for more information.

# design:
# a threaded worker accepts connections in the main loop, accepted
# connections are are added to the thread pool as a connection job. On
# keepalive connections are put back in the loop waiting for an event.
# If no event happen after the keep alive timeout, the connectoin is
# closed.

from collections import deque
from datetime import datetime
import errno
from functools import partial
***REMOVED***
import socket
import ssl
import sys
from threading import RLock
import time

from .. import http
from ..http import wsgi
from .. import util
from . import base
from .. import six


***REMOVED***
    import concurrent.futures as futures
except ImportError:
    raise RuntimeError(***REMOVED***
    You need to install the 'futures' package to use this worker with this
    Python version.
    ***REMOVED******REMOVED***

***REMOVED***
    from asyncio import selectors
except ImportError:
    from gunicorn import selectors


class TConn(object***REMOVED***:

    def __init__(self, cfg, sock, client, server***REMOVED***:
        self.cfg = cfg
        self.sock = sock
        self.client = client
        self.server = server

        self.timeout = None
        self.parser = None

        # set the socket to non blocking
        self.sock.setblocking(False***REMOVED***

    def init(self***REMOVED***:
        self.sock.setblocking(True***REMOVED***
        if self.parser is None:
            # wrap the socket if needed
            if self.cfg.is_ssl:
                self.sock = ssl.wrap_socket(self.sock, server_side=True,
                        **self.cfg.ssl_options***REMOVED***

            # initialize the parser
            self.parser = http.RequestParser(self.cfg, self.sock***REMOVED***

    def set_timeout(self***REMOVED***:
        # set the timeout
        self.timeout = time.time(***REMOVED*** + self.cfg.keepalive

    def close(self***REMOVED***:
        util.close(self.sock***REMOVED***

    def __lt__(self, other***REMOVED***:
        return self.timeout < other.timeout

    __cmp__ = __lt__


class ThreadWorker(base.Worker***REMOVED***:

    def __init__(self, *args, **kwargs***REMOVED***:
        super(ThreadWorker, self***REMOVED***.__init__(*args, **kwargs***REMOVED***
        self.worker_connections = self.cfg.worker_connections
        self.max_keepalived = self.cfg.worker_connections - self.cfg.threads
        # initialise the pool
        self.tpool = None
        self.poller = None
        self._lock = None
        self.futures = deque(***REMOVED***
        self._keep = deque(***REMOVED***
        self.nr_conns = 0

    @classmethod
    def check_config(cls, cfg, log***REMOVED***:
        max_keepalived = cfg.worker_connections - cfg.threads

        if max_keepalived <= 0 and cfg.keepalive:
            log.warning("No keepalived connections can be handled. " +
                    "Check the number of worker connections and threads."***REMOVED***

    def init_process(self***REMOVED***:
        self.tpool = futures.ThreadPoolExecutor(max_workers=self.cfg.threads***REMOVED***
        self.poller = selectors.DefaultSelector(***REMOVED***
        self._lock = RLock(***REMOVED***
        super(ThreadWorker, self***REMOVED***.init_process(***REMOVED***

    def handle_quit(self, sig, frame***REMOVED***:
        self.alive = False
        # worker_int callback
        self.cfg.worker_int(self***REMOVED***
        self.tpool.shutdown(False***REMOVED***
        time.sleep(0.1***REMOVED***
        sys.exit(0***REMOVED***

    def _wrap_future(self, fs, conn***REMOVED***:
        fs.conn = conn
        self.futures.append(fs***REMOVED***
        fs.add_done_callback(self.finish_request***REMOVED***

    def enqueue_req(self, conn***REMOVED***:
        conn.init(***REMOVED***
        # submit the connection to a worker
        fs = self.tpool.submit(self.handle, conn***REMOVED***
        self._wrap_future(fs, conn***REMOVED***

    def accept(self, server, listener***REMOVED***:
        ***REMOVED***
            sock, client = listener.accept(***REMOVED***
            # initialize the connection object
            conn = TConn(self.cfg, sock, client, server***REMOVED***
            self.nr_conns += 1
            # enqueue the job
            self.enqueue_req(conn***REMOVED***
        except EnvironmentError as e:
            if e.errno not in (errno.EAGAIN,
                    errno.ECONNABORTED, errno.EWOULDBLOCK***REMOVED***:
                raise

    def reuse_connection(self, conn, client***REMOVED***:
        with self._lock:
            # unregister the client from the poller
            self.poller.unregister(client***REMOVED***
            # remove the connection from keepalive
            ***REMOVED***
                self._keep.remove(conn***REMOVED***
            except ValueError:
                # race condition
                return

        # submit the connection to a worker
        self.enqueue_req(conn***REMOVED***

    def murder_keepalived(self***REMOVED***:
        now = time.time(***REMOVED***
        while True:
            with self._lock:
                ***REMOVED***
                    # remove the connection from the queue
                    conn = self._keep.popleft(***REMOVED***
                except IndexError:
                    break

            delta = conn.timeout - now
            if delta > 0:
                # add the connection back to the queue
                with self._lock:
                    self._keep.appendleft(conn***REMOVED***
                break
            else:
                self.nr_conns -= 1
                # remove the socket from the poller
                with self._lock:
                    ***REMOVED***
                        self.poller.unregister(conn.sock***REMOVED***
                    except EnvironmentError as e:
                        if e.errno != errno.EBADF:
                            raise
                    except KeyError:
                        # already removed by the system, continue
                        pass

                # close the socket
                conn.close(***REMOVED***

    def is_parent_alive(self***REMOVED***:
        # If our parent changed then we shut down.
        if self.ppid != os.getppid(***REMOVED***:
            self.log.info("Parent changed, shutting down: %s", self***REMOVED***
            return False
        return True

    def run(self***REMOVED***:
        # init listeners, add them to the event loop
        for sock in self.sockets:
            sock.setblocking(False***REMOVED***
            # a race condition during graceful shutdown may make the listener
            # name unavailable in the request handler so capture it once here
            server = sock.getsockname(***REMOVED***
            acceptor = partial(self.accept, server***REMOVED***
            self.poller.register(sock, selectors.EVENT_READ, acceptor***REMOVED***

        while self.alive:
            # notify the arbiter we are alive
            self.notify(***REMOVED***

            # can we accept more connections?
            if self.nr_conns < self.worker_connections:
                # wait for an event
                events = self.poller.select(1.0***REMOVED***
                for key, mask in events:
                    callback = key.data
                    callback(key.fileobj***REMOVED***

                # check (but do not wait***REMOVED*** for finished requests
                result = futures.wait(self.futures, timeout=0,
                        return_when=futures.FIRST_COMPLETED***REMOVED***
            else:
                # wait for a request to finish
                result = futures.wait(self.futures, timeout=1.0,
                        return_when=futures.FIRST_COMPLETED***REMOVED***

            # clean up finished requests
            for fut in result.done:
                self.futures.remove(fut***REMOVED***

            if not self.is_parent_alive(***REMOVED***:
                break

            # hanle keepalive timeouts
            self.murder_keepalived(***REMOVED***

        self.tpool.shutdown(False***REMOVED***
        self.poller.close(***REMOVED***

        for s in self.sockets:
            s.close(***REMOVED***

        futures.wait(self.futures, timeout=self.cfg.graceful_timeout***REMOVED***

    def finish_request(self, fs***REMOVED***:
        if fs.cancelled(***REMOVED***:
            fs.conn.close(***REMOVED***
            return

        ***REMOVED***
            (keepalive, conn***REMOVED*** = fs.result(***REMOVED***
            # if the connection should be kept alived add it
            # to the eventloop and record it
            if keepalive:
                # flag the socket as non blocked
                conn.sock.setblocking(False***REMOVED***

                # register the connection
                conn.set_timeout(***REMOVED***
                with self._lock:
                    self._keep.append(conn***REMOVED***

                    # add the socket to the event loop
                    self.poller.register(conn.sock, selectors.EVENT_READ,
                            partial(self.reuse_connection, conn***REMOVED******REMOVED***
            else:
                self.nr_conns -= 1
                conn.close(***REMOVED***
        ***REMOVED***
            # an exception happened, make sure to close the
            # socket.
            self.nr_conns -= 1
            fs.conn.close(***REMOVED***

    def handle(self, conn***REMOVED***:
        keepalive = False
        req = None
        ***REMOVED***
            req = six.next(conn.parser***REMOVED***
            if not req:
                return (False, conn***REMOVED***

            # handle the request
            keepalive = self.handle_request(req, conn***REMOVED***
            if keepalive:
                return (keepalive, conn***REMOVED***
        except http.errors.NoMoreData as e:
            self.log.debug("Ignored premature client disconnection. %s", e***REMOVED***

        except StopIteration as e:
            self.log.debug("Closing connection. %s", e***REMOVED***
        except ssl.SSLError as e:
            if e.args[0***REMOVED*** == ssl.SSL_ERROR_EOF:
                self.log.debug("ssl connection closed"***REMOVED***
                conn.sock.close(***REMOVED***
            else:
                self.log.debug("Error processing SSL request."***REMOVED***
                self.handle_error(req, conn.sock, conn.client, e***REMOVED***

        except EnvironmentError as e:
            if e.errno not in (errno.EPIPE, errno.ECONNRESET***REMOVED***:
                self.log.exception("Socket error processing request."***REMOVED***
            else:
                if e.errno == errno.ECONNRESET:
                    self.log.debug("Ignoring connection reset"***REMOVED***
                else:
                    self.log.debug("Ignoring connection epipe"***REMOVED***
        except Exception as e:
            self.handle_error(req, conn.sock, conn.client, e***REMOVED***

        return (False, conn***REMOVED***

    def handle_request(self, req, conn***REMOVED***:
        environ = {***REMOVED***
        resp = None
        ***REMOVED***
            self.cfg.pre_request(self, req***REMOVED***
            request_start = datetime.now(***REMOVED***
            resp, environ = wsgi.create(req, conn.sock, conn.client,
                    conn.server, self.cfg***REMOVED***
            environ["wsgi.multithread"***REMOVED*** = True
            self.nr += 1
            if self.alive and self.nr >= self.max_requests:
                self.log.info("Autorestarting worker after current request."***REMOVED***
                resp.force_close(***REMOVED***
                self.alive = False

            if not self.cfg.keepalive:
                resp.force_close(***REMOVED***
            elif len(self._keep***REMOVED*** >= self.max_keepalived:
                resp.force_close(***REMOVED***

            respiter = self.wsgi(environ, resp.start_response***REMOVED***
            ***REMOVED***
                if isinstance(respiter, environ['wsgi.file_wrapper'***REMOVED******REMOVED***:
                    resp.write_file(respiter***REMOVED***
                else:
                    for item in respiter:
                        resp.write(item***REMOVED***

                resp.close(***REMOVED***
                request_time = datetime.now(***REMOVED*** - request_start
                self.log.access(resp, req, environ, request_time***REMOVED***
            finally:
                if hasattr(respiter, "close"***REMOVED***:
                    respiter.close(***REMOVED***

            if resp.should_close(***REMOVED***:
                self.log.debug("Closing connection."***REMOVED***
                return False
        except EnvironmentError:
            # pass to next try-except level
            six.reraise(*sys.exc_info(***REMOVED******REMOVED***
        except Exception:
            if resp and resp.headers_sent:
                # If the requests have already been sent, we should close the
                # connection to indicate the error.
                self.log.exception("Error handling request"***REMOVED***
                ***REMOVED***
                    conn.sock.shutdown(socket.SHUT_RDWR***REMOVED***
                    conn.sock.close(***REMOVED***
                except EnvironmentError:
                    pass
                raise StopIteration(***REMOVED***
            raise
        finally:
            ***REMOVED***
                self.cfg.post_request(self, req, environ, resp***REMOVED***
            except Exception:
                self.log.exception("Exception in post_request hook"***REMOVED***

        return True
