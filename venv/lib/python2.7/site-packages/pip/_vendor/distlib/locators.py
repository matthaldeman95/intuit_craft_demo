# -*- coding: utf-8 -*-
#
# Copyright (C***REMOVED*** 2012-2015 Vinay Sajip.
# Licensed to the Python Software Foundation under a contributor agreement.
# See LICENSE.txt and CONTRIBUTORS.txt.
#

import gzip
from io import BytesIO
import json
import logging
***REMOVED***
import posixpath
import re
***REMOVED***
    import threading
except ImportError:  # pragma: no cover
    import dummy_threading as threading
import zlib

from . import DistlibException
from .compat import (urljoin, urlparse, urlunparse, url2pathname, pathname2url,
                     queue, quote, unescape, string_types, build_opener,
                     HTTPRedirectHandler as BaseRedirectHandler,
                     Request, HTTPError, URLError***REMOVED***
from .database import Distribution, DistributionPath, make_dist
from .metadata import Metadata
from .util import (cached_property, parse_credentials, ensure_slash,
                   split_filename, get_project_data, parse_requirement,
                   parse_name_and_version, ServerProxy***REMOVED***
from .version import get_scheme, UnsupportedVersionError
from .wheel import Wheel, is_compatible

logger = logging.getLogger(__name__***REMOVED***

HASHER_HASH = re.compile('^(\w+***REMOVED***=([a-f0-9***REMOVED***+***REMOVED***'***REMOVED***
CHARSET = re.compile(r';\s*charset\s*=\s*(.****REMOVED***\s*$', re.I***REMOVED***
HTML_CONTENT_TYPE = re.compile('text/html|application/x(ht***REMOVED***?ml'***REMOVED***
DEFAULT_INDEX = 'https://pypi.python.org/pypi'

def get_all_distribution_names(url=None***REMOVED***:
    ***REMOVED***
    Return all distribution names known by an index.
    :param url: The URL of the index.
    :return: A list of all known distribution names.
    ***REMOVED***
    if url is None:
        url = DEFAULT_INDEX
    client = ServerProxy(url, timeout=3.0***REMOVED***
    return client.list_packages(***REMOVED***

class RedirectHandler(BaseRedirectHandler***REMOVED***:
    ***REMOVED***
    A class to work around a bug in some Python 3.2.x releases.
    ***REMOVED***
    # There's a bug in the base version for some 3.2.x
    # (e.g. 3.2.2 on Ubuntu Oneiric***REMOVED***. If a Location header
    # returns e.g. /abc, it bails because it says the scheme ''
    # is bogus, when actually it should use the request's
    # URL for the scheme. See Python issue #13696.
    def http_error_302(self, req, fp, code, msg, headers***REMOVED***:
        # Some servers (incorrectly***REMOVED*** return multiple Location headers
        # (so probably same goes for URI***REMOVED***.  Use first header.
        newurl = None
        for key in ('location', 'uri'***REMOVED***:
            if key in headers:
                newurl = headers[key***REMOVED***
                break
        if newurl is None:
            return
        urlparts = urlparse(newurl***REMOVED***
        if urlparts.scheme == '':
            newurl = urljoin(req.get_full_url(***REMOVED***, newurl***REMOVED***
            if hasattr(headers, 'replace_header'***REMOVED***:
                headers.replace_header(key, newurl***REMOVED***
            else:
                headers[key***REMOVED*** = newurl
        return BaseRedirectHandler.http_error_302(self, req, fp, code, msg,
                                                  headers***REMOVED***

    http_error_301 = http_error_303 = http_error_307 = http_error_302

class Locator(object***REMOVED***:
    ***REMOVED***
    A base class for locators - things that locate distributions.
    ***REMOVED***
    source_extensions = ('.tar.gz', '.tar.bz2', '.tar', '.zip', '.tgz', '.tbz'***REMOVED***
    binary_extensions = ('.egg', '.exe', '.whl'***REMOVED***
    excluded_extensions = ('.pdf',***REMOVED***

    # A list of tags indicating which wheels you want to match. The default
    # value of None matches against the tags compatible with the running
    # Python. If you want to match other values, set wheel_tags on a locator
    # instance to a list of tuples (pyver, abi, arch***REMOVED*** which you want to match.
    wheel_tags = None

    downloadable_extensions = source_extensions + ('.whl',***REMOVED***

    def __init__(self, scheme='default'***REMOVED***:
        ***REMOVED***
        Initialise an instance.
        :param scheme: Because locators look for most recent versions, they
                       need to know the version scheme to use. This specifies
                       the current PEP-recommended scheme - use ``'legacy'``
                       if you need to support existing distributions on PyPI.
        ***REMOVED***
        self._cache = {***REMOVED***
        self.scheme = scheme
        # Because of bugs in some of the handlers on some of the platforms,
        # we use our own opener rather than just using urlopen.
        self.opener = build_opener(RedirectHandler(***REMOVED******REMOVED***
        # If get_project(***REMOVED*** is called from locate(***REMOVED***, the matcher instance
        # is set from the requirement passed to locate(***REMOVED***. See issue #18 for
        # why this can be useful to know.
        self.matcher = None

    def clear_cache(self***REMOVED***:
        self._cache.clear(***REMOVED***

    def _get_scheme(self***REMOVED***:
        return self._scheme

    def _set_scheme(self, value***REMOVED***:
        self._scheme = value

    scheme = property(_get_scheme, _set_scheme***REMOVED***

    def _get_project(self, name***REMOVED***:
        ***REMOVED***
        For a given project, get a dictionary mapping available versions to Distribution
        instances.

        This should be implemented in subclasses.

        If called from a locate(***REMOVED*** request, self.matcher will be set to a
        matcher for the requirement to satisfy, otherwise it will be None.
        ***REMOVED***
        raise NotImplementedError('Please implement in the subclass'***REMOVED***

    def get_distribution_names(self***REMOVED***:
        ***REMOVED***
        Return all the distribution names known to this locator.
        ***REMOVED***
        raise NotImplementedError('Please implement in the subclass'***REMOVED***

    def get_project(self, name***REMOVED***:
        ***REMOVED***
        For a given project, get a dictionary mapping available versions to Distribution
        instances.

        This calls _get_project to do all the work, and just implements a caching layer on top.
        ***REMOVED***
        if self._cache is None:
            result = self._get_project(name***REMOVED***
        elif name in self._cache:
            result = self._cache[name***REMOVED***
        else:
            result = self._get_project(name***REMOVED***
            self._cache[name***REMOVED*** = result
        return result

    def score_url(self, url***REMOVED***:
        ***REMOVED***
        Give an url a score which can be used to choose preferred URLs
        for a given project release.
        ***REMOVED***
        t = urlparse(url***REMOVED***
        basename = posixpath.basename(t.path***REMOVED***
        compatible = True
        is_wheel = basename.endswith('.whl'***REMOVED***
        if is_wheel:
            compatible = is_compatible(Wheel(basename***REMOVED***, self.wheel_tags***REMOVED***
        return (t.scheme != 'https', 'pypi.python.org' in t.netloc,
                is_wheel, compatible, basename***REMOVED***

    def prefer_url(self, url1, url2***REMOVED***:
        ***REMOVED***
        Choose one of two URLs where both are candidates for distribution
        archives for the same version of a distribution (for example,
        .tar.gz vs. zip***REMOVED***.

        The current implementation favours https:// URLs over http://, archives
        from PyPI over those from other locations, wheel compatibility (if a
        wheel***REMOVED*** and then the archive name.
        ***REMOVED***
        result = url2
        if url1:
            s1 = self.score_url(url1***REMOVED***
            s2 = self.score_url(url2***REMOVED***
            if s1 > s2:
                result = url1
            if result != url2:
                logger.debug('Not replacing %r with %r', url1, url2***REMOVED***
            else:
                logger.debug('Replacing %r with %r', url1, url2***REMOVED***
        return result

    def split_filename(self, filename, project_name***REMOVED***:
        ***REMOVED***
        Attempt to split a filename in project name, version and Python version.
        ***REMOVED***
        return split_filename(filename, project_name***REMOVED***

    def convert_url_to_download_info(self, url, project_name***REMOVED***:
        ***REMOVED***
        See if a URL is a candidate for a download URL for a project (the URL
        has typically been scraped from an HTML page***REMOVED***.

        If it is, a dictionary is returned with keys "name", "version",
        "filename" and "url"; otherwise, None is returned.
        ***REMOVED***
        def same_project(name1, name2***REMOVED***:
            name1, name2 = name1.lower(***REMOVED***, name2.lower(***REMOVED***
            if name1 == name2:
                result = True
            else:
                # distribute replaces '-' by '_' in project names, so it
                # can tell where the version starts in a filename.
                result = name1.replace('_', '-'***REMOVED*** == name2.replace('_', '-'***REMOVED***
            return result

        result = None
        scheme, netloc, path, params, query, frag = urlparse(url***REMOVED***
        if frag.lower(***REMOVED***.startswith('egg='***REMOVED***:
            logger.debug('%s: version hint in fragment: %r',
                         project_name, frag***REMOVED***
        m = HASHER_HASH.match(frag***REMOVED***
        if m:
            algo, digest = m.groups(***REMOVED***
        else:
            algo, digest = None, None
        origpath = path
        if path and path[-1***REMOVED*** == '/':
            path = path[:-1***REMOVED***
        if path.endswith('.whl'***REMOVED***:
            ***REMOVED***
                wheel = Wheel(path***REMOVED***
                if is_compatible(wheel, self.wheel_tags***REMOVED***:
                    if project_name is None:
                        include = True
                    else:
                        include = same_project(wheel.name, project_name***REMOVED***
                    if include:
                        result = {
                            'name': wheel.name,
                            'version': wheel.version,
                            'filename': wheel.filename,
                            'url': urlunparse((scheme, netloc, origpath,
                                               params, query, ''***REMOVED******REMOVED***,
                            'python-version': ', '.join(
                                ['.'.join(list(v[2:***REMOVED******REMOVED******REMOVED*** for v in wheel.pyver***REMOVED******REMOVED***,
                    ***REMOVED***
            except Exception as e:
                logger.warning('invalid path for wheel: %s', path***REMOVED***
        elif path.endswith(self.downloadable_extensions***REMOVED***:
            path = filename = posixpath.basename(path***REMOVED***
            for ext in self.downloadable_extensions:
                if path.endswith(ext***REMOVED***:
                    path = path[:-len(ext***REMOVED******REMOVED***
                    t = self.split_filename(path, project_name***REMOVED***
                    if not t:
                        logger.debug('No match for project/version: %s', path***REMOVED***
                    else:
                        name, version, pyver = t
                        if not project_name or same_project(project_name, name***REMOVED***:
                            result = {
                                'name': name,
                                'version': version,
                                'filename': filename,
                                'url': urlunparse((scheme, netloc, origpath,
                                                   params, query, ''***REMOVED******REMOVED***,
                                #'packagetype': 'sdist',
                        ***REMOVED***
                            if pyver:
                                result['python-version'***REMOVED*** = pyver
                    break
        if result and algo:
            result['%s_digest' % algo***REMOVED*** = digest
        return result

    def _get_digest(self, info***REMOVED***:
        ***REMOVED***
        Get a digest from a dictionary by looking at keys of the form
        'algo_digest'.

        Returns a 2-tuple (algo, digest***REMOVED*** if found, else None. Currently
        looks only for SHA256, then MD5.
        ***REMOVED***
        result = None
        for algo in ('sha256', 'md5'***REMOVED***:
            key = '%s_digest' % algo
            if key in info:
                result = (algo, info[key***REMOVED******REMOVED***
                break
        return result

    def _update_version_data(self, result, info***REMOVED***:
        ***REMOVED***
        Update a result dictionary (the final result from _get_project***REMOVED*** with a
        dictionary for a specific version, which typically holds information
        gleaned from a filename or URL for an archive for the distribution.
        ***REMOVED***
        name = info.pop('name'***REMOVED***
        version = info.pop('version'***REMOVED***
        if version in result:
            dist = result[version***REMOVED***
            md = dist.metadata
        else:
            dist = make_dist(name, version, scheme=self.scheme***REMOVED***
            md = dist.metadata
        dist.digest = digest = self._get_digest(info***REMOVED***
        url = info['url'***REMOVED***
        result['digests'***REMOVED***[url***REMOVED*** = digest
        if md.source_url != info['url'***REMOVED***:
            md.source_url = self.prefer_url(md.source_url, url***REMOVED***
            result['urls'***REMOVED***.setdefault(version, set(***REMOVED******REMOVED***.add(url***REMOVED***
        dist.locator = self
        result[version***REMOVED*** = dist

    def locate(self, requirement, prereleases=False***REMOVED***:
        ***REMOVED***
        Find the most recent distribution which matches the given
        requirement.

        :param requirement: A requirement of the form 'foo (1.0***REMOVED***' or perhaps
                            'foo (>= 1.0, < 2.0, != 1.3***REMOVED***'
        :param prereleases: If ``True``, allow pre-release versions
                            to be located. Otherwise, pre-release versions
                            are not returned.
        :return: A :class:`Distribution` instance, or ``None`` if no such
                 distribution could be located.
        ***REMOVED***
        result = None
        r = parse_requirement(requirement***REMOVED***
        if r is None:
            raise DistlibException('Not a valid requirement: %r' % requirement***REMOVED***
        scheme = get_scheme(self.scheme***REMOVED***
        self.matcher = matcher = scheme.matcher(r.requirement***REMOVED***
        logger.debug('matcher: %s (%s***REMOVED***', matcher, type(matcher***REMOVED***.__name__***REMOVED***
        versions = self.get_project(r.name***REMOVED***
        if len(versions***REMOVED*** > 2:   # urls and digests keys are present
            # sometimes, versions are invalid
            slist = [***REMOVED***
            vcls = matcher.version_class
            for k in versions:
                if k in ('urls', 'digests'***REMOVED***:
                    continue
                ***REMOVED***
                    if not matcher.match(k***REMOVED***:
                        logger.debug('%s did not match %r', matcher, k***REMOVED***
                    else:
                        if prereleases or not vcls(k***REMOVED***.is_prerelease:
                            slist.append(k***REMOVED***
                        else:
                            logger.debug('skipping pre-release '
                                         'version %s of %s', k, matcher.name***REMOVED***
                except Exception:  # pragma: no cover
                    logger.warning('error matching %s with %r', matcher, k***REMOVED***
                    pass # slist.append(k***REMOVED***
            if len(slist***REMOVED*** > 1:
                slist = sorted(slist, key=scheme.key***REMOVED***
            if slist:
                logger.debug('sorted list: %s', slist***REMOVED***
                version = slist[-1***REMOVED***
                result = versions[version***REMOVED***
        if result:
            if r.extras:
                result.extras = r.extras
            result.download_urls = versions.get('urls', {***REMOVED******REMOVED***.get(version, set(***REMOVED******REMOVED***
            d = {***REMOVED***
            sd = versions.get('digests', {***REMOVED******REMOVED***
            for url in result.download_urls:
                if url in sd:
                    d[url***REMOVED*** = sd[url***REMOVED***
            result.digests = d
        self.matcher = None
        return result


class PyPIRPCLocator(Locator***REMOVED***:
    ***REMOVED***
    This locator uses XML-RPC to locate distributions. It therefore
    cannot be used with simple mirrors (that only mirror file content***REMOVED***.
    ***REMOVED***
    def __init__(self, url, **kwargs***REMOVED***:
        ***REMOVED***
        Initialise an instance.

        :param url: The URL to use for XML-RPC.
        :param kwargs: Passed to the superclass constructor.
        ***REMOVED***
        super(PyPIRPCLocator, self***REMOVED***.__init__(**kwargs***REMOVED***
        self.base_url = url
        self.client = ServerProxy(url, timeout=3.0***REMOVED***

    def get_distribution_names(self***REMOVED***:
        ***REMOVED***
        Return all the distribution names known to this locator.
        ***REMOVED***
        return set(self.client.list_packages(***REMOVED******REMOVED***

    def _get_project(self, name***REMOVED***:
        result = {'urls': {***REMOVED***, 'digests': {***REMOVED******REMOVED***
        versions = self.client.package_releases(name, True***REMOVED***
        for v in versions:
            urls = self.client.release_urls(name, v***REMOVED***
            data = self.client.release_data(name, v***REMOVED***
            metadata = Metadata(scheme=self.scheme***REMOVED***
            metadata.name = data['name'***REMOVED***
            metadata.version = data['version'***REMOVED***
            metadata.license = data.get('license'***REMOVED***
            metadata.keywords = data.get('keywords', [***REMOVED******REMOVED***
            metadata.summary = data.get('summary'***REMOVED***
            dist = Distribution(metadata***REMOVED***
            if urls:
                info = urls[0***REMOVED***
                metadata.source_url = info['url'***REMOVED***
                dist.digest = self._get_digest(info***REMOVED***
                dist.locator = self
                result[v***REMOVED*** = dist
                for info in urls:
                    url = info['url'***REMOVED***
                    digest = self._get_digest(info***REMOVED***
                    result['urls'***REMOVED***.setdefault(v, set(***REMOVED******REMOVED***.add(url***REMOVED***
                    result['digests'***REMOVED***[url***REMOVED*** = digest
        return result

class PyPIJSONLocator(Locator***REMOVED***:
    ***REMOVED***
    This locator uses PyPI's JSON interface. It's very limited in functionality
    and probably not worth using.
    ***REMOVED***
    def __init__(self, url, **kwargs***REMOVED***:
        super(PyPIJSONLocator, self***REMOVED***.__init__(**kwargs***REMOVED***
        self.base_url = ensure_slash(url***REMOVED***

    def get_distribution_names(self***REMOVED***:
        ***REMOVED***
        Return all the distribution names known to this locator.
        ***REMOVED***
        raise NotImplementedError('Not available from this locator'***REMOVED***

    def _get_project(self, name***REMOVED***:
        result = {'urls': {***REMOVED***, 'digests': {***REMOVED******REMOVED***
        url = urljoin(self.base_url, '%s/json' % quote(name***REMOVED******REMOVED***
        ***REMOVED***
            resp = self.opener.open(url***REMOVED***
            data = resp.read(***REMOVED***.decode(***REMOVED*** # for now
            d = json.loads(data***REMOVED***
            md = Metadata(scheme=self.scheme***REMOVED***
            data = d['info'***REMOVED***
            md.name = data['name'***REMOVED***
            md.version = data['version'***REMOVED***
            md.license = data.get('license'***REMOVED***
            md.keywords = data.get('keywords', [***REMOVED******REMOVED***
            md.summary = data.get('summary'***REMOVED***
            dist = Distribution(md***REMOVED***
            dist.locator = self
            urls = d['urls'***REMOVED***
            result[md.version***REMOVED*** = dist
            for info in d['urls'***REMOVED***:
                url = info['url'***REMOVED***
                dist.download_urls.add(url***REMOVED***
                dist.digests[url***REMOVED*** = self._get_digest(info***REMOVED***
                result['urls'***REMOVED***.setdefault(md.version, set(***REMOVED******REMOVED***.add(url***REMOVED***
                result['digests'***REMOVED***[url***REMOVED*** = self._get_digest(info***REMOVED***
            # Now get other releases
            for version, infos in d['releases'***REMOVED***.items(***REMOVED***:
                if version == md.version:
                    continue    # already done
                omd = Metadata(scheme=self.scheme***REMOVED***
                omd.name = md.name
                omd.version = version
                odist = Distribution(omd***REMOVED***
                odist.locator = self
                result[version***REMOVED*** = odist
                for info in infos:
                    url = info['url'***REMOVED***
                    odist.download_urls.add(url***REMOVED***
                    odist.digests[url***REMOVED*** = self._get_digest(info***REMOVED***
                    result['urls'***REMOVED***.setdefault(version, set(***REMOVED******REMOVED***.add(url***REMOVED***
                    result['digests'***REMOVED***[url***REMOVED*** = self._get_digest(info***REMOVED***
#            for info in urls:
#                md.source_url = info['url'***REMOVED***
#                dist.digest = self._get_digest(info***REMOVED***
#                dist.locator = self
#                for info in urls:
#                    url = info['url'***REMOVED***
#                    result['urls'***REMOVED***.setdefault(md.version, set(***REMOVED******REMOVED***.add(url***REMOVED***
#                    result['digests'***REMOVED***[url***REMOVED*** = self._get_digest(info***REMOVED***
        except Exception as e:
            logger.exception('JSON fetch failed: %s', e***REMOVED***
        return result


class Page(object***REMOVED***:
    ***REMOVED***
    This class represents a scraped HTML page.
    ***REMOVED***
    # The following slightly hairy-looking regex just looks for the contents of
    # an anchor link, which has an attribute "href" either immediately preceded
    # or immediately followed by a "rel" attribute. The attribute values can be
    # declared with double quotes, single quotes or no quotes - which leads to
    # the length of the expression.
    _href = re.compile(***REMOVED***
(rel\s*=\s*(?:"(?P<rel1>[^"***REMOVED*******REMOVED***"|'(?P<rel2>[^'***REMOVED*******REMOVED***'|(?P<rel3>[^>\s\n***REMOVED*******REMOVED******REMOVED***\s+***REMOVED***?
href\s*=\s*(?:"(?P<url1>[^"***REMOVED*******REMOVED***"|'(?P<url2>[^'***REMOVED*******REMOVED***'|(?P<url3>[^>\s\n***REMOVED*******REMOVED******REMOVED***
(\s+rel\s*=\s*(?:"(?P<rel4>[^"***REMOVED*******REMOVED***"|'(?P<rel5>[^'***REMOVED*******REMOVED***'|(?P<rel6>[^>\s\n***REMOVED*******REMOVED******REMOVED******REMOVED***?
***REMOVED***, re.I | re.S | re.X***REMOVED***
    _base = re.compile(r***REMOVED***<base\s+href\s*=\s*['"***REMOVED***?([^'">***REMOVED***+***REMOVED******REMOVED***, re.I | re.S***REMOVED***

    def __init__(self, data, url***REMOVED***:
        ***REMOVED***
        Initialise an instance with the Unicode page contents and the URL they
        came from.
        ***REMOVED***
        self.data = data
        self.base_url = self.url = url
        m = self._base.search(self.data***REMOVED***
        if m:
            self.base_url = m.group(1***REMOVED***

    _clean_re = re.compile(r'[^a-z0-9$&+,/:;=?@.#%_\\|-***REMOVED***', re.I***REMOVED***

    @cached_property
    def links(self***REMOVED***:
        ***REMOVED***
        Return the URLs of all the links on a page together with information
        about their "rel" attribute, for determining which ones to treat as
        downloads and which ones to queue for further scraping.
        ***REMOVED***
        def clean(url***REMOVED***:
            "Tidy up an URL."
            scheme, netloc, path, params, query, frag = urlparse(url***REMOVED***
            return urlunparse((scheme, netloc, quote(path***REMOVED***,
                               params, query, frag***REMOVED******REMOVED***

        result = set(***REMOVED***
        for match in self._href.finditer(self.data***REMOVED***:
            d = match.groupdict(''***REMOVED***
            rel = (d['rel1'***REMOVED*** or d['rel2'***REMOVED*** or d['rel3'***REMOVED*** or
                   d['rel4'***REMOVED*** or d['rel5'***REMOVED*** or d['rel6'***REMOVED******REMOVED***
            url = d['url1'***REMOVED*** or d['url2'***REMOVED*** or d['url3'***REMOVED***
            url = urljoin(self.base_url, url***REMOVED***
            url = unescape(url***REMOVED***
            url = self._clean_re.sub(lambda m: '%%%2x' % ord(m.group(0***REMOVED******REMOVED***, url***REMOVED***
            result.add((url, rel***REMOVED******REMOVED***
        # We sort the result, hoping to bring the most recent versions
        # to the front
        result = sorted(result, key=lambda t: t[0***REMOVED***, reverse=True***REMOVED***
        return result


class SimpleScrapingLocator(Locator***REMOVED***:
    ***REMOVED***
    A locator which scrapes HTML pages to locate downloads for a distribution.
    This runs multiple threads to do the I/O; performance is at least as good
    as pip's PackageFinder, which works in an analogous fashion.
    ***REMOVED***

    # These are used to deal with various Content-Encoding schemes.
    decoders = {
        'deflate': zlib.decompress,
        'gzip': lambda b: gzip.GzipFile(fileobj=BytesIO(d***REMOVED******REMOVED***.read(***REMOVED***,
        'none': lambda b: b,
***REMOVED***

    def __init__(self, url, timeout=None, num_workers=10, **kwargs***REMOVED***:
        ***REMOVED***
        Initialise an instance.
        :param url: The root URL to use for scraping.
        :param timeout: The timeout, in seconds, to be applied to requests.
                        This defaults to ``None`` (no timeout specified***REMOVED***.
        :param num_workers: The number of worker threads you want to do I/O,
                            This defaults to 10.
        :param kwargs: Passed to the superclass.
        ***REMOVED***
        super(SimpleScrapingLocator, self***REMOVED***.__init__(**kwargs***REMOVED***
        self.base_url = ensure_slash(url***REMOVED***
        self.timeout = timeout
        self._page_cache = {***REMOVED***
        self._seen = set(***REMOVED***
        self._to_fetch = queue.Queue(***REMOVED***
        self._bad_hosts = set(***REMOVED***
        self.skip_externals = False
        self.num_workers = num_workers
        self._lock = threading.RLock(***REMOVED***
        # See issue #45: we need to be resilient when the locator is used
        # in a thread, e.g. with concurrent.futures. We can't use self._lock
        # as it is for coordinating our internal threads - the ones created
        # in _prepare_threads.
        self._gplock = threading.RLock(***REMOVED***

    def _prepare_threads(self***REMOVED***:
        ***REMOVED***
        Threads are created only when get_project is called, and terminate
        before it returns. They are there primarily to parallelise I/O (i.e.
        fetching web pages***REMOVED***.
        ***REMOVED***
        self._threads = [***REMOVED***
        for i in range(self.num_workers***REMOVED***:
            t = threading.Thread(target=self._fetch***REMOVED***
            t.setDaemon(True***REMOVED***
            t.start(***REMOVED***
            self._threads.append(t***REMOVED***

    def _wait_threads(self***REMOVED***:
        ***REMOVED***
        Tell all the threads to terminate (by sending a sentinel value***REMOVED*** and
        wait for them to do so.
        ***REMOVED***
        # Note that you need two loops, since you can't say which
        # thread will get each sentinel
        for t in self._threads:
            self._to_fetch.put(None***REMOVED***    # sentinel
        for t in self._threads:
            t.join(***REMOVED***
        self._threads = [***REMOVED***

    def _get_project(self, name***REMOVED***:
        result = {'urls': {***REMOVED***, 'digests': {***REMOVED******REMOVED***
        with self._gplock:
            self.result = result
            self.project_name = name
            url = urljoin(self.base_url, '%s/' % quote(name***REMOVED******REMOVED***
            self._seen.clear(***REMOVED***
            self._page_cache.clear(***REMOVED***
            self._prepare_threads(***REMOVED***
            ***REMOVED***
                logger.debug('Queueing %s', url***REMOVED***
                self._to_fetch.put(url***REMOVED***
                self._to_fetch.join(***REMOVED***
            finally:
                self._wait_threads(***REMOVED***
            del self.result
        return result

    platform_dependent = re.compile(r'\b(linux-(i\d86|x86_64|arm\w+***REMOVED***|'
                                    r'win(32|-amd64***REMOVED***|macosx-?\d+***REMOVED***\b', re.I***REMOVED***

    def _is_platform_dependent(self, url***REMOVED***:
        ***REMOVED***
        Does an URL refer to a platform-specific download?
        ***REMOVED***
        return self.platform_dependent.search(url***REMOVED***

    def _process_download(self, url***REMOVED***:
        ***REMOVED***
        See if an URL is a suitable download for a project.

        If it is, register information in the result dictionary (for
        _get_project***REMOVED*** about the specific version it's for.

        Note that the return value isn't actually used other than as a boolean
        value.
        ***REMOVED***
        if self._is_platform_dependent(url***REMOVED***:
            info = None
        else:
            info = self.convert_url_to_download_info(url, self.project_name***REMOVED***
        logger.debug('process_download: %s -> %s', url, info***REMOVED***
        if info:
            with self._lock:    # needed because self.result is shared
                self._update_version_data(self.result, info***REMOVED***
        return info

    def _should_queue(self, link, referrer, rel***REMOVED***:
        ***REMOVED***
        Determine whether a link URL from a referring page and with a
        particular "rel" attribute should be queued for scraping.
        ***REMOVED***
        scheme, netloc, path, _, _, _ = urlparse(link***REMOVED***
        if path.endswith(self.source_extensions + self.binary_extensions +
                         self.excluded_extensions***REMOVED***:
            result = False
        elif self.skip_externals and not link.startswith(self.base_url***REMOVED***:
            result = False
        elif not referrer.startswith(self.base_url***REMOVED***:
            result = False
        elif rel not in ('homepage', 'download'***REMOVED***:
            result = False
        elif scheme not in ('http', 'https', 'ftp'***REMOVED***:
            result = False
        elif self._is_platform_dependent(link***REMOVED***:
            result = False
        else:
            host = netloc.split(':', 1***REMOVED***[0***REMOVED***
            if host.lower(***REMOVED*** == 'localhost':
                result = False
            else:
                result = True
        logger.debug('should_queue: %s (%s***REMOVED*** from %s -> %s', link, rel,
                     referrer, result***REMOVED***
        return result

    def _fetch(self***REMOVED***:
        ***REMOVED***
        Get a URL to fetch from the work queue, get the HTML page, examine its
        links for download candidates and candidates for further scraping.

        This is a handy method to run in a thread.
        ***REMOVED***
        while True:
            url = self._to_fetch.get(***REMOVED***
            ***REMOVED***
                if url:
                    page = self.get_page(url***REMOVED***
                    if page is None:    # e.g. after an error
                        continue
                    for link, rel in page.links:
                        if link not in self._seen:
                            self._seen.add(link***REMOVED***
                            if (not self._process_download(link***REMOVED*** and
                                self._should_queue(link, url, rel***REMOVED******REMOVED***:
                                logger.debug('Queueing %s from %s', link, url***REMOVED***
                                self._to_fetch.put(link***REMOVED***
            finally:
                # always do this, to avoid hangs :-***REMOVED***
                self._to_fetch.task_done(***REMOVED***
            if not url:
                #logger.debug('Sentinel seen, quitting.'***REMOVED***
                break

    def get_page(self, url***REMOVED***:
        ***REMOVED***
        Get the HTML for an URL, possibly from an in-memory cache.

        XXX TODO Note: this cache is never actually cleared. It's assumed that
        the data won't get stale over the lifetime of a locator instance (not
        necessarily true for the default_locator***REMOVED***.
        ***REMOVED***
        # http://peak.telecommunity.com/DevCenter/EasyInstall#package-index-api
        scheme, netloc, path, _, _, _ = urlparse(url***REMOVED***
        if scheme == 'file' and os.path.isdir(url2pathname(path***REMOVED******REMOVED***:
            url = urljoin(ensure_slash(url***REMOVED***, 'index.html'***REMOVED***

        if url in self._page_cache:
            result = self._page_cache[url***REMOVED***
            logger.debug('Returning %s from cache: %s', url, result***REMOVED***
        else:
            host = netloc.split(':', 1***REMOVED***[0***REMOVED***
            result = None
            if host in self._bad_hosts:
                logger.debug('Skipping %s due to bad host %s', url, host***REMOVED***
            else:
                req = Request(url, headers={'Accept-encoding': 'identity'***REMOVED******REMOVED***
                ***REMOVED***
                    logger.debug('Fetching %s', url***REMOVED***
                    resp = self.opener.open(req, timeout=self.timeout***REMOVED***
                    logger.debug('Fetched %s', url***REMOVED***
                    headers = resp.info(***REMOVED***
                    content_type = headers.get('Content-Type', ''***REMOVED***
                    if HTML_CONTENT_TYPE.match(content_type***REMOVED***:
                        final_url = resp.geturl(***REMOVED***
                        data = resp.read(***REMOVED***
                        encoding = headers.get('Content-Encoding'***REMOVED***
                        if encoding:
                            decoder = self.decoders[encoding***REMOVED***   # fail if not found
                            data = decoder(data***REMOVED***
                        encoding = 'utf-8'
                        m = CHARSET.search(content_type***REMOVED***
                        if m:
                            encoding = m.group(1***REMOVED***
                        ***REMOVED***
                            data = data.decode(encoding***REMOVED***
                        except UnicodeError:  # pragma: no cover
                            data = data.decode('latin-1'***REMOVED***    # fallback
                        result = Page(data, final_url***REMOVED***
                        self._page_cache[final_url***REMOVED*** = result
                except HTTPError as e:
                    if e.code != 404:
                        logger.exception('Fetch failed: %s: %s', url, e***REMOVED***
                except URLError as e:  # pragma: no cover
                    logger.exception('Fetch failed: %s: %s', url, e***REMOVED***
                    with self._lock:
                        self._bad_hosts.add(host***REMOVED***
                except Exception as e:  # pragma: no cover
                    logger.exception('Fetch failed: %s: %s', url, e***REMOVED***
                finally:
                    self._page_cache[url***REMOVED*** = result   # even if None (failure***REMOVED***
        return result

    _distname_re = re.compile('<a href=[^>***REMOVED****>([^<***REMOVED***+***REMOVED***<'***REMOVED***

    def get_distribution_names(self***REMOVED***:
        ***REMOVED***
        Return all the distribution names known to this locator.
        ***REMOVED***
        result = set(***REMOVED***
        page = self.get_page(self.base_url***REMOVED***
        if not page:
            raise DistlibException('Unable to get %s' % self.base_url***REMOVED***
        for match in self._distname_re.finditer(page.data***REMOVED***:
            result.add(match.group(1***REMOVED******REMOVED***
        return result

class DirectoryLocator(Locator***REMOVED***:
    ***REMOVED***
    This class locates distributions in a directory tree.
    ***REMOVED***

    def __init__(self, path, **kwargs***REMOVED***:
        ***REMOVED***
        Initialise an instance.
        :param path: The root of the directory tree to search.
        :param kwargs: Passed to the superclass constructor,
                       except for:
                       * recursive - if True (the default***REMOVED***, subdirectories are
                         recursed into. If False, only the top-level directory
                         is searched,
        ***REMOVED***
        self.recursive = kwargs.pop('recursive', True***REMOVED***
        super(DirectoryLocator, self***REMOVED***.__init__(**kwargs***REMOVED***
        path = os.path.abspath(path***REMOVED***
        if not os.path.isdir(path***REMOVED***:  # pragma: no cover
            raise DistlibException('Not a directory: %r' % path***REMOVED***
        self.base_dir = path

    def should_include(self, filename, parent***REMOVED***:
        ***REMOVED***
        Should a filename be considered as a candidate for a distribution
        archive? As well as the filename, the directory which contains it
        is provided, though not used by the current implementation.
        ***REMOVED***
        return filename.endswith(self.downloadable_extensions***REMOVED***

    def _get_project(self, name***REMOVED***:
        result = {'urls': {***REMOVED***, 'digests': {***REMOVED******REMOVED***
        for root, dirs, files in os.walk(self.base_dir***REMOVED***:
            for fn in files:
                if self.should_include(fn, root***REMOVED***:
                    fn = os.path.join(root, fn***REMOVED***
                    url = urlunparse(('file', '',
                                      pathname2url(os.path.abspath(fn***REMOVED******REMOVED***,
                                      '', '', ''***REMOVED******REMOVED***
                    info = self.convert_url_to_download_info(url, name***REMOVED***
                    if info:
                        self._update_version_data(result, info***REMOVED***
            if not self.recursive:
                break
        return result

    def get_distribution_names(self***REMOVED***:
        ***REMOVED***
        Return all the distribution names known to this locator.
        ***REMOVED***
        result = set(***REMOVED***
        for root, dirs, files in os.walk(self.base_dir***REMOVED***:
            for fn in files:
                if self.should_include(fn, root***REMOVED***:
                    fn = os.path.join(root, fn***REMOVED***
                    url = urlunparse(('file', '',
                                      pathname2url(os.path.abspath(fn***REMOVED******REMOVED***,
                                      '', '', ''***REMOVED******REMOVED***
                    info = self.convert_url_to_download_info(url, None***REMOVED***
                    if info:
                        result.add(info['name'***REMOVED******REMOVED***
            if not self.recursive:
                break
        return result

class JSONLocator(Locator***REMOVED***:
    ***REMOVED***
    This locator uses special extended metadata (not available on PyPI***REMOVED*** and is
    the basis of performant dependency resolution in distlib. Other locators
    require archive downloads before dependencies can be determined! As you
    might imagine, that can be slow.
    ***REMOVED***
    def get_distribution_names(self***REMOVED***:
        ***REMOVED***
        Return all the distribution names known to this locator.
        ***REMOVED***
        raise NotImplementedError('Not available from this locator'***REMOVED***

    def _get_project(self, name***REMOVED***:
        result = {'urls': {***REMOVED***, 'digests': {***REMOVED******REMOVED***
        data = get_project_data(name***REMOVED***
        if data:
            for info in data.get('files', [***REMOVED******REMOVED***:
                if info['ptype'***REMOVED*** != 'sdist' or info['pyversion'***REMOVED*** != 'source':
                    continue
                # We don't store summary in project metadata as it makes
                # the data bigger for no benefit during dependency
                # resolution
                dist = make_dist(data['name'***REMOVED***, info['version'***REMOVED***,
                                 summary=data.get('summary',
                                                  'Placeholder for summary'***REMOVED***,
                                 scheme=self.scheme***REMOVED***
                md = dist.metadata
                md.source_url = info['url'***REMOVED***
                # TODO SHA256 digest
                if 'digest' in info and info['digest'***REMOVED***:
                    dist.digest = ('md5', info['digest'***REMOVED******REMOVED***
                md.dependencies = info.get('requirements', {***REMOVED******REMOVED***
                dist.exports = info.get('exports', {***REMOVED******REMOVED***
                result[dist.version***REMOVED*** = dist
                result['urls'***REMOVED***.setdefault(dist.version, set(***REMOVED******REMOVED***.add(info['url'***REMOVED******REMOVED***
        return result

class DistPathLocator(Locator***REMOVED***:
    ***REMOVED***
    This locator finds installed distributions in a path. It can be useful for
    adding to an :class:`AggregatingLocator`.
    ***REMOVED***
    def __init__(self, distpath, **kwargs***REMOVED***:
        ***REMOVED***
        Initialise an instance.

        :param distpath: A :class:`DistributionPath` instance to search.
        ***REMOVED***
        super(DistPathLocator, self***REMOVED***.__init__(**kwargs***REMOVED***
        assert isinstance(distpath, DistributionPath***REMOVED***
        self.distpath = distpath

    def _get_project(self, name***REMOVED***:
        dist = self.distpath.get_distribution(name***REMOVED***
        if dist is None:
            result = {'urls': {***REMOVED***, 'digests': {***REMOVED******REMOVED***
        else:
            result = {
                dist.version: dist,
                'urls': {dist.version: set([dist.source_url***REMOVED******REMOVED******REMOVED***,
                'digests': {dist.version: set([None***REMOVED******REMOVED******REMOVED***
        ***REMOVED***
        return result


class AggregatingLocator(Locator***REMOVED***:
    ***REMOVED***
    This class allows you to chain and/or merge a list of locators.
    ***REMOVED***
    def __init__(self, *locators, **kwargs***REMOVED***:
        ***REMOVED***
        Initialise an instance.

        :param locators: The list of locators to search.
        :param kwargs: Passed to the superclass constructor,
                       except for:
                       * merge - if False (the default***REMOVED***, the first successful
                         search from any of the locators is returned. If True,
                         the results from all locators are merged (this can be
                         slow***REMOVED***.
        ***REMOVED***
        self.merge = kwargs.pop('merge', False***REMOVED***
        self.locators = locators
        super(AggregatingLocator, self***REMOVED***.__init__(**kwargs***REMOVED***

    def clear_cache(self***REMOVED***:
        super(AggregatingLocator, self***REMOVED***.clear_cache(***REMOVED***
        for locator in self.locators:
            locator.clear_cache(***REMOVED***

    def _set_scheme(self, value***REMOVED***:
        self._scheme = value
        for locator in self.locators:
            locator.scheme = value

    scheme = property(Locator.scheme.fget, _set_scheme***REMOVED***

    def _get_project(self, name***REMOVED***:
        result = {***REMOVED***
        for locator in self.locators:
            d = locator.get_project(name***REMOVED***
            if d:
                if self.merge:
                    files = result.get('urls', {***REMOVED******REMOVED***
                    digests = result.get('digests', {***REMOVED******REMOVED***
                    # next line could overwrite result['urls'***REMOVED***, result['digests'***REMOVED***
                    result.update(d***REMOVED***
                    df = result.get('urls'***REMOVED***
                    if files and df:
                        for k, v in files.items(***REMOVED***:
                            if k in df:
                                df[k***REMOVED*** |= v
                            else:
                                df[k***REMOVED*** = v
                    dd = result.get('digests'***REMOVED***
                    if digests and dd:
                        dd.update(digests***REMOVED***
                else:
                    # See issue #18. If any dists are found and we're looking
                    # for specific constraints, we only return something if
                    # a match is found. For example, if a DirectoryLocator
                    # returns just foo (1.0***REMOVED*** while we're looking for
                    # foo (>= 2.0***REMOVED***, we'll pretend there was nothing there so
                    # that subsequent locators can be queried. Otherwise we
                    # would just return foo (1.0***REMOVED*** which would then lead to a
                    # failure to find foo (>= 2.0***REMOVED***, because other locators
                    # weren't searched. Note that this only matters when
                    # merge=False.
                    if self.matcher is None:
                        found = True
                    else:
                        found = False
                        for k in d:
                            if self.matcher.match(k***REMOVED***:
                                found = True
                                break
                    if found:
                        result = d
                        break
        return result

    def get_distribution_names(self***REMOVED***:
        ***REMOVED***
        Return all the distribution names known to this locator.
        ***REMOVED***
        result = set(***REMOVED***
        for locator in self.locators:
            ***REMOVED***
                result |= locator.get_distribution_names(***REMOVED***
            except NotImplementedError:
                pass
        return result


# We use a legacy scheme simply because most of the dists on PyPI use legacy
# versions which don't conform to PEP 426 / PEP 440.
default_locator = AggregatingLocator(
                    JSONLocator(***REMOVED***,
                    SimpleScrapingLocator('https://pypi.python.org/simple/',
                                          timeout=3.0***REMOVED***,
                    scheme='legacy'***REMOVED***

locate = default_locator.locate

NAME_VERSION_RE = re.compile(r'(?P<name>[\w-***REMOVED***+***REMOVED***\s*'
                             r'\(\s*(==\s****REMOVED***?(?P<ver>[^***REMOVED******REMOVED***+***REMOVED***\***REMOVED***$'***REMOVED***

class DependencyFinder(object***REMOVED***:
    ***REMOVED***
    Locate dependencies for distributions.
    ***REMOVED***

    def __init__(self, locator=None***REMOVED***:
        ***REMOVED***
        Initialise an instance, using the specified locator
        to locate distributions.
        ***REMOVED***
        self.locator = locator or default_locator
        self.scheme = get_scheme(self.locator.scheme***REMOVED***

    def add_distribution(self, dist***REMOVED***:
        ***REMOVED***
        Add a distribution to the finder. This will update internal information
        about who provides what.
        :param dist: The distribution to add.
        ***REMOVED***
        logger.debug('adding distribution %s', dist***REMOVED***
        name = dist.key
        self.dists_by_name[name***REMOVED*** = dist
        self.dists[(name, dist.version***REMOVED******REMOVED*** = dist
        for p in dist.provides:
            name, version = parse_name_and_version(p***REMOVED***
            logger.debug('Add to provided: %s, %s, %s', name, version, dist***REMOVED***
            self.provided.setdefault(name, set(***REMOVED******REMOVED***.add((version, dist***REMOVED******REMOVED***

    def remove_distribution(self, dist***REMOVED***:
        ***REMOVED***
        Remove a distribution from the finder. This will update internal
        information about who provides what.
        :param dist: The distribution to remove.
        ***REMOVED***
        logger.debug('removing distribution %s', dist***REMOVED***
        name = dist.key
        del self.dists_by_name[name***REMOVED***
        del self.dists[(name, dist.version***REMOVED******REMOVED***
        for p in dist.provides:
            name, version = parse_name_and_version(p***REMOVED***
            logger.debug('Remove from provided: %s, %s, %s', name, version, dist***REMOVED***
            s = self.provided[name***REMOVED***
            s.remove((version, dist***REMOVED******REMOVED***
            if not s:
                del self.provided[name***REMOVED***

    def get_matcher(self, reqt***REMOVED***:
        ***REMOVED***
        Get a version matcher for a requirement.
        :param reqt: The requirement
        :type reqt: str
        :return: A version matcher (an instance of
                 :class:`distlib.version.Matcher`***REMOVED***.
        ***REMOVED***
        ***REMOVED***
            matcher = self.scheme.matcher(reqt***REMOVED***
        except UnsupportedVersionError:  # pragma: no cover
            # XXX compat-mode if cannot read the version
            name = reqt.split(***REMOVED***[0***REMOVED***
            matcher = self.scheme.matcher(name***REMOVED***
        return matcher

    def find_providers(self, reqt***REMOVED***:
        ***REMOVED***
        Find the distributions which can fulfill a requirement.

        :param reqt: The requirement.
         :type reqt: str
        :return: A set of distribution which can fulfill the requirement.
        ***REMOVED***
        matcher = self.get_matcher(reqt***REMOVED***
        name = matcher.key   # case-insensitive
        result = set(***REMOVED***
        provided = self.provided
        if name in provided:
            for version, provider in provided[name***REMOVED***:
                ***REMOVED***
                    match = matcher.match(version***REMOVED***
                except UnsupportedVersionError:
                    match = False

                if match:
                    result.add(provider***REMOVED***
                    break
        return result

    def try_to_replace(self, provider, other, problems***REMOVED***:
        ***REMOVED***
        Attempt to replace one provider with another. This is typically used
        when resolving dependencies from multiple sources, e.g. A requires
        (B >= 1.0***REMOVED*** while C requires (B >= 1.1***REMOVED***.

        For successful replacement, ``provider`` must meet all the requirements
        which ``other`` fulfills.

        :param provider: The provider we are trying to replace with.
        :param other: The provider we're trying to replace.
        :param problems: If False is returned, this will contain what
                         problems prevented replacement. This is currently
                         a tuple of the literal string 'cantreplace',
                         ``provider``, ``other``  and the set of requirements
                         that ``provider`` couldn't fulfill.
        :return: True if we can replace ``other`` with ``provider``, else
                 False.
        ***REMOVED***
        rlist = self.reqts[other***REMOVED***
        unmatched = set(***REMOVED***
        for s in rlist:
            matcher = self.get_matcher(s***REMOVED***
            if not matcher.match(provider.version***REMOVED***:
                unmatched.add(s***REMOVED***
        if unmatched:
            # can't replace other with provider
            problems.add(('cantreplace', provider, other,
                          frozenset(unmatched***REMOVED******REMOVED******REMOVED***
            result = False
        else:
            # can replace other with provider
            self.remove_distribution(other***REMOVED***
            del self.reqts[other***REMOVED***
            for s in rlist:
                self.reqts.setdefault(provider, set(***REMOVED******REMOVED***.add(s***REMOVED***
            self.add_distribution(provider***REMOVED***
            result = True
        return result

    def find(self, requirement, meta_extras=None, prereleases=False***REMOVED***:
        ***REMOVED***
        Find a distribution and all distributions it depends on.

        :param requirement: The requirement specifying the distribution to
                            find, or a Distribution instance.
        :param meta_extras: A list of meta extras such as :test:, :build: and
                            so on.
        :param prereleases: If ``True``, allow pre-release versions to be
                            returned - otherwise, don't return prereleases
                            unless they're all that's available.

        Return a set of :class:`Distribution` instances and a set of
        problems.

        The distributions returned should be such that they have the
        :attr:`required` attribute set to ``True`` if they were
        from the ``requirement`` passed to ``find(***REMOVED***``, and they have the
        :attr:`build_time_dependency` attribute set to ``True`` unless they
        are post-installation dependencies of the ``requirement``.

        The problems should be a tuple consisting of the string
        ``'unsatisfied'`` and the requirement which couldn't be satisfied
        by any distribution known to the locator.
        ***REMOVED***

        self.provided = {***REMOVED***
        self.dists = {***REMOVED***
        self.dists_by_name = {***REMOVED***
        self.reqts = {***REMOVED***

        meta_extras = set(meta_extras or [***REMOVED******REMOVED***
        if ':*:' in meta_extras:
            meta_extras.remove(':*:'***REMOVED***
            # :meta: and :run: are implicitly included
            meta_extras |= set([':test:', ':build:', ':dev:'***REMOVED******REMOVED***

        if isinstance(requirement, Distribution***REMOVED***:
            dist = odist = requirement
            logger.debug('passed %s as requirement', odist***REMOVED***
        else:
            dist = odist = self.locator.locate(requirement,
                                               prereleases=prereleases***REMOVED***
            if dist is None:
                raise DistlibException('Unable to locate %r' % requirement***REMOVED***
            logger.debug('located %s', odist***REMOVED***
        dist.requested = True
        problems = set(***REMOVED***
        todo = set([dist***REMOVED******REMOVED***
        install_dists = set([odist***REMOVED******REMOVED***
        while todo:
            dist = todo.pop(***REMOVED***
            name = dist.key     # case-insensitive
            if name not in self.dists_by_name:
                self.add_distribution(dist***REMOVED***
            else:
                #import pdb; pdb.set_trace(***REMOVED***
                other = self.dists_by_name[name***REMOVED***
                if other != dist:
                    self.try_to_replace(dist, other, problems***REMOVED***

            ireqts = dist.run_requires | dist.meta_requires
            sreqts = dist.build_requires
            ereqts = set(***REMOVED***
            if dist in install_dists:
                for key in ('test', 'build', 'dev'***REMOVED***:
                    e = ':%s:' % key
                    if e in meta_extras:
                        ereqts |= getattr(dist, '%s_requires' % key***REMOVED***
            all_reqts = ireqts | sreqts | ereqts
            for r in all_reqts:
                providers = self.find_providers(r***REMOVED***
                if not providers:
                    logger.debug('No providers found for %r', r***REMOVED***
                    provider = self.locator.locate(r, prereleases=prereleases***REMOVED***
                    # If no provider is found and we didn't consider
                    # prereleases, consider them now.
                    if provider is None and not prereleases:
                        provider = self.locator.locate(r, prereleases=True***REMOVED***
                    if provider is None:
                        logger.debug('Cannot satisfy %r', r***REMOVED***
                        problems.add(('unsatisfied', r***REMOVED******REMOVED***
                    else:
                        n, v = provider.key, provider.version
                        if (n, v***REMOVED*** not in self.dists:
                            todo.add(provider***REMOVED***
                        providers.add(provider***REMOVED***
                        if r in ireqts and dist in install_dists:
                            install_dists.add(provider***REMOVED***
                            logger.debug('Adding %s to install_dists',
                                         provider.name_and_version***REMOVED***
                for p in providers:
                    name = p.key
                    if name not in self.dists_by_name:
                        self.reqts.setdefault(p, set(***REMOVED******REMOVED***.add(r***REMOVED***
                    else:
                        other = self.dists_by_name[name***REMOVED***
                        if other != p:
                            # see if other can be replaced by p
                            self.try_to_replace(p, other, problems***REMOVED***

        dists = set(self.dists.values(***REMOVED******REMOVED***
        for dist in dists:
            dist.build_time_dependency = dist not in install_dists
            if dist.build_time_dependency:
                logger.debug('%s is a build-time dependency only.',
                             dist.name_and_version***REMOVED***
        logger.debug('find done for %s', odist***REMOVED***
        return dists, problems
