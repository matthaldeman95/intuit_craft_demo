from __future__ import absolute_import, division, unicode_literals
from pip._vendor.six import text_type
from pip._vendor.six.moves import http_client

import codecs
import re

from .constants import EOF, spaceCharacters, asciiLetters, asciiUppercase
from .constants import encodings, ReparseException
from . import utils

from io import StringIO

***REMOVED***
    from io import BytesIO
except ImportError:
    BytesIO = StringIO

***REMOVED***
    from io import BufferedIOBase
except ImportError:
    class BufferedIOBase(object***REMOVED***:
        pass

# Non-unicode versions of constants for use in the pre-parser
spaceCharactersBytes = frozenset([item.encode("ascii"***REMOVED*** for item in spaceCharacters***REMOVED******REMOVED***
asciiLettersBytes = frozenset([item.encode("ascii"***REMOVED*** for item in asciiLetters***REMOVED******REMOVED***
asciiUppercaseBytes = frozenset([item.encode("ascii"***REMOVED*** for item in asciiUppercase***REMOVED******REMOVED***
spacesAngleBrackets = spaceCharactersBytes | frozenset([b">", b"<"***REMOVED******REMOVED***


invalid_unicode_no_surrogate = "[\u0001-\u0008\u000B\u000E-\u001F\u007F-\u009F\uFDD0-\uFDEF\uFFFE\uFFFF\U0001FFFE\U0001FFFF\U0002FFFE\U0002FFFF\U0003FFFE\U0003FFFF\U0004FFFE\U0004FFFF\U0005FFFE\U0005FFFF\U0006FFFE\U0006FFFF\U0007FFFE\U0007FFFF\U0008FFFE\U0008FFFF\U0009FFFE\U0009FFFF\U000AFFFE\U000AFFFF\U000BFFFE\U000BFFFF\U000CFFFE\U000CFFFF\U000DFFFE\U000DFFFF\U000EFFFE\U000EFFFF\U000FFFFE\U000FFFFF\U0010FFFE\U0010FFFF***REMOVED***"

if utils.supports_lone_surrogates:
    # Use one extra step of indirection and create surrogates with
    # unichr. Not using this indirection would introduce an illegal
    # unicode literal on platforms not supporting such lone
    # surrogates.
    invalid_unicode_re = re.compile(invalid_unicode_no_surrogate +
                                    eval('"\\uD800-\\uDFFF"'***REMOVED******REMOVED***
else:
    invalid_unicode_re = re.compile(invalid_unicode_no_surrogate***REMOVED***

non_bmp_invalid_codepoints = set([0x1FFFE, 0x1FFFF, 0x2FFFE, 0x2FFFF, 0x3FFFE,
                                  0x3FFFF, 0x4FFFE, 0x4FFFF, 0x5FFFE, 0x5FFFF,
                                  0x6FFFE, 0x6FFFF, 0x7FFFE, 0x7FFFF, 0x8FFFE,
                                  0x8FFFF, 0x9FFFE, 0x9FFFF, 0xAFFFE, 0xAFFFF,
                                  0xBFFFE, 0xBFFFF, 0xCFFFE, 0xCFFFF, 0xDFFFE,
                                  0xDFFFF, 0xEFFFE, 0xEFFFF, 0xFFFFE, 0xFFFFF,
                                  0x10FFFE, 0x10FFFF***REMOVED******REMOVED***

ascii_punctuation_re = re.compile("[\u0009-\u000D\u0020-\u002F\u003A-\u0040\u005B-\u0060\u007B-\u007E***REMOVED***"***REMOVED***

# Cache for charsUntil(***REMOVED***
charsUntilRegEx = {***REMOVED***


class BufferedStream(object***REMOVED***:
    ***REMOVED***Buffering for streams that do not have buffering of their own

    The buffer is implemented as a list of chunks on the assumption that
    joining many strings will be slow since it is O(n**2***REMOVED***
    ***REMOVED***

    def __init__(self, stream***REMOVED***:
        self.stream = stream
        self.buffer = [***REMOVED***
        self.position = [-1, 0***REMOVED***  # chunk number, offset

    def tell(self***REMOVED***:
        pos = 0
        for chunk in self.buffer[:self.position[0***REMOVED******REMOVED***:
            pos += len(chunk***REMOVED***
        pos += self.position[1***REMOVED***
        return pos

    def seek(self, pos***REMOVED***:
        assert pos <= self._bufferedBytes(***REMOVED***
        offset = pos
        i = 0
        while len(self.buffer[i***REMOVED******REMOVED*** < offset:
            offset -= len(self.buffer[i***REMOVED******REMOVED***
            i += 1
        self.position = [i, offset***REMOVED***

    def read(self, bytes***REMOVED***:
        if not self.buffer:
            return self._readStream(bytes***REMOVED***
        elif (self.position[0***REMOVED*** == len(self.buffer***REMOVED*** and
              self.position[1***REMOVED*** == len(self.buffer[-1***REMOVED******REMOVED******REMOVED***:
            return self._readStream(bytes***REMOVED***
        else:
            return self._readFromBuffer(bytes***REMOVED***

    def _bufferedBytes(self***REMOVED***:
        return sum([len(item***REMOVED*** for item in self.buffer***REMOVED******REMOVED***

    def _readStream(self, bytes***REMOVED***:
        data = self.stream.read(bytes***REMOVED***
        self.buffer.append(data***REMOVED***
        self.position[0***REMOVED*** += 1
        self.position[1***REMOVED*** = len(data***REMOVED***
        return data

    def _readFromBuffer(self, bytes***REMOVED***:
        remainingBytes = bytes
        rv = [***REMOVED***
        bufferIndex = self.position[0***REMOVED***
        bufferOffset = self.position[1***REMOVED***
        while bufferIndex < len(self.buffer***REMOVED*** and remainingBytes != 0:
            assert remainingBytes > 0
            bufferedData = self.buffer[bufferIndex***REMOVED***

            if remainingBytes <= len(bufferedData***REMOVED*** - bufferOffset:
                bytesToRead = remainingBytes
                self.position = [bufferIndex, bufferOffset + bytesToRead***REMOVED***
            else:
                bytesToRead = len(bufferedData***REMOVED*** - bufferOffset
                self.position = [bufferIndex, len(bufferedData***REMOVED******REMOVED***
                bufferIndex += 1
            rv.append(bufferedData[bufferOffset:bufferOffset + bytesToRead***REMOVED******REMOVED***
            remainingBytes -= bytesToRead

            bufferOffset = 0

        if remainingBytes:
            rv.append(self._readStream(remainingBytes***REMOVED******REMOVED***

        return b"".join(rv***REMOVED***


def HTMLInputStream(source, encoding=None, parseMeta=True, chardet=True***REMOVED***:
    if isinstance(source, http_client.HTTPResponse***REMOVED***:
        # Work around Python bug #20007: read(0***REMOVED*** closes the connection.
        # http://bugs.python.org/issue20007
        isUnicode = False
    elif hasattr(source, "read"***REMOVED***:
        isUnicode = isinstance(source.read(0***REMOVED***, text_type***REMOVED***
    else:
        isUnicode = isinstance(source, text_type***REMOVED***

    if isUnicode:
        if encoding is not None:
            raise TypeError("Cannot explicitly set an encoding with a unicode string"***REMOVED***

        return HTMLUnicodeInputStream(source***REMOVED***
    else:
        return HTMLBinaryInputStream(source, encoding, parseMeta, chardet***REMOVED***


class HTMLUnicodeInputStream(object***REMOVED***:
    ***REMOVED***Provides a unicode stream of characters to the HTMLTokenizer.

    This class takes care of character encoding and removing or replacing
    incorrect byte-sequences and also provides column and line tracking.

    ***REMOVED***

    _defaultChunkSize = 10240

    def __init__(self, source***REMOVED***:
        ***REMOVED***Initialises the HTMLInputStream.

        HTMLInputStream(source, [encoding***REMOVED******REMOVED*** -> Normalized stream from source
        for use by html5lib.

        source can be either a file-object, local filename or a string.

        The optional encoding parameter must be a string that indicates
        the encoding.  If specified, that encoding will be used,
        regardless of any BOM or later declaration (such as in a meta
        element***REMOVED***

        parseMeta - Look for a <meta> element containing encoding information

        ***REMOVED***

        if not utils.supports_lone_surrogates:
            # Such platforms will have already checked for such
            # surrogate errors, so no need to do this checking.
            self.reportCharacterErrors = None
            self.replaceCharactersRegexp = None
        elif len("\U0010FFFF"***REMOVED*** == 1:
            self.reportCharacterErrors = self.characterErrorsUCS4
            self.replaceCharactersRegexp = re.compile(eval('"[\\uD800-\\uDFFF***REMOVED***"'***REMOVED******REMOVED***
        else:
            self.reportCharacterErrors = self.characterErrorsUCS2
            self.replaceCharactersRegexp = re.compile(
                eval('"([\\uD800-\\uDBFF***REMOVED***(?![\\uDC00-\\uDFFF***REMOVED******REMOVED***|(?<![\\uD800-\\uDBFF***REMOVED******REMOVED***[\\uDC00-\\uDFFF***REMOVED******REMOVED***"'***REMOVED******REMOVED***

        # List of where new lines occur
        self.newLines = [0***REMOVED***

        self.charEncoding = ("utf-8", "certain"***REMOVED***
        self.dataStream = self.openStream(source***REMOVED***

        self.reset(***REMOVED***

    def reset(self***REMOVED***:
        self.chunk = ""
        self.chunkSize = 0
        self.chunkOffset = 0
        self.errors = [***REMOVED***

        # number of (complete***REMOVED*** lines in previous chunks
        self.prevNumLines = 0
        # number of columns in the last line of the previous chunk
        self.prevNumCols = 0

        # Deal with CR LF and surrogates split over chunk boundaries
        self._bufferedCharacter = None

    def openStream(self, source***REMOVED***:
        ***REMOVED***Produces a file object from source.

        source can be either a file object, local filename or a string.

        ***REMOVED***
        # Already a file object
        if hasattr(source, 'read'***REMOVED***:
            stream = source
        else:
            stream = StringIO(source***REMOVED***

        return stream

    def _position(self, offset***REMOVED***:
        chunk = self.chunk
        nLines = chunk.count('\n', 0, offset***REMOVED***
        positionLine = self.prevNumLines + nLines
        lastLinePos = chunk.rfind('\n', 0, offset***REMOVED***
        if lastLinePos == -1:
            positionColumn = self.prevNumCols + offset
        else:
            positionColumn = offset - (lastLinePos + 1***REMOVED***
        return (positionLine, positionColumn***REMOVED***

    def position(self***REMOVED***:
        ***REMOVED***Returns (line, col***REMOVED*** of the current position in the stream.***REMOVED***
        line, col = self._position(self.chunkOffset***REMOVED***
        return (line + 1, col***REMOVED***

    def char(self***REMOVED***:
        ***REMOVED*** Read one character from the stream or queue if available. Return
            EOF when EOF is reached.
        ***REMOVED***
        # Read a new chunk from the input stream if necessary
        if self.chunkOffset >= self.chunkSize:
            if not self.readChunk(***REMOVED***:
                return EOF

        chunkOffset = self.chunkOffset
        char = self.chunk[chunkOffset***REMOVED***
        self.chunkOffset = chunkOffset + 1

        return char

    def readChunk(self, chunkSize=None***REMOVED***:
        if chunkSize is None:
            chunkSize = self._defaultChunkSize

        self.prevNumLines, self.prevNumCols = self._position(self.chunkSize***REMOVED***

        self.chunk = ""
        self.chunkSize = 0
        self.chunkOffset = 0

        data = self.dataStream.read(chunkSize***REMOVED***

        # Deal with CR LF and surrogates broken across chunks
        if self._bufferedCharacter:
            data = self._bufferedCharacter + data
            self._bufferedCharacter = None
        elif not data:
            # We have no more data, bye-bye stream
            return False

        if len(data***REMOVED*** > 1:
            lastv = ord(data[-1***REMOVED******REMOVED***
            if lastv == 0x0D or 0xD800 <= lastv <= 0xDBFF:
                self._bufferedCharacter = data[-1***REMOVED***
                data = data[:-1***REMOVED***

        if self.reportCharacterErrors:
            self.reportCharacterErrors(data***REMOVED***

            # Replace invalid characters
            # Note U+0000 is dealt with in the tokenizer
            data = self.replaceCharactersRegexp.sub("\ufffd", data***REMOVED***

        data = data.replace("\r\n", "\n"***REMOVED***
        data = data.replace("\r", "\n"***REMOVED***

        self.chunk = data
        self.chunkSize = len(data***REMOVED***

        return True

    def characterErrorsUCS4(self, data***REMOVED***:
        for i in range(len(invalid_unicode_re.findall(data***REMOVED******REMOVED******REMOVED***:
            self.errors.append("invalid-codepoint"***REMOVED***

    def characterErrorsUCS2(self, data***REMOVED***:
        # Someone picked the wrong compile option
        # You lose
        skip = False
        for match in invalid_unicode_re.finditer(data***REMOVED***:
            if skip:
                continue
            codepoint = ord(match.group(***REMOVED******REMOVED***
            pos = match.start(***REMOVED***
            # Pretty sure there should be endianness issues here
            if utils.isSurrogatePair(data[pos:pos + 2***REMOVED******REMOVED***:
                # We have a surrogate pair!
                char_val = utils.surrogatePairToCodepoint(data[pos:pos + 2***REMOVED******REMOVED***
                if char_val in non_bmp_invalid_codepoints:
                    self.errors.append("invalid-codepoint"***REMOVED***
                skip = True
            elif (codepoint >= 0xD800 and codepoint <= 0xDFFF and
                  pos == len(data***REMOVED*** - 1***REMOVED***:
                self.errors.append("invalid-codepoint"***REMOVED***
            else:
                skip = False
                self.errors.append("invalid-codepoint"***REMOVED***

    def charsUntil(self, characters, opposite=False***REMOVED***:
        ***REMOVED*** Returns a string of characters from the stream up to but not
        including any character in 'characters' or EOF. 'characters' must be
        a container that supports the 'in' method and iteration over its
        characters.
        ***REMOVED***

        # Use a cache of regexps to find the required characters
        ***REMOVED***
            chars = charsUntilRegEx[(characters, opposite***REMOVED******REMOVED***
        except KeyError:
            if __debug__:
                for c in characters:
                    assert(ord(c***REMOVED*** < 128***REMOVED***
            regex = "".join(["\\x%02x" % ord(c***REMOVED*** for c in characters***REMOVED******REMOVED***
            if not opposite:
                regex = "^%s" % regex
            chars = charsUntilRegEx[(characters, opposite***REMOVED******REMOVED*** = re.compile("[%s***REMOVED***+" % regex***REMOVED***

        rv = [***REMOVED***

        while True:
            # Find the longest matching prefix
            m = chars.match(self.chunk, self.chunkOffset***REMOVED***
            if m is None:
                # If nothing matched, and it wasn't because we ran out of chunk,
                # then stop
                if self.chunkOffset != self.chunkSize:
                    break
            else:
                end = m.end(***REMOVED***
                # If not the whole chunk matched, return everything
                # up to the part that didn't match
                if end != self.chunkSize:
                    rv.append(self.chunk[self.chunkOffset:end***REMOVED******REMOVED***
                    self.chunkOffset = end
                    break
            # If the whole remainder of the chunk matched,
            # use it all and read the next chunk
            rv.append(self.chunk[self.chunkOffset:***REMOVED******REMOVED***
            if not self.readChunk(***REMOVED***:
                # Reached EOF
                break

        r = "".join(rv***REMOVED***
        return r

    def unget(self, char***REMOVED***:
        # Only one character is allowed to be ungotten at once - it must
        # be consumed again before any further call to unget
        if char is not None:
            if self.chunkOffset == 0:
                # unget is called quite rarely, so it's a good idea to do
                # more work here if it saves a bit of work in the frequently
                # called char and charsUntil.
                # So, just prepend the ungotten character onto the current
                # chunk:
                self.chunk = char + self.chunk
                self.chunkSize += 1
            else:
                self.chunkOffset -= 1
                assert self.chunk[self.chunkOffset***REMOVED*** == char


class HTMLBinaryInputStream(HTMLUnicodeInputStream***REMOVED***:
    ***REMOVED***Provides a unicode stream of characters to the HTMLTokenizer.

    This class takes care of character encoding and removing or replacing
    incorrect byte-sequences and also provides column and line tracking.

    ***REMOVED***

    def __init__(self, source, encoding=None, parseMeta=True, chardet=True***REMOVED***:
        ***REMOVED***Initialises the HTMLInputStream.

        HTMLInputStream(source, [encoding***REMOVED******REMOVED*** -> Normalized stream from source
        for use by html5lib.

        source can be either a file-object, local filename or a string.

        The optional encoding parameter must be a string that indicates
        the encoding.  If specified, that encoding will be used,
        regardless of any BOM or later declaration (such as in a meta
        element***REMOVED***

        parseMeta - Look for a <meta> element containing encoding information

        ***REMOVED***
        # Raw Stream - for unicode objects this will encode to utf-8 and set
        #              self.charEncoding as appropriate
        self.rawStream = self.openStream(source***REMOVED***

        HTMLUnicodeInputStream.__init__(self, self.rawStream***REMOVED***

        self.charEncoding = (codecName(encoding***REMOVED***, "certain"***REMOVED***

        # Encoding Information
        # Number of bytes to use when looking for a meta element with
        # encoding information
        self.numBytesMeta = 512
        # Number of bytes to use when using detecting encoding using chardet
        self.numBytesChardet = 100
        # Encoding to use if no other information can be found
        self.defaultEncoding = "windows-1252"

        # Detect encoding iff no explicit "transport level" encoding is supplied
        if (self.charEncoding[0***REMOVED*** is None***REMOVED***:
            self.charEncoding = self.detectEncoding(parseMeta, chardet***REMOVED***

        # Call superclass
        self.reset(***REMOVED***

    def reset(self***REMOVED***:
        self.dataStream = codecs.getreader(self.charEncoding[0***REMOVED******REMOVED***(self.rawStream,
                                                                 'replace'***REMOVED***
        HTMLUnicodeInputStream.reset(self***REMOVED***

    def openStream(self, source***REMOVED***:
        ***REMOVED***Produces a file object from source.

        source can be either a file object, local filename or a string.

        ***REMOVED***
        # Already a file object
        if hasattr(source, 'read'***REMOVED***:
            stream = source
        else:
            stream = BytesIO(source***REMOVED***

        ***REMOVED***
            stream.seek(stream.tell(***REMOVED******REMOVED***
        ***REMOVED***
            stream = BufferedStream(stream***REMOVED***

        return stream

    def detectEncoding(self, parseMeta=True, chardet=True***REMOVED***:
        # First look for a BOM
        # This will also read past the BOM if present
        encoding = self.detectBOM(***REMOVED***
        confidence = "certain"
        # If there is no BOM need to look for meta elements with encoding
        # information
        if encoding is None and parseMeta:
            encoding = self.detectEncodingMeta(***REMOVED***
            confidence = "tentative"
        # Guess with chardet, if avaliable
        if encoding is None and chardet:
            confidence = "tentative"
            ***REMOVED***
                ***REMOVED***
                    from charade.universaldetector import UniversalDetector
                except ImportError:
                    from chardet.universaldetector import UniversalDetector
                buffers = [***REMOVED***
                detector = UniversalDetector(***REMOVED***
                while not detector.done:
                    buffer = self.rawStream.read(self.numBytesChardet***REMOVED***
                    assert isinstance(buffer, bytes***REMOVED***
                    if not buffer:
                        break
                    buffers.append(buffer***REMOVED***
                    detector.feed(buffer***REMOVED***
                detector.close(***REMOVED***
                encoding = detector.result['encoding'***REMOVED***
                self.rawStream.seek(0***REMOVED***
            except ImportError:
                pass
        # If all else fails use the default encoding
        if encoding is None:
            confidence = "tentative"
            encoding = self.defaultEncoding

        # Substitute for equivalent encodings:
        encodingSub = {"iso-8859-1": "windows-1252"***REMOVED***

        if encoding.lower(***REMOVED*** in encodingSub:
            encoding = encodingSub[encoding.lower(***REMOVED******REMOVED***

        return encoding, confidence

    def changeEncoding(self, newEncoding***REMOVED***:
        assert self.charEncoding[1***REMOVED*** != "certain"
        newEncoding = codecName(newEncoding***REMOVED***
        if newEncoding in ("utf-16", "utf-16-be", "utf-16-le"***REMOVED***:
            newEncoding = "utf-8"
        if newEncoding is None:
            return
        elif newEncoding == self.charEncoding[0***REMOVED***:
            self.charEncoding = (self.charEncoding[0***REMOVED***, "certain"***REMOVED***
        else:
            self.rawStream.seek(0***REMOVED***
            self.reset(***REMOVED***
            self.charEncoding = (newEncoding, "certain"***REMOVED***
            raise ReparseException("Encoding changed from %s to %s" % (self.charEncoding[0***REMOVED***, newEncoding***REMOVED******REMOVED***

    def detectBOM(self***REMOVED***:
        ***REMOVED***Attempts to detect at BOM at the start of the stream. If
        an encoding can be determined from the BOM return the name of the
        encoding otherwise return None***REMOVED***
        bomDict = {
            codecs.BOM_UTF8: 'utf-8',
            codecs.BOM_UTF16_LE: 'utf-16-le', codecs.BOM_UTF16_BE: 'utf-16-be',
            codecs.BOM_UTF32_LE: 'utf-32-le', codecs.BOM_UTF32_BE: 'utf-32-be'
    ***REMOVED***

        # Go to beginning of file and read in 4 bytes
        string = self.rawStream.read(4***REMOVED***
        assert isinstance(string, bytes***REMOVED***

        # Try detecting the BOM using bytes from the string
        encoding = bomDict.get(string[:3***REMOVED******REMOVED***         # UTF-8
        seek = 3
        if not encoding:
            # Need to detect UTF-32 before UTF-16
            encoding = bomDict.get(string***REMOVED***         # UTF-32
            seek = 4
            if not encoding:
                encoding = bomDict.get(string[:2***REMOVED******REMOVED***  # UTF-16
                seek = 2

        # Set the read position past the BOM if one was found, otherwise
        # set it to the start of the stream
        self.rawStream.seek(encoding and seek or 0***REMOVED***

        return encoding

    def detectEncodingMeta(self***REMOVED***:
        ***REMOVED***Report the encoding declared by the meta element
        ***REMOVED***
        buffer = self.rawStream.read(self.numBytesMeta***REMOVED***
        assert isinstance(buffer, bytes***REMOVED***
        parser = EncodingParser(buffer***REMOVED***
        self.rawStream.seek(0***REMOVED***
        encoding = parser.getEncoding(***REMOVED***

        if encoding in ("utf-16", "utf-16-be", "utf-16-le"***REMOVED***:
            encoding = "utf-8"

        return encoding


class EncodingBytes(bytes***REMOVED***:
    ***REMOVED***String-like object with an associated position and various extra methods
    If the position is ever greater than the string length then an exception is
    raised***REMOVED***
    def __new__(self, value***REMOVED***:
        assert isinstance(value, bytes***REMOVED***
        return bytes.__new__(self, value.lower(***REMOVED******REMOVED***

    def __init__(self, value***REMOVED***:
        self._position = -1

    def __iter__(self***REMOVED***:
        return self

    def __next__(self***REMOVED***:
        p = self._position = self._position + 1
        if p >= len(self***REMOVED***:
            raise StopIteration
        elif p < 0:
            raise TypeError
        return self[p:p + 1***REMOVED***

    def next(self***REMOVED***:
        # Py2 compat
        return self.__next__(***REMOVED***

    def previous(self***REMOVED***:
        p = self._position
        if p >= len(self***REMOVED***:
            raise StopIteration
        elif p < 0:
            raise TypeError
        self._position = p = p - 1
        return self[p:p + 1***REMOVED***

    def setPosition(self, position***REMOVED***:
        if self._position >= len(self***REMOVED***:
            raise StopIteration
        self._position = position

    def getPosition(self***REMOVED***:
        if self._position >= len(self***REMOVED***:
            raise StopIteration
        if self._position >= 0:
            return self._position
        else:
            return None

    position = property(getPosition, setPosition***REMOVED***

    def getCurrentByte(self***REMOVED***:
        return self[self.position:self.position + 1***REMOVED***

    currentByte = property(getCurrentByte***REMOVED***

    def skip(self, chars=spaceCharactersBytes***REMOVED***:
        ***REMOVED***Skip past a list of characters***REMOVED***
        p = self.position               # use property for the error-checking
        while p < len(self***REMOVED***:
            c = self[p:p + 1***REMOVED***
            if c not in chars:
                self._position = p
                return c
            p += 1
        self._position = p
        return None

    def skipUntil(self, chars***REMOVED***:
        p = self.position
        while p < len(self***REMOVED***:
            c = self[p:p + 1***REMOVED***
            if c in chars:
                self._position = p
                return c
            p += 1
        self._position = p
        return None

    def matchBytes(self, bytes***REMOVED***:
        ***REMOVED***Look for a sequence of bytes at the start of a string. If the bytes
        are found return True and advance the position to the byte after the
        match. Otherwise return False and leave the position alone***REMOVED***
        p = self.position
        data = self[p:p + len(bytes***REMOVED******REMOVED***
        rv = data.startswith(bytes***REMOVED***
        if rv:
            self.position += len(bytes***REMOVED***
        return rv

    def jumpTo(self, bytes***REMOVED***:
        ***REMOVED***Look for the next sequence of bytes matching a given sequence. If
        a match is found advance the position to the last byte of the match***REMOVED***
        newPosition = self[self.position:***REMOVED***.find(bytes***REMOVED***
        if newPosition > -1:
            # XXX: This is ugly, but I can't see a nicer way to fix this.
            if self._position == -1:
                self._position = 0
            self._position += (newPosition + len(bytes***REMOVED*** - 1***REMOVED***
            return True
        else:
            raise StopIteration


class EncodingParser(object***REMOVED***:
    ***REMOVED***Mini parser for detecting character encoding from meta elements***REMOVED***

    def __init__(self, data***REMOVED***:
        ***REMOVED***string - the data to work on for encoding detection***REMOVED***
        self.data = EncodingBytes(data***REMOVED***
        self.encoding = None

    def getEncoding(self***REMOVED***:
        methodDispatch = (
            (b"<!--", self.handleComment***REMOVED***,
            (b"<meta", self.handleMeta***REMOVED***,
            (b"</", self.handlePossibleEndTag***REMOVED***,
            (b"<!", self.handleOther***REMOVED***,
            (b"<?", self.handleOther***REMOVED***,
            (b"<", self.handlePossibleStartTag***REMOVED******REMOVED***
        for byte in self.data:
            keepParsing = True
            for key, method in methodDispatch:
                if self.data.matchBytes(key***REMOVED***:
                    ***REMOVED***
                        keepParsing = method(***REMOVED***
                        break
                    except StopIteration:
                        keepParsing = False
                        break
            if not keepParsing:
                break

        return self.encoding

    def handleComment(self***REMOVED***:
        ***REMOVED***Skip over comments***REMOVED***
        return self.data.jumpTo(b"-->"***REMOVED***

    def handleMeta(self***REMOVED***:
        if self.data.currentByte not in spaceCharactersBytes:
            # if we have <meta not followed by a space so just keep going
            return True
        # We have a valid meta element we want to search for attributes
        hasPragma = False
        pendingEncoding = None
        while True:
            # Try to find the next attribute after the current position
            attr = self.getAttribute(***REMOVED***
            if attr is None:
                return True
            else:
                if attr[0***REMOVED*** == b"http-equiv":
                    hasPragma = attr[1***REMOVED*** == b"content-type"
                    if hasPragma and pendingEncoding is not None:
                        self.encoding = pendingEncoding
                        return False
                elif attr[0***REMOVED*** == b"charset":
                    tentativeEncoding = attr[1***REMOVED***
                    codec = codecName(tentativeEncoding***REMOVED***
                    if codec is not None:
                        self.encoding = codec
                        return False
                elif attr[0***REMOVED*** == b"content":
                    contentParser = ContentAttrParser(EncodingBytes(attr[1***REMOVED******REMOVED******REMOVED***
                    tentativeEncoding = contentParser.parse(***REMOVED***
                    if tentativeEncoding is not None:
                        codec = codecName(tentativeEncoding***REMOVED***
                        if codec is not None:
                            if hasPragma:
                                self.encoding = codec
                                return False
                            else:
                                pendingEncoding = codec

    def handlePossibleStartTag(self***REMOVED***:
        return self.handlePossibleTag(False***REMOVED***

    def handlePossibleEndTag(self***REMOVED***:
        next(self.data***REMOVED***
        return self.handlePossibleTag(True***REMOVED***

    def handlePossibleTag(self, endTag***REMOVED***:
        data = self.data
        if data.currentByte not in asciiLettersBytes:
            # If the next byte is not an ascii letter either ignore this
            # fragment (possible start tag case***REMOVED*** or treat it according to
            # handleOther
            if endTag:
                data.previous(***REMOVED***
                self.handleOther(***REMOVED***
            return True

        c = data.skipUntil(spacesAngleBrackets***REMOVED***
        if c == b"<":
            # return to the first step in the overall "two step" algorithm
            # reprocessing the < byte
            data.previous(***REMOVED***
        else:
            # Read all attributes
            attr = self.getAttribute(***REMOVED***
            while attr is not None:
                attr = self.getAttribute(***REMOVED***
        return True

    def handleOther(self***REMOVED***:
        return self.data.jumpTo(b">"***REMOVED***

    def getAttribute(self***REMOVED***:
        ***REMOVED***Return a name,value pair for the next attribute in the stream,
        if one is found, or None***REMOVED***
        data = self.data
        # Step 1 (skip chars***REMOVED***
        c = data.skip(spaceCharactersBytes | frozenset([b"/"***REMOVED******REMOVED******REMOVED***
        assert c is None or len(c***REMOVED*** == 1
        # Step 2
        if c in (b">", None***REMOVED***:
            return None
        # Step 3
        attrName = [***REMOVED***
        attrValue = [***REMOVED***
        # Step 4 attribute name
        while True:
            if c == b"=" and attrName:
                break
            elif c in spaceCharactersBytes:
                # Step 6!
                c = data.skip(***REMOVED***
                break
            elif c in (b"/", b">"***REMOVED***:
                return b"".join(attrName***REMOVED***, b""
            elif c in asciiUppercaseBytes:
                attrName.append(c.lower(***REMOVED******REMOVED***
            elif c is None:
                return None
            else:
                attrName.append(c***REMOVED***
            # Step 5
            c = next(data***REMOVED***
        # Step 7
        if c != b"=":
            data.previous(***REMOVED***
            return b"".join(attrName***REMOVED***, b""
        # Step 8
        next(data***REMOVED***
        # Step 9
        c = data.skip(***REMOVED***
        # Step 10
        if c in (b"'", b'"'***REMOVED***:
            # 10.1
            quoteChar = c
            while True:
                # 10.2
                c = next(data***REMOVED***
                # 10.3
                if c == quoteChar:
                    next(data***REMOVED***
                    return b"".join(attrName***REMOVED***, b"".join(attrValue***REMOVED***
                # 10.4
                elif c in asciiUppercaseBytes:
                    attrValue.append(c.lower(***REMOVED******REMOVED***
                # 10.5
                else:
                    attrValue.append(c***REMOVED***
        elif c == b">":
            return b"".join(attrName***REMOVED***, b""
        elif c in asciiUppercaseBytes:
            attrValue.append(c.lower(***REMOVED******REMOVED***
        elif c is None:
            return None
        else:
            attrValue.append(c***REMOVED***
        # Step 11
        while True:
            c = next(data***REMOVED***
            if c in spacesAngleBrackets:
                return b"".join(attrName***REMOVED***, b"".join(attrValue***REMOVED***
            elif c in asciiUppercaseBytes:
                attrValue.append(c.lower(***REMOVED******REMOVED***
            elif c is None:
                return None
            else:
                attrValue.append(c***REMOVED***


class ContentAttrParser(object***REMOVED***:
    def __init__(self, data***REMOVED***:
        assert isinstance(data, bytes***REMOVED***
        self.data = data

    def parse(self***REMOVED***:
        ***REMOVED***
            # Check if the attr name is charset
            # otherwise return
            self.data.jumpTo(b"charset"***REMOVED***
            self.data.position += 1
            self.data.skip(***REMOVED***
            if not self.data.currentByte == b"=":
                # If there is no = sign keep looking for attrs
                return None
            self.data.position += 1
            self.data.skip(***REMOVED***
            # Look for an encoding between matching quote marks
            if self.data.currentByte in (b'"', b"'"***REMOVED***:
                quoteMark = self.data.currentByte
                self.data.position += 1
                oldPosition = self.data.position
                if self.data.jumpTo(quoteMark***REMOVED***:
                    return self.data[oldPosition:self.data.position***REMOVED***
                else:
                    return None
            else:
                # Unquoted value
                oldPosition = self.data.position
                ***REMOVED***
                    self.data.skipUntil(spaceCharactersBytes***REMOVED***
                    return self.data[oldPosition:self.data.position***REMOVED***
                except StopIteration:
                    # Return the whole remaining value
                    return self.data[oldPosition:***REMOVED***
        except StopIteration:
            return None


def codecName(encoding***REMOVED***:
    ***REMOVED***Return the python codec name corresponding to an encoding or None if the
    string doesn't correspond to a valid encoding.***REMOVED***
    if isinstance(encoding, bytes***REMOVED***:
        ***REMOVED***
            encoding = encoding.decode("ascii"***REMOVED***
        except UnicodeDecodeError:
            return None
    if encoding:
        canonicalName = ascii_punctuation_re.sub("", encoding***REMOVED***.lower(***REMOVED***
        return encodings.get(canonicalName, None***REMOVED***
    else:
        return None
