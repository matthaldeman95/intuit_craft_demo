from __future__ import absolute_import, division, unicode_literals

***REMOVED***
    chr = unichr # flake8: noqa
except NameError:
    pass

from collections import deque

from .constants import spaceCharacters
from .constants import entities
from .constants import asciiLetters, asciiUpper2Lower
from .constants import digits, hexDigits, EOF
from .constants import tokenTypes, tagTokenTypes
from .constants import replacementCharacters

from .inputstream import HTMLInputStream

from .trie import Trie

entitiesTrie = Trie(entities***REMOVED***


class HTMLTokenizer(object***REMOVED***:
    ***REMOVED*** This class takes care of tokenizing HTML.

    * self.currentToken
      Holds the token that is currently being processed.

    * self.state
      Holds a reference to the method to be invoked... XXX

    * self.stream
      Points to HTMLInputStream object.
    ***REMOVED***

    def __init__(self, stream, encoding=None, parseMeta=True, useChardet=True,
                 lowercaseElementName=True, lowercaseAttrName=True, parser=None***REMOVED***:

        self.stream = HTMLInputStream(stream, encoding, parseMeta, useChardet***REMOVED***
        self.parser = parser

        # Perform case conversions?
        self.lowercaseElementName = lowercaseElementName
        self.lowercaseAttrName = lowercaseAttrName

        # Setup the initial tokenizer state
        self.escapeFlag = False
        self.lastFourChars = [***REMOVED***
        self.state = self.dataState
        self.escape = False

        # The current token being created
        self.currentToken = None
        super(HTMLTokenizer, self***REMOVED***.__init__(***REMOVED***

    def __iter__(self***REMOVED***:
        ***REMOVED*** This is where the magic happens.

        We do our usually processing through the states and when we have a token
        to return we yield the token which pauses processing until the next token
        is requested.
        ***REMOVED***
        self.tokenQueue = deque([***REMOVED******REMOVED***
        # Start processing. When EOF is reached self.state will return False
        # instead of True and the loop will terminate.
        while self.state(***REMOVED***:
            while self.stream.errors:
                yield {"type": tokenTypes["ParseError"***REMOVED***, "data": self.stream.errors.pop(0***REMOVED******REMOVED***
            while self.tokenQueue:
                yield self.tokenQueue.popleft(***REMOVED***

    def consumeNumberEntity(self, isHex***REMOVED***:
        ***REMOVED***This function returns either U+FFFD or the character based on the
        decimal or hexadecimal representation. It also discards ";" if present.
        If not present self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED******REMOVED******REMOVED*** is invoked.
        ***REMOVED***

        allowed = digits
        radix = 10
        if isHex:
            allowed = hexDigits
            radix = 16

        charStack = [***REMOVED***

        # Consume all the characters that are in range while making sure we
        # don't hit an EOF.
        c = self.stream.char(***REMOVED***
        while c in allowed and c is not EOF:
            charStack.append(c***REMOVED***
            c = self.stream.char(***REMOVED***

        # Convert the set of characters consumed to an int.
        charAsInt = int("".join(charStack***REMOVED***, radix***REMOVED***

        # Certain characters get replaced with others
        if charAsInt in replacementCharacters:
            char = replacementCharacters[charAsInt***REMOVED***
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "illegal-codepoint-for-numeric-entity",
                                    "datavars": {"charAsInt": charAsInt***REMOVED******REMOVED******REMOVED***
        elif ((0xD800 <= charAsInt <= 0xDFFF***REMOVED*** or
              (charAsInt > 0x10FFFF***REMOVED******REMOVED***:
            char = "\uFFFD"
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "illegal-codepoint-for-numeric-entity",
                                    "datavars": {"charAsInt": charAsInt***REMOVED******REMOVED******REMOVED***
        else:
            # Should speed up this check somehow (e.g. move the set to a constant***REMOVED***
            if ((0x0001 <= charAsInt <= 0x0008***REMOVED*** or
                (0x000E <= charAsInt <= 0x001F***REMOVED*** or
                (0x007F <= charAsInt <= 0x009F***REMOVED*** or
                (0xFDD0 <= charAsInt <= 0xFDEF***REMOVED*** or
                charAsInt in frozenset([0x000B, 0xFFFE, 0xFFFF, 0x1FFFE,
                                        0x1FFFF, 0x2FFFE, 0x2FFFF, 0x3FFFE,
                                        0x3FFFF, 0x4FFFE, 0x4FFFF, 0x5FFFE,
                                        0x5FFFF, 0x6FFFE, 0x6FFFF, 0x7FFFE,
                                        0x7FFFF, 0x8FFFE, 0x8FFFF, 0x9FFFE,
                                        0x9FFFF, 0xAFFFE, 0xAFFFF, 0xBFFFE,
                                        0xBFFFF, 0xCFFFE, 0xCFFFF, 0xDFFFE,
                                        0xDFFFF, 0xEFFFE, 0xEFFFF, 0xFFFFE,
                                        0xFFFFF, 0x10FFFE, 0x10FFFF***REMOVED******REMOVED******REMOVED***:
                self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                        "data":
                                        "illegal-codepoint-for-numeric-entity",
                                        "datavars": {"charAsInt": charAsInt***REMOVED******REMOVED******REMOVED***
            ***REMOVED***
                # Try/except needed as UCS-2 Python builds' unichar only works
                # within the BMP.
                char = chr(charAsInt***REMOVED***
            except ValueError:
                v = charAsInt - 0x10000
                char = chr(0xD800 | (v >> 10***REMOVED******REMOVED*** + chr(0xDC00 | (v & 0x3FF***REMOVED******REMOVED***

        # Discard the ; if present. Otherwise, put it back on the queue and
        # invoke parseError on parser.
        if c != ";":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "numeric-entity-without-semicolon"***REMOVED******REMOVED***
            self.stream.unget(c***REMOVED***

        return char

    def consumeEntity(self, allowedChar=None, fromAttribute=False***REMOVED***:
        # Initialise to the default output for when no entity is matched
        output = "&"

        charStack = [self.stream.char(***REMOVED******REMOVED***
        if (charStack[0***REMOVED*** in spaceCharacters or charStack[0***REMOVED*** in (EOF, "<", "&"***REMOVED***
                or (allowedChar is not None and allowedChar == charStack[0***REMOVED******REMOVED******REMOVED***:
            self.stream.unget(charStack[0***REMOVED******REMOVED***

        elif charStack[0***REMOVED*** == "#":
            # Read the next character to see if it's hex or decimal
            hex = False
            charStack.append(self.stream.char(***REMOVED******REMOVED***
            if charStack[-1***REMOVED*** in ("x", "X"***REMOVED***:
                hex = True
                charStack.append(self.stream.char(***REMOVED******REMOVED***

            # charStack[-1***REMOVED*** should be the first digit
            if (hex and charStack[-1***REMOVED*** in hexDigits***REMOVED*** \
                    or (not hex and charStack[-1***REMOVED*** in digits***REMOVED***:
                # At least one digit found, so consume the whole number
                self.stream.unget(charStack[-1***REMOVED******REMOVED***
                output = self.consumeNumberEntity(hex***REMOVED***
            else:
                # No digits found
                self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                        "data": "expected-numeric-entity"***REMOVED******REMOVED***
                self.stream.unget(charStack.pop(***REMOVED******REMOVED***
                output = "&" + "".join(charStack***REMOVED***

        else:
            # At this point in the process might have named entity. Entities
            # are stored in the global variable "entities".
            #
            # Consume characters and compare to these to a substring of the
            # entity names in the list until the substring no longer matches.
            while (charStack[-1***REMOVED*** is not EOF***REMOVED***:
                if not entitiesTrie.has_keys_with_prefix("".join(charStack***REMOVED******REMOVED***:
                    break
                charStack.append(self.stream.char(***REMOVED******REMOVED***

            # At this point we have a string that starts with some characters
            # that may match an entity
            # Try to find the longest entity the string will match to take care
            # of &noti for instance.
            ***REMOVED***
                entityName = entitiesTrie.longest_prefix("".join(charStack[:-1***REMOVED******REMOVED******REMOVED***
                entityLength = len(entityName***REMOVED***
            except KeyError:
                entityName = None

            if entityName is not None:
                if entityName[-1***REMOVED*** != ";":
                    self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                            "named-entity-without-semicolon"***REMOVED******REMOVED***
                if (entityName[-1***REMOVED*** != ";" and fromAttribute and
                    (charStack[entityLength***REMOVED*** in asciiLetters or
                     charStack[entityLength***REMOVED*** in digits or
                     charStack[entityLength***REMOVED*** == "="***REMOVED******REMOVED***:
                    self.stream.unget(charStack.pop(***REMOVED******REMOVED***
                    output = "&" + "".join(charStack***REMOVED***
                else:
                    output = entities[entityName***REMOVED***
                    self.stream.unget(charStack.pop(***REMOVED******REMOVED***
                    output += "".join(charStack[entityLength:***REMOVED******REMOVED***
            else:
                self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                        "expected-named-entity"***REMOVED******REMOVED***
                self.stream.unget(charStack.pop(***REMOVED******REMOVED***
                output = "&" + "".join(charStack***REMOVED***

        if fromAttribute:
            self.currentToken["data"***REMOVED***[-1***REMOVED***[1***REMOVED*** += output
        else:
            if output in spaceCharacters:
                tokenType = "SpaceCharacters"
            else:
                tokenType = "Characters"
            self.tokenQueue.append({"type": tokenTypes[tokenType***REMOVED***, "data": output***REMOVED******REMOVED***

    def processEntityInAttribute(self, allowedChar***REMOVED***:
        ***REMOVED***This method replaces the need for "entityInAttributeValueState".
        ***REMOVED***
        self.consumeEntity(allowedChar=allowedChar, fromAttribute=True***REMOVED***

    def emitCurrentToken(self***REMOVED***:
        ***REMOVED***This method is a generic handler for emitting the tags. It also sets
        the state to "data" because that's what's needed after a token has been
        emitted.
        ***REMOVED***
        token = self.currentToken
        # Add token to the queue to be yielded
        if (token["type"***REMOVED*** in tagTokenTypes***REMOVED***:
            if self.lowercaseElementName:
                token["name"***REMOVED*** = token["name"***REMOVED***.translate(asciiUpper2Lower***REMOVED***
            if token["type"***REMOVED*** == tokenTypes["EndTag"***REMOVED***:
                if token["data"***REMOVED***:
                    self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                            "data": "attributes-in-end-tag"***REMOVED******REMOVED***
                if token["selfClosing"***REMOVED***:
                    self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                            "data": "self-closing-flag-on-end-tag"***REMOVED******REMOVED***
        self.tokenQueue.append(token***REMOVED***
        self.state = self.dataState

    # Below are the various tokenizer states worked out.
    def dataState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data == "&":
            self.state = self.entityDataState
        elif data == "<":
            self.state = self.tagOpenState
        elif data == "\u0000":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data": "invalid-codepoint"***REMOVED******REMOVED***
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***,
                                    "data": "\u0000"***REMOVED******REMOVED***
        elif data is EOF:
            # Tokenization ends.
            return False
        elif data in spaceCharacters:
            # Directly after emitting a token you switch back to the "data
            # state". At that point spaceCharacters are important so they are
            # emitted separately.
            self.tokenQueue.append({"type": tokenTypes["SpaceCharacters"***REMOVED***, "data":
                                    data + self.stream.charsUntil(spaceCharacters, True***REMOVED******REMOVED******REMOVED***
            # No need to update lastFourChars here, since the first space will
            # have already been appended to lastFourChars and will have broken
            # any <!-- or --> sequences
        else:
            chars = self.stream.charsUntil(("&", "<", "\u0000"***REMOVED******REMOVED***
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data":
                                    data + chars***REMOVED******REMOVED***
        return True

    def entityDataState(self***REMOVED***:
        self.consumeEntity(***REMOVED***
        self.state = self.dataState
        return True

    def rcdataState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data == "&":
            self.state = self.characterReferenceInRcdata
        elif data == "<":
            self.state = self.rcdataLessThanSignState
        elif data == EOF:
            # Tokenization ends.
            return False
        elif data == "\u0000":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data": "invalid-codepoint"***REMOVED******REMOVED***
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***,
                                    "data": "\uFFFD"***REMOVED******REMOVED***
        elif data in spaceCharacters:
            # Directly after emitting a token you switch back to the "data
            # state". At that point spaceCharacters are important so they are
            # emitted separately.
            self.tokenQueue.append({"type": tokenTypes["SpaceCharacters"***REMOVED***, "data":
                                    data + self.stream.charsUntil(spaceCharacters, True***REMOVED******REMOVED******REMOVED***
            # No need to update lastFourChars here, since the first space will
            # have already been appended to lastFourChars and will have broken
            # any <!-- or --> sequences
        else:
            chars = self.stream.charsUntil(("&", "<", "\u0000"***REMOVED******REMOVED***
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data":
                                    data + chars***REMOVED******REMOVED***
        return True

    def characterReferenceInRcdata(self***REMOVED***:
        self.consumeEntity(***REMOVED***
        self.state = self.rcdataState
        return True

    def rawtextState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data == "<":
            self.state = self.rawtextLessThanSignState
        elif data == "\u0000":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data": "invalid-codepoint"***REMOVED******REMOVED***
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***,
                                    "data": "\uFFFD"***REMOVED******REMOVED***
        elif data == EOF:
            # Tokenization ends.
            return False
        else:
            chars = self.stream.charsUntil(("<", "\u0000"***REMOVED******REMOVED***
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data":
                                    data + chars***REMOVED******REMOVED***
        return True

    def scriptDataState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data == "<":
            self.state = self.scriptDataLessThanSignState
        elif data == "\u0000":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data": "invalid-codepoint"***REMOVED******REMOVED***
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***,
                                    "data": "\uFFFD"***REMOVED******REMOVED***
        elif data == EOF:
            # Tokenization ends.
            return False
        else:
            chars = self.stream.charsUntil(("<", "\u0000"***REMOVED******REMOVED***
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data":
                                    data + chars***REMOVED******REMOVED***
        return True

    def plaintextState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data == EOF:
            # Tokenization ends.
            return False
        elif data == "\u0000":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data": "invalid-codepoint"***REMOVED******REMOVED***
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***,
                                    "data": "\uFFFD"***REMOVED******REMOVED***
        else:
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data":
                                    data + self.stream.charsUntil("\u0000"***REMOVED******REMOVED******REMOVED***
        return True

    def tagOpenState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data == "!":
            self.state = self.markupDeclarationOpenState
        elif data == "/":
            self.state = self.closeTagOpenState
        elif data in asciiLetters:
            self.currentToken = {"type": tokenTypes["StartTag"***REMOVED***,
                                 "name": data, "data": [***REMOVED***,
                                 "selfClosing": False,
                                 "selfClosingAcknowledged": False***REMOVED***
            self.state = self.tagNameState
        elif data == ">":
            # XXX In theory it could be something besides a tag name. But
            # do we really care?
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "expected-tag-name-but-got-right-bracket"***REMOVED******REMOVED***
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": "<>"***REMOVED******REMOVED***
            self.state = self.dataState
        elif data == "?":
            # XXX In theory it could be something besides a tag name. But
            # do we really care?
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "expected-tag-name-but-got-question-mark"***REMOVED******REMOVED***
            self.stream.unget(data***REMOVED***
            self.state = self.bogusCommentState
        else:
            # XXX
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "expected-tag-name"***REMOVED******REMOVED***
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": "<"***REMOVED******REMOVED***
            self.stream.unget(data***REMOVED***
            self.state = self.dataState
        return True

    def closeTagOpenState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data in asciiLetters:
            self.currentToken = {"type": tokenTypes["EndTag"***REMOVED***, "name": data,
                                 "data": [***REMOVED***, "selfClosing": False***REMOVED***
            self.state = self.tagNameState
        elif data == ">":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "expected-closing-tag-but-got-right-bracket"***REMOVED******REMOVED***
            self.state = self.dataState
        elif data is EOF:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "expected-closing-tag-but-got-eof"***REMOVED******REMOVED***
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": "</"***REMOVED******REMOVED***
            self.state = self.dataState
        else:
            # XXX data can be _'_...
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "expected-closing-tag-but-got-char",
                                    "datavars": {"data": data***REMOVED******REMOVED******REMOVED***
            self.stream.unget(data***REMOVED***
            self.state = self.bogusCommentState
        return True

    def tagNameState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data in spaceCharacters:
            self.state = self.beforeAttributeNameState
        elif data == ">":
            self.emitCurrentToken(***REMOVED***
        elif data is EOF:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "eof-in-tag-name"***REMOVED******REMOVED***
            self.state = self.dataState
        elif data == "/":
            self.state = self.selfClosingStartTagState
        elif data == "\u0000":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data": "invalid-codepoint"***REMOVED******REMOVED***
            self.currentToken["name"***REMOVED*** += "\uFFFD"
        else:
            self.currentToken["name"***REMOVED*** += data
            # (Don't use charsUntil here, because tag names are
            # very short and it's faster to not do anything fancy***REMOVED***
        return True

    def rcdataLessThanSignState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data == "/":
            self.temporaryBuffer = ""
            self.state = self.rcdataEndTagOpenState
        else:
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": "<"***REMOVED******REMOVED***
            self.stream.unget(data***REMOVED***
            self.state = self.rcdataState
        return True

    def rcdataEndTagOpenState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data in asciiLetters:
            self.temporaryBuffer += data
            self.state = self.rcdataEndTagNameState
        else:
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": "</"***REMOVED******REMOVED***
            self.stream.unget(data***REMOVED***
            self.state = self.rcdataState
        return True

    def rcdataEndTagNameState(self***REMOVED***:
        appropriate = self.currentToken and self.currentToken["name"***REMOVED***.lower(***REMOVED*** == self.temporaryBuffer.lower(***REMOVED***
        data = self.stream.char(***REMOVED***
        if data in spaceCharacters and appropriate:
            self.currentToken = {"type": tokenTypes["EndTag"***REMOVED***,
                                 "name": self.temporaryBuffer,
                                 "data": [***REMOVED***, "selfClosing": False***REMOVED***
            self.state = self.beforeAttributeNameState
        elif data == "/" and appropriate:
            self.currentToken = {"type": tokenTypes["EndTag"***REMOVED***,
                                 "name": self.temporaryBuffer,
                                 "data": [***REMOVED***, "selfClosing": False***REMOVED***
            self.state = self.selfClosingStartTagState
        elif data == ">" and appropriate:
            self.currentToken = {"type": tokenTypes["EndTag"***REMOVED***,
                                 "name": self.temporaryBuffer,
                                 "data": [***REMOVED***, "selfClosing": False***REMOVED***
            self.emitCurrentToken(***REMOVED***
            self.state = self.dataState
        elif data in asciiLetters:
            self.temporaryBuffer += data
        else:
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***,
                                    "data": "</" + self.temporaryBuffer***REMOVED******REMOVED***
            self.stream.unget(data***REMOVED***
            self.state = self.rcdataState
        return True

    def rawtextLessThanSignState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data == "/":
            self.temporaryBuffer = ""
            self.state = self.rawtextEndTagOpenState
        else:
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": "<"***REMOVED******REMOVED***
            self.stream.unget(data***REMOVED***
            self.state = self.rawtextState
        return True

    def rawtextEndTagOpenState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data in asciiLetters:
            self.temporaryBuffer += data
            self.state = self.rawtextEndTagNameState
        else:
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": "</"***REMOVED******REMOVED***
            self.stream.unget(data***REMOVED***
            self.state = self.rawtextState
        return True

    def rawtextEndTagNameState(self***REMOVED***:
        appropriate = self.currentToken and self.currentToken["name"***REMOVED***.lower(***REMOVED*** == self.temporaryBuffer.lower(***REMOVED***
        data = self.stream.char(***REMOVED***
        if data in spaceCharacters and appropriate:
            self.currentToken = {"type": tokenTypes["EndTag"***REMOVED***,
                                 "name": self.temporaryBuffer,
                                 "data": [***REMOVED***, "selfClosing": False***REMOVED***
            self.state = self.beforeAttributeNameState
        elif data == "/" and appropriate:
            self.currentToken = {"type": tokenTypes["EndTag"***REMOVED***,
                                 "name": self.temporaryBuffer,
                                 "data": [***REMOVED***, "selfClosing": False***REMOVED***
            self.state = self.selfClosingStartTagState
        elif data == ">" and appropriate:
            self.currentToken = {"type": tokenTypes["EndTag"***REMOVED***,
                                 "name": self.temporaryBuffer,
                                 "data": [***REMOVED***, "selfClosing": False***REMOVED***
            self.emitCurrentToken(***REMOVED***
            self.state = self.dataState
        elif data in asciiLetters:
            self.temporaryBuffer += data
        else:
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***,
                                    "data": "</" + self.temporaryBuffer***REMOVED******REMOVED***
            self.stream.unget(data***REMOVED***
            self.state = self.rawtextState
        return True

    def scriptDataLessThanSignState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data == "/":
            self.temporaryBuffer = ""
            self.state = self.scriptDataEndTagOpenState
        elif data == "!":
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": "<!"***REMOVED******REMOVED***
            self.state = self.scriptDataEscapeStartState
        else:
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": "<"***REMOVED******REMOVED***
            self.stream.unget(data***REMOVED***
            self.state = self.scriptDataState
        return True

    def scriptDataEndTagOpenState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data in asciiLetters:
            self.temporaryBuffer += data
            self.state = self.scriptDataEndTagNameState
        else:
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": "</"***REMOVED******REMOVED***
            self.stream.unget(data***REMOVED***
            self.state = self.scriptDataState
        return True

    def scriptDataEndTagNameState(self***REMOVED***:
        appropriate = self.currentToken and self.currentToken["name"***REMOVED***.lower(***REMOVED*** == self.temporaryBuffer.lower(***REMOVED***
        data = self.stream.char(***REMOVED***
        if data in spaceCharacters and appropriate:
            self.currentToken = {"type": tokenTypes["EndTag"***REMOVED***,
                                 "name": self.temporaryBuffer,
                                 "data": [***REMOVED***, "selfClosing": False***REMOVED***
            self.state = self.beforeAttributeNameState
        elif data == "/" and appropriate:
            self.currentToken = {"type": tokenTypes["EndTag"***REMOVED***,
                                 "name": self.temporaryBuffer,
                                 "data": [***REMOVED***, "selfClosing": False***REMOVED***
            self.state = self.selfClosingStartTagState
        elif data == ">" and appropriate:
            self.currentToken = {"type": tokenTypes["EndTag"***REMOVED***,
                                 "name": self.temporaryBuffer,
                                 "data": [***REMOVED***, "selfClosing": False***REMOVED***
            self.emitCurrentToken(***REMOVED***
            self.state = self.dataState
        elif data in asciiLetters:
            self.temporaryBuffer += data
        else:
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***,
                                    "data": "</" + self.temporaryBuffer***REMOVED******REMOVED***
            self.stream.unget(data***REMOVED***
            self.state = self.scriptDataState
        return True

    def scriptDataEscapeStartState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data == "-":
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": "-"***REMOVED******REMOVED***
            self.state = self.scriptDataEscapeStartDashState
        else:
            self.stream.unget(data***REMOVED***
            self.state = self.scriptDataState
        return True

    def scriptDataEscapeStartDashState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data == "-":
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": "-"***REMOVED******REMOVED***
            self.state = self.scriptDataEscapedDashDashState
        else:
            self.stream.unget(data***REMOVED***
            self.state = self.scriptDataState
        return True

    def scriptDataEscapedState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data == "-":
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": "-"***REMOVED******REMOVED***
            self.state = self.scriptDataEscapedDashState
        elif data == "<":
            self.state = self.scriptDataEscapedLessThanSignState
        elif data == "\u0000":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data": "invalid-codepoint"***REMOVED******REMOVED***
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***,
                                    "data": "\uFFFD"***REMOVED******REMOVED***
        elif data == EOF:
            self.state = self.dataState
        else:
            chars = self.stream.charsUntil(("<", "-", "\u0000"***REMOVED******REMOVED***
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data":
                                    data + chars***REMOVED******REMOVED***
        return True

    def scriptDataEscapedDashState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data == "-":
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": "-"***REMOVED******REMOVED***
            self.state = self.scriptDataEscapedDashDashState
        elif data == "<":
            self.state = self.scriptDataEscapedLessThanSignState
        elif data == "\u0000":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data": "invalid-codepoint"***REMOVED******REMOVED***
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***,
                                    "data": "\uFFFD"***REMOVED******REMOVED***
            self.state = self.scriptDataEscapedState
        elif data == EOF:
            self.state = self.dataState
        else:
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": data***REMOVED******REMOVED***
            self.state = self.scriptDataEscapedState
        return True

    def scriptDataEscapedDashDashState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data == "-":
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": "-"***REMOVED******REMOVED***
        elif data == "<":
            self.state = self.scriptDataEscapedLessThanSignState
        elif data == ">":
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": ">"***REMOVED******REMOVED***
            self.state = self.scriptDataState
        elif data == "\u0000":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data": "invalid-codepoint"***REMOVED******REMOVED***
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***,
                                    "data": "\uFFFD"***REMOVED******REMOVED***
            self.state = self.scriptDataEscapedState
        elif data == EOF:
            self.state = self.dataState
        else:
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": data***REMOVED******REMOVED***
            self.state = self.scriptDataEscapedState
        return True

    def scriptDataEscapedLessThanSignState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data == "/":
            self.temporaryBuffer = ""
            self.state = self.scriptDataEscapedEndTagOpenState
        elif data in asciiLetters:
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": "<" + data***REMOVED******REMOVED***
            self.temporaryBuffer = data
            self.state = self.scriptDataDoubleEscapeStartState
        else:
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": "<"***REMOVED******REMOVED***
            self.stream.unget(data***REMOVED***
            self.state = self.scriptDataEscapedState
        return True

    def scriptDataEscapedEndTagOpenState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data in asciiLetters:
            self.temporaryBuffer = data
            self.state = self.scriptDataEscapedEndTagNameState
        else:
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": "</"***REMOVED******REMOVED***
            self.stream.unget(data***REMOVED***
            self.state = self.scriptDataEscapedState
        return True

    def scriptDataEscapedEndTagNameState(self***REMOVED***:
        appropriate = self.currentToken and self.currentToken["name"***REMOVED***.lower(***REMOVED*** == self.temporaryBuffer.lower(***REMOVED***
        data = self.stream.char(***REMOVED***
        if data in spaceCharacters and appropriate:
            self.currentToken = {"type": tokenTypes["EndTag"***REMOVED***,
                                 "name": self.temporaryBuffer,
                                 "data": [***REMOVED***, "selfClosing": False***REMOVED***
            self.state = self.beforeAttributeNameState
        elif data == "/" and appropriate:
            self.currentToken = {"type": tokenTypes["EndTag"***REMOVED***,
                                 "name": self.temporaryBuffer,
                                 "data": [***REMOVED***, "selfClosing": False***REMOVED***
            self.state = self.selfClosingStartTagState
        elif data == ">" and appropriate:
            self.currentToken = {"type": tokenTypes["EndTag"***REMOVED***,
                                 "name": self.temporaryBuffer,
                                 "data": [***REMOVED***, "selfClosing": False***REMOVED***
            self.emitCurrentToken(***REMOVED***
            self.state = self.dataState
        elif data in asciiLetters:
            self.temporaryBuffer += data
        else:
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***,
                                    "data": "</" + self.temporaryBuffer***REMOVED******REMOVED***
            self.stream.unget(data***REMOVED***
            self.state = self.scriptDataEscapedState
        return True

    def scriptDataDoubleEscapeStartState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data in (spaceCharacters | frozenset(("/", ">"***REMOVED******REMOVED******REMOVED***:
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": data***REMOVED******REMOVED***
            if self.temporaryBuffer.lower(***REMOVED*** == "script":
                self.state = self.scriptDataDoubleEscapedState
            else:
                self.state = self.scriptDataEscapedState
        elif data in asciiLetters:
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": data***REMOVED******REMOVED***
            self.temporaryBuffer += data
        else:
            self.stream.unget(data***REMOVED***
            self.state = self.scriptDataEscapedState
        return True

    def scriptDataDoubleEscapedState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data == "-":
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": "-"***REMOVED******REMOVED***
            self.state = self.scriptDataDoubleEscapedDashState
        elif data == "<":
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": "<"***REMOVED******REMOVED***
            self.state = self.scriptDataDoubleEscapedLessThanSignState
        elif data == "\u0000":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data": "invalid-codepoint"***REMOVED******REMOVED***
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***,
                                    "data": "\uFFFD"***REMOVED******REMOVED***
        elif data == EOF:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "eof-in-script-in-script"***REMOVED******REMOVED***
            self.state = self.dataState
        else:
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": data***REMOVED******REMOVED***
        return True

    def scriptDataDoubleEscapedDashState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data == "-":
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": "-"***REMOVED******REMOVED***
            self.state = self.scriptDataDoubleEscapedDashDashState
        elif data == "<":
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": "<"***REMOVED******REMOVED***
            self.state = self.scriptDataDoubleEscapedLessThanSignState
        elif data == "\u0000":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data": "invalid-codepoint"***REMOVED******REMOVED***
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***,
                                    "data": "\uFFFD"***REMOVED******REMOVED***
            self.state = self.scriptDataDoubleEscapedState
        elif data == EOF:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "eof-in-script-in-script"***REMOVED******REMOVED***
            self.state = self.dataState
        else:
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": data***REMOVED******REMOVED***
            self.state = self.scriptDataDoubleEscapedState
        return True

    def scriptDataDoubleEscapedDashDashState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data == "-":
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": "-"***REMOVED******REMOVED***
        elif data == "<":
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": "<"***REMOVED******REMOVED***
            self.state = self.scriptDataDoubleEscapedLessThanSignState
        elif data == ">":
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": ">"***REMOVED******REMOVED***
            self.state = self.scriptDataState
        elif data == "\u0000":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data": "invalid-codepoint"***REMOVED******REMOVED***
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***,
                                    "data": "\uFFFD"***REMOVED******REMOVED***
            self.state = self.scriptDataDoubleEscapedState
        elif data == EOF:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "eof-in-script-in-script"***REMOVED******REMOVED***
            self.state = self.dataState
        else:
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": data***REMOVED******REMOVED***
            self.state = self.scriptDataDoubleEscapedState
        return True

    def scriptDataDoubleEscapedLessThanSignState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data == "/":
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": "/"***REMOVED******REMOVED***
            self.temporaryBuffer = ""
            self.state = self.scriptDataDoubleEscapeEndState
        else:
            self.stream.unget(data***REMOVED***
            self.state = self.scriptDataDoubleEscapedState
        return True

    def scriptDataDoubleEscapeEndState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data in (spaceCharacters | frozenset(("/", ">"***REMOVED******REMOVED******REMOVED***:
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": data***REMOVED******REMOVED***
            if self.temporaryBuffer.lower(***REMOVED*** == "script":
                self.state = self.scriptDataEscapedState
            else:
                self.state = self.scriptDataDoubleEscapedState
        elif data in asciiLetters:
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***, "data": data***REMOVED******REMOVED***
            self.temporaryBuffer += data
        else:
            self.stream.unget(data***REMOVED***
            self.state = self.scriptDataDoubleEscapedState
        return True

    def beforeAttributeNameState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data in spaceCharacters:
            self.stream.charsUntil(spaceCharacters, True***REMOVED***
        elif data in asciiLetters:
            self.currentToken["data"***REMOVED***.append([data, ""***REMOVED******REMOVED***
            self.state = self.attributeNameState
        elif data == ">":
            self.emitCurrentToken(***REMOVED***
        elif data == "/":
            self.state = self.selfClosingStartTagState
        elif data in ("'", '"', "=", "<"***REMOVED***:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "invalid-character-in-attribute-name"***REMOVED******REMOVED***
            self.currentToken["data"***REMOVED***.append([data, ""***REMOVED******REMOVED***
            self.state = self.attributeNameState
        elif data == "\u0000":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data": "invalid-codepoint"***REMOVED******REMOVED***
            self.currentToken["data"***REMOVED***.append(["\uFFFD", ""***REMOVED******REMOVED***
            self.state = self.attributeNameState
        elif data is EOF:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "expected-attribute-name-but-got-eof"***REMOVED******REMOVED***
            self.state = self.dataState
        else:
            self.currentToken["data"***REMOVED***.append([data, ""***REMOVED******REMOVED***
            self.state = self.attributeNameState
        return True

    def attributeNameState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        leavingThisState = True
        emitToken = False
        if data == "=":
            self.state = self.beforeAttributeValueState
        elif data in asciiLetters:
            self.currentToken["data"***REMOVED***[-1***REMOVED***[0***REMOVED*** += data +\
                self.stream.charsUntil(asciiLetters, True***REMOVED***
            leavingThisState = False
        elif data == ">":
            # XXX If we emit here the attributes are converted to a dict
            # without being checked and when the code below runs we error
            # because data is a dict not a list
            emitToken = True
        elif data in spaceCharacters:
            self.state = self.afterAttributeNameState
        elif data == "/":
            self.state = self.selfClosingStartTagState
        elif data == "\u0000":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data": "invalid-codepoint"***REMOVED******REMOVED***
            self.currentToken["data"***REMOVED***[-1***REMOVED***[0***REMOVED*** += "\uFFFD"
            leavingThisState = False
        elif data in ("'", '"', "<"***REMOVED***:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data":
                                    "invalid-character-in-attribute-name"***REMOVED******REMOVED***
            self.currentToken["data"***REMOVED***[-1***REMOVED***[0***REMOVED*** += data
            leavingThisState = False
        elif data is EOF:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data": "eof-in-attribute-name"***REMOVED******REMOVED***
            self.state = self.dataState
        else:
            self.currentToken["data"***REMOVED***[-1***REMOVED***[0***REMOVED*** += data
            leavingThisState = False

        if leavingThisState:
            # Attributes are not dropped at this stage. That happens when the
            # start tag token is emitted so values can still be safely appended
            # to attributes, but we do want to report the parse error in time.
            if self.lowercaseAttrName:
                self.currentToken["data"***REMOVED***[-1***REMOVED***[0***REMOVED*** = (
                    self.currentToken["data"***REMOVED***[-1***REMOVED***[0***REMOVED***.translate(asciiUpper2Lower***REMOVED******REMOVED***
            for name, value in self.currentToken["data"***REMOVED***[:-1***REMOVED***:
                if self.currentToken["data"***REMOVED***[-1***REMOVED***[0***REMOVED*** == name:
                    self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                            "duplicate-attribute"***REMOVED******REMOVED***
                    break
            # XXX Fix for above XXX
            if emitToken:
                self.emitCurrentToken(***REMOVED***
        return True

    def afterAttributeNameState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data in spaceCharacters:
            self.stream.charsUntil(spaceCharacters, True***REMOVED***
        elif data == "=":
            self.state = self.beforeAttributeValueState
        elif data == ">":
            self.emitCurrentToken(***REMOVED***
        elif data in asciiLetters:
            self.currentToken["data"***REMOVED***.append([data, ""***REMOVED******REMOVED***
            self.state = self.attributeNameState
        elif data == "/":
            self.state = self.selfClosingStartTagState
        elif data == "\u0000":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data": "invalid-codepoint"***REMOVED******REMOVED***
            self.currentToken["data"***REMOVED***.append(["\uFFFD", ""***REMOVED******REMOVED***
            self.state = self.attributeNameState
        elif data in ("'", '"', "<"***REMOVED***:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "invalid-character-after-attribute-name"***REMOVED******REMOVED***
            self.currentToken["data"***REMOVED***.append([data, ""***REMOVED******REMOVED***
            self.state = self.attributeNameState
        elif data is EOF:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "expected-end-of-tag-but-got-eof"***REMOVED******REMOVED***
            self.state = self.dataState
        else:
            self.currentToken["data"***REMOVED***.append([data, ""***REMOVED******REMOVED***
            self.state = self.attributeNameState
        return True

    def beforeAttributeValueState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data in spaceCharacters:
            self.stream.charsUntil(spaceCharacters, True***REMOVED***
        elif data == "\"":
            self.state = self.attributeValueDoubleQuotedState
        elif data == "&":
            self.state = self.attributeValueUnQuotedState
            self.stream.unget(data***REMOVED***
        elif data == "'":
            self.state = self.attributeValueSingleQuotedState
        elif data == ">":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "expected-attribute-value-but-got-right-bracket"***REMOVED******REMOVED***
            self.emitCurrentToken(***REMOVED***
        elif data == "\u0000":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data": "invalid-codepoint"***REMOVED******REMOVED***
            self.currentToken["data"***REMOVED***[-1***REMOVED***[1***REMOVED*** += "\uFFFD"
            self.state = self.attributeValueUnQuotedState
        elif data in ("=", "<", "`"***REMOVED***:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "equals-in-unquoted-attribute-value"***REMOVED******REMOVED***
            self.currentToken["data"***REMOVED***[-1***REMOVED***[1***REMOVED*** += data
            self.state = self.attributeValueUnQuotedState
        elif data is EOF:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "expected-attribute-value-but-got-eof"***REMOVED******REMOVED***
            self.state = self.dataState
        else:
            self.currentToken["data"***REMOVED***[-1***REMOVED***[1***REMOVED*** += data
            self.state = self.attributeValueUnQuotedState
        return True

    def attributeValueDoubleQuotedState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data == "\"":
            self.state = self.afterAttributeValueState
        elif data == "&":
            self.processEntityInAttribute('"'***REMOVED***
        elif data == "\u0000":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data": "invalid-codepoint"***REMOVED******REMOVED***
            self.currentToken["data"***REMOVED***[-1***REMOVED***[1***REMOVED*** += "\uFFFD"
        elif data is EOF:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "eof-in-attribute-value-double-quote"***REMOVED******REMOVED***
            self.state = self.dataState
        else:
            self.currentToken["data"***REMOVED***[-1***REMOVED***[1***REMOVED*** += data +\
                self.stream.charsUntil(("\"", "&", "\u0000"***REMOVED******REMOVED***
        return True

    def attributeValueSingleQuotedState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data == "'":
            self.state = self.afterAttributeValueState
        elif data == "&":
            self.processEntityInAttribute("'"***REMOVED***
        elif data == "\u0000":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data": "invalid-codepoint"***REMOVED******REMOVED***
            self.currentToken["data"***REMOVED***[-1***REMOVED***[1***REMOVED*** += "\uFFFD"
        elif data is EOF:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "eof-in-attribute-value-single-quote"***REMOVED******REMOVED***
            self.state = self.dataState
        else:
            self.currentToken["data"***REMOVED***[-1***REMOVED***[1***REMOVED*** += data +\
                self.stream.charsUntil(("'", "&", "\u0000"***REMOVED******REMOVED***
        return True

    def attributeValueUnQuotedState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data in spaceCharacters:
            self.state = self.beforeAttributeNameState
        elif data == "&":
            self.processEntityInAttribute(">"***REMOVED***
        elif data == ">":
            self.emitCurrentToken(***REMOVED***
        elif data in ('"', "'", "=", "<", "`"***REMOVED***:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "unexpected-character-in-unquoted-attribute-value"***REMOVED******REMOVED***
            self.currentToken["data"***REMOVED***[-1***REMOVED***[1***REMOVED*** += data
        elif data == "\u0000":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data": "invalid-codepoint"***REMOVED******REMOVED***
            self.currentToken["data"***REMOVED***[-1***REMOVED***[1***REMOVED*** += "\uFFFD"
        elif data is EOF:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "eof-in-attribute-value-no-quotes"***REMOVED******REMOVED***
            self.state = self.dataState
        else:
            self.currentToken["data"***REMOVED***[-1***REMOVED***[1***REMOVED*** += data + self.stream.charsUntil(
                frozenset(("&", ">", '"', "'", "=", "<", "`", "\u0000"***REMOVED******REMOVED*** | spaceCharacters***REMOVED***
        return True

    def afterAttributeValueState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data in spaceCharacters:
            self.state = self.beforeAttributeNameState
        elif data == ">":
            self.emitCurrentToken(***REMOVED***
        elif data == "/":
            self.state = self.selfClosingStartTagState
        elif data is EOF:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "unexpected-EOF-after-attribute-value"***REMOVED******REMOVED***
            self.stream.unget(data***REMOVED***
            self.state = self.dataState
        else:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "unexpected-character-after-attribute-value"***REMOVED******REMOVED***
            self.stream.unget(data***REMOVED***
            self.state = self.beforeAttributeNameState
        return True

    def selfClosingStartTagState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data == ">":
            self.currentToken["selfClosing"***REMOVED*** = True
            self.emitCurrentToken(***REMOVED***
        elif data is EOF:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data":
                                    "unexpected-EOF-after-solidus-in-tag"***REMOVED******REMOVED***
            self.stream.unget(data***REMOVED***
            self.state = self.dataState
        else:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "unexpected-character-after-solidus-in-tag"***REMOVED******REMOVED***
            self.stream.unget(data***REMOVED***
            self.state = self.beforeAttributeNameState
        return True

    def bogusCommentState(self***REMOVED***:
        # Make a new comment token and give it as value all the characters
        # until the first > or EOF (charsUntil checks for EOF automatically***REMOVED***
        # and emit it.
        data = self.stream.charsUntil(">"***REMOVED***
        data = data.replace("\u0000", "\uFFFD"***REMOVED***
        self.tokenQueue.append(
        ***REMOVED***"type": tokenTypes["Comment"***REMOVED***, "data": data***REMOVED******REMOVED***

        # Eat the character directly after the bogus comment which is either a
        # ">" or an EOF.
        self.stream.char(***REMOVED***
        self.state = self.dataState
        return True

    def markupDeclarationOpenState(self***REMOVED***:
        charStack = [self.stream.char(***REMOVED******REMOVED***
        if charStack[-1***REMOVED*** == "-":
            charStack.append(self.stream.char(***REMOVED******REMOVED***
            if charStack[-1***REMOVED*** == "-":
                self.currentToken = {"type": tokenTypes["Comment"***REMOVED***, "data": ""***REMOVED***
                self.state = self.commentStartState
                return True
        elif charStack[-1***REMOVED*** in ('d', 'D'***REMOVED***:
            matched = True
            for expected in (('o', 'O'***REMOVED***, ('c', 'C'***REMOVED***, ('t', 'T'***REMOVED***,
                             ('y', 'Y'***REMOVED***, ('p', 'P'***REMOVED***, ('e', 'E'***REMOVED******REMOVED***:
                charStack.append(self.stream.char(***REMOVED******REMOVED***
                if charStack[-1***REMOVED*** not in expected:
                    matched = False
                    break
            if matched:
                self.currentToken = {"type": tokenTypes["Doctype"***REMOVED***,
                                     "name": "",
                                     "publicId": None, "systemId": None,
                                     "correct": True***REMOVED***
                self.state = self.doctypeState
                return True
        elif (charStack[-1***REMOVED*** == "[" and
              self.parser is not None and
              self.parser.tree.openElements and
              self.parser.tree.openElements[-1***REMOVED***.namespace != self.parser.tree.defaultNamespace***REMOVED***:
            matched = True
            for expected in ["C", "D", "A", "T", "A", "["***REMOVED***:
                charStack.append(self.stream.char(***REMOVED******REMOVED***
                if charStack[-1***REMOVED*** != expected:
                    matched = False
                    break
            if matched:
                self.state = self.cdataSectionState
                return True

        self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                "expected-dashes-or-doctype"***REMOVED******REMOVED***

        while charStack:
            self.stream.unget(charStack.pop(***REMOVED******REMOVED***
        self.state = self.bogusCommentState
        return True

    def commentStartState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data == "-":
            self.state = self.commentStartDashState
        elif data == "\u0000":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data": "invalid-codepoint"***REMOVED******REMOVED***
            self.currentToken["data"***REMOVED*** += "\uFFFD"
        elif data == ">":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "incorrect-comment"***REMOVED******REMOVED***
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        elif data is EOF:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "eof-in-comment"***REMOVED******REMOVED***
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        else:
            self.currentToken["data"***REMOVED*** += data
            self.state = self.commentState
        return True

    def commentStartDashState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data == "-":
            self.state = self.commentEndState
        elif data == "\u0000":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data": "invalid-codepoint"***REMOVED******REMOVED***
            self.currentToken["data"***REMOVED*** += "-\uFFFD"
        elif data == ">":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "incorrect-comment"***REMOVED******REMOVED***
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        elif data is EOF:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "eof-in-comment"***REMOVED******REMOVED***
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        else:
            self.currentToken["data"***REMOVED*** += "-" + data
            self.state = self.commentState
        return True

    def commentState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data == "-":
            self.state = self.commentEndDashState
        elif data == "\u0000":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data": "invalid-codepoint"***REMOVED******REMOVED***
            self.currentToken["data"***REMOVED*** += "\uFFFD"
        elif data is EOF:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data": "eof-in-comment"***REMOVED******REMOVED***
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        else:
            self.currentToken["data"***REMOVED*** += data + \
                self.stream.charsUntil(("-", "\u0000"***REMOVED******REMOVED***
        return True

    def commentEndDashState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data == "-":
            self.state = self.commentEndState
        elif data == "\u0000":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data": "invalid-codepoint"***REMOVED******REMOVED***
            self.currentToken["data"***REMOVED*** += "-\uFFFD"
            self.state = self.commentState
        elif data is EOF:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "eof-in-comment-end-dash"***REMOVED******REMOVED***
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        else:
            self.currentToken["data"***REMOVED*** += "-" + data
            self.state = self.commentState
        return True

    def commentEndState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data == ">":
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        elif data == "\u0000":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data": "invalid-codepoint"***REMOVED******REMOVED***
            self.currentToken["data"***REMOVED*** += "--\uFFFD"
            self.state = self.commentState
        elif data == "!":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "unexpected-bang-after-double-dash-in-comment"***REMOVED******REMOVED***
            self.state = self.commentEndBangState
        elif data == "-":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "unexpected-dash-after-double-dash-in-comment"***REMOVED******REMOVED***
            self.currentToken["data"***REMOVED*** += data
        elif data is EOF:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "eof-in-comment-double-dash"***REMOVED******REMOVED***
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        else:
            # XXX
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "unexpected-char-in-comment"***REMOVED******REMOVED***
            self.currentToken["data"***REMOVED*** += "--" + data
            self.state = self.commentState
        return True

    def commentEndBangState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data == ">":
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        elif data == "-":
            self.currentToken["data"***REMOVED*** += "--!"
            self.state = self.commentEndDashState
        elif data == "\u0000":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data": "invalid-codepoint"***REMOVED******REMOVED***
            self.currentToken["data"***REMOVED*** += "--!\uFFFD"
            self.state = self.commentState
        elif data is EOF:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "eof-in-comment-end-bang-state"***REMOVED******REMOVED***
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        else:
            self.currentToken["data"***REMOVED*** += "--!" + data
            self.state = self.commentState
        return True

    def doctypeState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data in spaceCharacters:
            self.state = self.beforeDoctypeNameState
        elif data is EOF:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "expected-doctype-name-but-got-eof"***REMOVED******REMOVED***
            self.currentToken["correct"***REMOVED*** = False
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        else:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "need-space-after-doctype"***REMOVED******REMOVED***
            self.stream.unget(data***REMOVED***
            self.state = self.beforeDoctypeNameState
        return True

    def beforeDoctypeNameState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data in spaceCharacters:
            pass
        elif data == ">":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "expected-doctype-name-but-got-right-bracket"***REMOVED******REMOVED***
            self.currentToken["correct"***REMOVED*** = False
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        elif data == "\u0000":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data": "invalid-codepoint"***REMOVED******REMOVED***
            self.currentToken["name"***REMOVED*** = "\uFFFD"
            self.state = self.doctypeNameState
        elif data is EOF:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "expected-doctype-name-but-got-eof"***REMOVED******REMOVED***
            self.currentToken["correct"***REMOVED*** = False
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        else:
            self.currentToken["name"***REMOVED*** = data
            self.state = self.doctypeNameState
        return True

    def doctypeNameState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data in spaceCharacters:
            self.currentToken["name"***REMOVED*** = self.currentToken["name"***REMOVED***.translate(asciiUpper2Lower***REMOVED***
            self.state = self.afterDoctypeNameState
        elif data == ">":
            self.currentToken["name"***REMOVED*** = self.currentToken["name"***REMOVED***.translate(asciiUpper2Lower***REMOVED***
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        elif data == "\u0000":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data": "invalid-codepoint"***REMOVED******REMOVED***
            self.currentToken["name"***REMOVED*** += "\uFFFD"
            self.state = self.doctypeNameState
        elif data is EOF:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "eof-in-doctype-name"***REMOVED******REMOVED***
            self.currentToken["correct"***REMOVED*** = False
            self.currentToken["name"***REMOVED*** = self.currentToken["name"***REMOVED***.translate(asciiUpper2Lower***REMOVED***
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        else:
            self.currentToken["name"***REMOVED*** += data
        return True

    def afterDoctypeNameState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data in spaceCharacters:
            pass
        elif data == ">":
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        elif data is EOF:
            self.currentToken["correct"***REMOVED*** = False
            self.stream.unget(data***REMOVED***
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "eof-in-doctype"***REMOVED******REMOVED***
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        else:
            if data in ("p", "P"***REMOVED***:
                matched = True
                for expected in (("u", "U"***REMOVED***, ("b", "B"***REMOVED***, ("l", "L"***REMOVED***,
                                 ("i", "I"***REMOVED***, ("c", "C"***REMOVED******REMOVED***:
                    data = self.stream.char(***REMOVED***
                    if data not in expected:
                        matched = False
                        break
                if matched:
                    self.state = self.afterDoctypePublicKeywordState
                    return True
            elif data in ("s", "S"***REMOVED***:
                matched = True
                for expected in (("y", "Y"***REMOVED***, ("s", "S"***REMOVED***, ("t", "T"***REMOVED***,
                                 ("e", "E"***REMOVED***, ("m", "M"***REMOVED******REMOVED***:
                    data = self.stream.char(***REMOVED***
                    if data not in expected:
                        matched = False
                        break
                if matched:
                    self.state = self.afterDoctypeSystemKeywordState
                    return True

            # All the characters read before the current 'data' will be
            # [a-zA-Z***REMOVED***, so they're garbage in the bogus doctype and can be
            # discarded; only the latest character might be '>' or EOF
            # and needs to be ungetted
            self.stream.unget(data***REMOVED***
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "expected-space-or-right-bracket-in-doctype", "datavars":
                                ***REMOVED***"data": data***REMOVED******REMOVED******REMOVED***
            self.currentToken["correct"***REMOVED*** = False
            self.state = self.bogusDoctypeState

        return True

    def afterDoctypePublicKeywordState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data in spaceCharacters:
            self.state = self.beforeDoctypePublicIdentifierState
        elif data in ("'", '"'***REMOVED***:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "unexpected-char-in-doctype"***REMOVED******REMOVED***
            self.stream.unget(data***REMOVED***
            self.state = self.beforeDoctypePublicIdentifierState
        elif data is EOF:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "eof-in-doctype"***REMOVED******REMOVED***
            self.currentToken["correct"***REMOVED*** = False
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        else:
            self.stream.unget(data***REMOVED***
            self.state = self.beforeDoctypePublicIdentifierState
        return True

    def beforeDoctypePublicIdentifierState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data in spaceCharacters:
            pass
        elif data == "\"":
            self.currentToken["publicId"***REMOVED*** = ""
            self.state = self.doctypePublicIdentifierDoubleQuotedState
        elif data == "'":
            self.currentToken["publicId"***REMOVED*** = ""
            self.state = self.doctypePublicIdentifierSingleQuotedState
        elif data == ">":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "unexpected-end-of-doctype"***REMOVED******REMOVED***
            self.currentToken["correct"***REMOVED*** = False
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        elif data is EOF:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "eof-in-doctype"***REMOVED******REMOVED***
            self.currentToken["correct"***REMOVED*** = False
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        else:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "unexpected-char-in-doctype"***REMOVED******REMOVED***
            self.currentToken["correct"***REMOVED*** = False
            self.state = self.bogusDoctypeState
        return True

    def doctypePublicIdentifierDoubleQuotedState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data == "\"":
            self.state = self.afterDoctypePublicIdentifierState
        elif data == "\u0000":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data": "invalid-codepoint"***REMOVED******REMOVED***
            self.currentToken["publicId"***REMOVED*** += "\uFFFD"
        elif data == ">":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "unexpected-end-of-doctype"***REMOVED******REMOVED***
            self.currentToken["correct"***REMOVED*** = False
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        elif data is EOF:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "eof-in-doctype"***REMOVED******REMOVED***
            self.currentToken["correct"***REMOVED*** = False
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        else:
            self.currentToken["publicId"***REMOVED*** += data
        return True

    def doctypePublicIdentifierSingleQuotedState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data == "'":
            self.state = self.afterDoctypePublicIdentifierState
        elif data == "\u0000":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data": "invalid-codepoint"***REMOVED******REMOVED***
            self.currentToken["publicId"***REMOVED*** += "\uFFFD"
        elif data == ">":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "unexpected-end-of-doctype"***REMOVED******REMOVED***
            self.currentToken["correct"***REMOVED*** = False
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        elif data is EOF:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "eof-in-doctype"***REMOVED******REMOVED***
            self.currentToken["correct"***REMOVED*** = False
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        else:
            self.currentToken["publicId"***REMOVED*** += data
        return True

    def afterDoctypePublicIdentifierState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data in spaceCharacters:
            self.state = self.betweenDoctypePublicAndSystemIdentifiersState
        elif data == ">":
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        elif data == '"':
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "unexpected-char-in-doctype"***REMOVED******REMOVED***
            self.currentToken["systemId"***REMOVED*** = ""
            self.state = self.doctypeSystemIdentifierDoubleQuotedState
        elif data == "'":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "unexpected-char-in-doctype"***REMOVED******REMOVED***
            self.currentToken["systemId"***REMOVED*** = ""
            self.state = self.doctypeSystemIdentifierSingleQuotedState
        elif data is EOF:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "eof-in-doctype"***REMOVED******REMOVED***
            self.currentToken["correct"***REMOVED*** = False
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        else:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "unexpected-char-in-doctype"***REMOVED******REMOVED***
            self.currentToken["correct"***REMOVED*** = False
            self.state = self.bogusDoctypeState
        return True

    def betweenDoctypePublicAndSystemIdentifiersState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data in spaceCharacters:
            pass
        elif data == ">":
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        elif data == '"':
            self.currentToken["systemId"***REMOVED*** = ""
            self.state = self.doctypeSystemIdentifierDoubleQuotedState
        elif data == "'":
            self.currentToken["systemId"***REMOVED*** = ""
            self.state = self.doctypeSystemIdentifierSingleQuotedState
        elif data == EOF:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "eof-in-doctype"***REMOVED******REMOVED***
            self.currentToken["correct"***REMOVED*** = False
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        else:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "unexpected-char-in-doctype"***REMOVED******REMOVED***
            self.currentToken["correct"***REMOVED*** = False
            self.state = self.bogusDoctypeState
        return True

    def afterDoctypeSystemKeywordState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data in spaceCharacters:
            self.state = self.beforeDoctypeSystemIdentifierState
        elif data in ("'", '"'***REMOVED***:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "unexpected-char-in-doctype"***REMOVED******REMOVED***
            self.stream.unget(data***REMOVED***
            self.state = self.beforeDoctypeSystemIdentifierState
        elif data is EOF:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "eof-in-doctype"***REMOVED******REMOVED***
            self.currentToken["correct"***REMOVED*** = False
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        else:
            self.stream.unget(data***REMOVED***
            self.state = self.beforeDoctypeSystemIdentifierState
        return True

    def beforeDoctypeSystemIdentifierState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data in spaceCharacters:
            pass
        elif data == "\"":
            self.currentToken["systemId"***REMOVED*** = ""
            self.state = self.doctypeSystemIdentifierDoubleQuotedState
        elif data == "'":
            self.currentToken["systemId"***REMOVED*** = ""
            self.state = self.doctypeSystemIdentifierSingleQuotedState
        elif data == ">":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "unexpected-char-in-doctype"***REMOVED******REMOVED***
            self.currentToken["correct"***REMOVED*** = False
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        elif data is EOF:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "eof-in-doctype"***REMOVED******REMOVED***
            self.currentToken["correct"***REMOVED*** = False
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        else:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "unexpected-char-in-doctype"***REMOVED******REMOVED***
            self.currentToken["correct"***REMOVED*** = False
            self.state = self.bogusDoctypeState
        return True

    def doctypeSystemIdentifierDoubleQuotedState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data == "\"":
            self.state = self.afterDoctypeSystemIdentifierState
        elif data == "\u0000":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data": "invalid-codepoint"***REMOVED******REMOVED***
            self.currentToken["systemId"***REMOVED*** += "\uFFFD"
        elif data == ">":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "unexpected-end-of-doctype"***REMOVED******REMOVED***
            self.currentToken["correct"***REMOVED*** = False
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        elif data is EOF:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "eof-in-doctype"***REMOVED******REMOVED***
            self.currentToken["correct"***REMOVED*** = False
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        else:
            self.currentToken["systemId"***REMOVED*** += data
        return True

    def doctypeSystemIdentifierSingleQuotedState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data == "'":
            self.state = self.afterDoctypeSystemIdentifierState
        elif data == "\u0000":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                    "data": "invalid-codepoint"***REMOVED******REMOVED***
            self.currentToken["systemId"***REMOVED*** += "\uFFFD"
        elif data == ">":
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "unexpected-end-of-doctype"***REMOVED******REMOVED***
            self.currentToken["correct"***REMOVED*** = False
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        elif data is EOF:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "eof-in-doctype"***REMOVED******REMOVED***
            self.currentToken["correct"***REMOVED*** = False
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        else:
            self.currentToken["systemId"***REMOVED*** += data
        return True

    def afterDoctypeSystemIdentifierState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data in spaceCharacters:
            pass
        elif data == ">":
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        elif data is EOF:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "eof-in-doctype"***REMOVED******REMOVED***
            self.currentToken["correct"***REMOVED*** = False
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        else:
            self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***, "data":
                                    "unexpected-char-in-doctype"***REMOVED******REMOVED***
            self.state = self.bogusDoctypeState
        return True

    def bogusDoctypeState(self***REMOVED***:
        data = self.stream.char(***REMOVED***
        if data == ">":
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        elif data is EOF:
            # XXX EMIT
            self.stream.unget(data***REMOVED***
            self.tokenQueue.append(self.currentToken***REMOVED***
            self.state = self.dataState
        else:
            pass
        return True

    def cdataSectionState(self***REMOVED***:
        data = [***REMOVED***
        while True:
            data.append(self.stream.charsUntil("***REMOVED***"***REMOVED******REMOVED***
            data.append(self.stream.charsUntil(">"***REMOVED******REMOVED***
            char = self.stream.char(***REMOVED***
            if char == EOF:
                break
            else:
                assert char == ">"
                if data[-1***REMOVED***[-2:***REMOVED*** == "***REMOVED******REMOVED***":
                    data[-1***REMOVED*** = data[-1***REMOVED***[:-2***REMOVED***
                    break
                else:
                    data.append(char***REMOVED***

        data = "".join(data***REMOVED***
        # Deal with null here rather than in the parser
        nullCount = data.count("\u0000"***REMOVED***
        if nullCount > 0:
            for i in range(nullCount***REMOVED***:
                self.tokenQueue.append({"type": tokenTypes["ParseError"***REMOVED***,
                                        "data": "invalid-codepoint"***REMOVED******REMOVED***
            data = data.replace("\u0000", "\uFFFD"***REMOVED***
        if data:
            self.tokenQueue.append({"type": tokenTypes["Characters"***REMOVED***,
                                    "data": data***REMOVED******REMOVED***
        self.state = self.dataState
        return True
