# -*- coding: utf-8 -*-

***REMOVED***
requests.utils
~~~~~~~~~~~~~~

This module provides utility functions that are used within Requests
that are also useful for external consumption.
***REMOVED***

import cgi
import codecs
import collections
import io
***REMOVED***
import re
import socket
import struct
import warnings

from . import __version__
from . import certs
from .compat import parse_http_list as _parse_list_header
from .compat import (quote, urlparse, bytes, str, OrderedDict, unquote, is_py2,
                     builtin_str, getproxies, proxy_bypass, urlunparse,
                     basestring***REMOVED***
from .cookies import RequestsCookieJar, cookiejar_from_dict
from .structures import CaseInsensitiveDict
from .exceptions import InvalidURL, InvalidHeader, FileModeWarning

_hush_pyflakes = (RequestsCookieJar,***REMOVED***

NETRC_FILES = ('.netrc', '_netrc'***REMOVED***

DEFAULT_CA_BUNDLE_PATH = certs.where(***REMOVED***


def dict_to_sequence(d***REMOVED***:
    ***REMOVED***Returns an internal sequence dictionary update.***REMOVED***

    if hasattr(d, 'items'***REMOVED***:
        d = d.items(***REMOVED***

    return d


def super_len(o***REMOVED***:
    total_length = 0
    current_position = 0

    if hasattr(o, '__len__'***REMOVED***:
        total_length = len(o***REMOVED***

    elif hasattr(o, 'len'***REMOVED***:
        total_length = o.len

    elif hasattr(o, 'getvalue'***REMOVED***:
        # e.g. BytesIO, cStringIO.StringIO
        total_length = len(o.getvalue(***REMOVED******REMOVED***

    elif hasattr(o, 'fileno'***REMOVED***:
        ***REMOVED***
            fileno = o.fileno(***REMOVED***
        except io.UnsupportedOperation:
            pass
        else:
            total_length = os.fstat(fileno***REMOVED***.st_size

            # Having used fstat to determine the file length, we need to
            # confirm that this file was opened up in binary mode.
            if 'b' not in o.mode:
                warnings.warn((
                    "Requests has determined the content-length for this "
                    "request using the binary size of the file: however, the "
                    "file has been opened in text mode (i.e. without the 'b' "
                    "flag in the mode***REMOVED***. This may lead to an incorrect "
                    "content-length. In Requests 3.0, support will be removed "
                    "for files in text mode."***REMOVED***,
                    FileModeWarning
                ***REMOVED***

    if hasattr(o, 'tell'***REMOVED***:
        ***REMOVED***
            current_position = o.tell(***REMOVED***
        except (OSError, IOError***REMOVED***:
            # This can happen in some weird situations, such as when the file
            # is actually a special file descriptor like stdin. In this
            # instance, we don't know what the length is, so set it to zero and
            # let requests chunk it instead.
            current_position = total_length

    return max(0, total_length - current_position***REMOVED***


def get_netrc_auth(url, raise_errors=False***REMOVED***:
    ***REMOVED***Returns the Requests tuple auth for a given url from netrc.***REMOVED***

    ***REMOVED***
        from netrc import netrc, NetrcParseError

        netrc_path = None

        for f in NETRC_FILES:
            ***REMOVED***
                loc = os.path.expanduser('~/{0***REMOVED***'.format(f***REMOVED******REMOVED***
            except KeyError:
                # os.path.expanduser can fail when $HOME is undefined and
                # getpwuid fails. See http://bugs.python.org/issue20164 &
                # https://github.com/kennethreitz/requests/issues/1846
                return

            if os.path.exists(loc***REMOVED***:
                netrc_path = loc
                break

        # Abort early if there isn't one.
        if netrc_path is None:
            return

        ri = urlparse(url***REMOVED***

        # Strip port numbers from netloc. This weird `if...encode`` dance is
        # used for Python 3.2, which doesn't support unicode literals.
        splitstr = b':'
        if isinstance(url, str***REMOVED***:
            splitstr = splitstr.decode('ascii'***REMOVED***
        host = ri.netloc.split(splitstr***REMOVED***[0***REMOVED***

        ***REMOVED***
            _netrc = netrc(netrc_path***REMOVED***.authenticators(host***REMOVED***
            if _netrc:
                # Return with login / password
                login_i = (0 if _netrc[0***REMOVED*** else 1***REMOVED***
                return (_netrc[login_i***REMOVED***, _netrc[2***REMOVED******REMOVED***
        except (NetrcParseError, IOError***REMOVED***:
            # If there was a parsing error or a permissions issue reading the file,
            # we'll just skip netrc auth unless explicitly asked to raise errors.
            if raise_errors:
                raise

    # AppEngine hackiness.
    except (ImportError, AttributeError***REMOVED***:
        pass


def guess_filename(obj***REMOVED***:
    ***REMOVED***Tries to guess the filename of the given object.***REMOVED***
    name = getattr(obj, 'name', None***REMOVED***
    if (name and isinstance(name, basestring***REMOVED*** and name[0***REMOVED*** != '<' and
            name[-1***REMOVED*** != '>'***REMOVED***:
        return os.path.basename(name***REMOVED***


def from_key_val_list(value***REMOVED***:
    ***REMOVED***Take an object and test to see if it can be represented as a
    dictionary. Unless it can not be represented as such, return an
    OrderedDict, e.g.,

    ::

        >>> from_key_val_list([('key', 'val'***REMOVED******REMOVED******REMOVED***
        OrderedDict([('key', 'val'***REMOVED******REMOVED******REMOVED***
        >>> from_key_val_list('string'***REMOVED***
        ValueError: need more than 1 value to unpack
        >>> from_key_val_list({'key': 'val'***REMOVED******REMOVED***
        OrderedDict([('key', 'val'***REMOVED******REMOVED******REMOVED***

    :rtype: OrderedDict
    ***REMOVED***
    if value is None:
        return None

    if isinstance(value, (str, bytes, bool, int***REMOVED******REMOVED***:
        raise ValueError('cannot encode objects that are not 2-tuples'***REMOVED***

    return OrderedDict(value***REMOVED***


def to_key_val_list(value***REMOVED***:
    ***REMOVED***Take an object and test to see if it can be represented as a
    dictionary. If it can be, return a list of tuples, e.g.,

    ::

        >>> to_key_val_list([('key', 'val'***REMOVED******REMOVED******REMOVED***
        [('key', 'val'***REMOVED******REMOVED***
        >>> to_key_val_list({'key': 'val'***REMOVED******REMOVED***
        [('key', 'val'***REMOVED******REMOVED***
        >>> to_key_val_list('string'***REMOVED***
        ValueError: cannot encode objects that are not 2-tuples.

    :rtype: list
    ***REMOVED***
    if value is None:
        return None

    if isinstance(value, (str, bytes, bool, int***REMOVED******REMOVED***:
        raise ValueError('cannot encode objects that are not 2-tuples'***REMOVED***

    if isinstance(value, collections.Mapping***REMOVED***:
        value = value.items(***REMOVED***

    return list(value***REMOVED***


# From mitsuhiko/werkzeug (used with permission***REMOVED***.
def parse_list_header(value***REMOVED***:
    ***REMOVED***Parse lists as described by RFC 2068 Section 2.

    In particular, parse comma-separated lists where the elements of
    the list may include quoted-strings.  A quoted-string could
    contain a comma.  A non-quoted string could have quotes in the
    middle.  Quotes are removed automatically after parsing.

    It basically works like :func:`parse_set_header` just that items
    may appear multiple times and case sensitivity is preserved.

    The return value is a standard :class:`list`:

    >>> parse_list_header('token, "quoted value"'***REMOVED***
    ['token', 'quoted value'***REMOVED***

    To create a header from the :class:`list` again, use the
    :func:`dump_header` function.

    :param value: a string with a list header.
    :return: :class:`list`
    :rtype: list
    ***REMOVED***
    result = [***REMOVED***
    for item in _parse_list_header(value***REMOVED***:
        if item[:1***REMOVED*** == item[-1:***REMOVED*** == '"':
            item = unquote_header_value(item[1:-1***REMOVED******REMOVED***
        result.append(item***REMOVED***
    return result


# From mitsuhiko/werkzeug (used with permission***REMOVED***.
def parse_dict_header(value***REMOVED***:
    ***REMOVED***Parse lists of key, value pairs as described by RFC 2068 Section 2 and
    convert them into a python dict:

    >>> d = parse_dict_header('foo="is a fish", bar="as well"'***REMOVED***
    >>> type(d***REMOVED*** is dict
    True
    >>> sorted(d.items(***REMOVED******REMOVED***
    [('bar', 'as well'***REMOVED***, ('foo', 'is a fish'***REMOVED******REMOVED***

    If there is no value for a key it will be `None`:

    >>> parse_dict_header('key_without_value'***REMOVED***
***REMOVED***'key_without_value': None***REMOVED***

    To create a header from the :class:`dict` again, use the
    :func:`dump_header` function.

    :param value: a string with a dict header.
    :return: :class:`dict`
    :rtype: dict
    ***REMOVED***
    result = {***REMOVED***
    for item in _parse_list_header(value***REMOVED***:
        if '=' not in item:
            result[item***REMOVED*** = None
            continue
        name, value = item.split('=', 1***REMOVED***
        if value[:1***REMOVED*** == value[-1:***REMOVED*** == '"':
            value = unquote_header_value(value[1:-1***REMOVED******REMOVED***
        result[name***REMOVED*** = value
    return result


# From mitsuhiko/werkzeug (used with permission***REMOVED***.
def unquote_header_value(value, is_filename=False***REMOVED***:
    r***REMOVED***Unquotes a header value.  (Reversal of :func:`quote_header_value`***REMOVED***.
    This does not use the real unquoting but what browsers are actually
    using for quoting.

    :param value: the header value to unquote.
    :rtype: str
    ***REMOVED***
    if value and value[0***REMOVED*** == value[-1***REMOVED*** == '"':
        # this is not the real unquoting, but fixing this so that the
        # RFC is met will result in bugs with internet explorer and
        # probably some other browsers as well.  IE for example is
        # uploading files with "C:\foo\bar.txt" as filename
        value = value[1:-1***REMOVED***

        # if this is a filename and the starting characters look like
        # a UNC path, then just return the value without quotes.  Using the
        # replace sequence below on a UNC path has the effect of turning
        # the leading double slash into a single slash and then
        # _fix_ie_filename(***REMOVED*** doesn't work correctly.  See #458.
        if not is_filename or value[:2***REMOVED*** != '\\\\':
            return value.replace('\\\\', '\\'***REMOVED***.replace('\\"', '"'***REMOVED***
    return value


def dict_from_cookiejar(cj***REMOVED***:
    ***REMOVED***Returns a key/value dictionary from a CookieJar.

    :param cj: CookieJar object to extract cookies from.
    :rtype: dict
    ***REMOVED***

    cookie_dict = {***REMOVED***

    for cookie in cj:
        cookie_dict[cookie.name***REMOVED*** = cookie.value

    return cookie_dict


def add_dict_to_cookiejar(cj, cookie_dict***REMOVED***:
    ***REMOVED***Returns a CookieJar from a key/value dictionary.

    :param cj: CookieJar to insert cookies into.
    :param cookie_dict: Dict of key/values to insert into CookieJar.
    :rtype: CookieJar
    ***REMOVED***

    cj2 = cookiejar_from_dict(cookie_dict***REMOVED***
    cj.update(cj2***REMOVED***
    return cj


def get_encodings_from_content(content***REMOVED***:
    ***REMOVED***Returns encodings from given content string.

    :param content: bytestring to extract encodings from.
    ***REMOVED***
    warnings.warn((
        'In requests 3.0, get_encodings_from_content will be removed. For '
        'more information, please see the discussion on issue #2266. (This'
        ' warning should only appear once.***REMOVED***'***REMOVED***,
        DeprecationWarning***REMOVED***

    charset_re = re.compile(r'<meta.*?charset=["\'***REMOVED****(.+?***REMOVED***["\'>***REMOVED***', flags=re.I***REMOVED***
    pragma_re = re.compile(r'<meta.*?content=["\'***REMOVED****;?charset=(.+?***REMOVED***["\'>***REMOVED***', flags=re.I***REMOVED***
    xml_re = re.compile(r'^<\?xml.*?encoding=["\'***REMOVED****(.+?***REMOVED***["\'>***REMOVED***'***REMOVED***

    return (charset_re.findall(content***REMOVED*** +
            pragma_re.findall(content***REMOVED*** +
            xml_re.findall(content***REMOVED******REMOVED***


def get_encoding_from_headers(headers***REMOVED***:
    ***REMOVED***Returns encodings from given HTTP Header Dict.

    :param headers: dictionary to extract encoding from.
    :rtype: str
    ***REMOVED***

    content_type = headers.get('content-type'***REMOVED***

    if not content_type:
        return None

    content_type, params = cgi.parse_header(content_type***REMOVED***

    if 'charset' in params:
        return params['charset'***REMOVED***.strip("'\""***REMOVED***

    if 'text' in content_type:
        return 'ISO-8859-1'


def stream_decode_response_unicode(iterator, r***REMOVED***:
    ***REMOVED***Stream decodes a iterator.***REMOVED***

    if r.encoding is None:
        for item in iterator:
            yield item
        return

    decoder = codecs.getincrementaldecoder(r.encoding***REMOVED***(errors='replace'***REMOVED***
    for chunk in iterator:
        rv = decoder.decode(chunk***REMOVED***
        if rv:
            yield rv
    rv = decoder.decode(b'', final=True***REMOVED***
    if rv:
        yield rv


def iter_slices(string, slice_length***REMOVED***:
    ***REMOVED***Iterate over slices of a string.***REMOVED***
    pos = 0
    if slice_length is None or slice_length <= 0:
        slice_length = len(string***REMOVED***
    while pos < len(string***REMOVED***:
        yield string[pos:pos + slice_length***REMOVED***
        pos += slice_length


def get_unicode_from_response(r***REMOVED***:
    ***REMOVED***Returns the requested content back in unicode.

    :param r: Response object to get unicode content from.

    Tried:

    1. charset from content-type
    2. fall back and replace all unicode characters

    :rtype: str
    ***REMOVED***
    warnings.warn((
        'In requests 3.0, get_unicode_from_response will be removed. For '
        'more information, please see the discussion on issue #2266. (This'
        ' warning should only appear once.***REMOVED***'***REMOVED***,
        DeprecationWarning***REMOVED***

    tried_encodings = [***REMOVED***

    # Try charset from content-type
    encoding = get_encoding_from_headers(r.headers***REMOVED***

    if encoding:
        ***REMOVED***
            return str(r.content, encoding***REMOVED***
        except UnicodeError:
            tried_encodings.append(encoding***REMOVED***

    # Fall back:
    ***REMOVED***
        return str(r.content, encoding, errors='replace'***REMOVED***
    except TypeError:
        return r.content


# The unreserved URI characters (RFC 3986***REMOVED***
UNRESERVED_SET = frozenset(
    "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz"
    + "0123456789-._~"***REMOVED***


def unquote_unreserved(uri***REMOVED***:
    ***REMOVED***Un-escape any percent-escape sequences in a URI that are unreserved
    characters. This leaves all reserved, illegal and non-ASCII bytes encoded.

    :rtype: str
    ***REMOVED***
    parts = uri.split('%'***REMOVED***
    for i in range(1, len(parts***REMOVED******REMOVED***:
        h = parts[i***REMOVED***[0:2***REMOVED***
        if len(h***REMOVED*** == 2 and h.isalnum(***REMOVED***:
            ***REMOVED***
                c = chr(int(h, 16***REMOVED******REMOVED***
            except ValueError:
                raise InvalidURL("Invalid percent-escape sequence: '%s'" % h***REMOVED***

            if c in UNRESERVED_SET:
                parts[i***REMOVED*** = c + parts[i***REMOVED***[2:***REMOVED***
            else:
                parts[i***REMOVED*** = '%' + parts[i***REMOVED***
        else:
            parts[i***REMOVED*** = '%' + parts[i***REMOVED***
    return ''.join(parts***REMOVED***


def requote_uri(uri***REMOVED***:
    ***REMOVED***Re-quote the given URI.

    This function passes the given URI through an unquote/quote cycle to
    ensure that it is fully and consistently quoted.

    :rtype: str
    ***REMOVED***
    safe_with_percent = "!#$%&'(***REMOVED****+,/:;=?@[***REMOVED***~"
    safe_without_percent = "!#$&'(***REMOVED****+,/:;=?@[***REMOVED***~"
    ***REMOVED***
        # Unquote only the unreserved characters
        # Then quote only illegal characters (do not quote reserved,
        # unreserved, or '%'***REMOVED***
        return quote(unquote_unreserved(uri***REMOVED***, safe=safe_with_percent***REMOVED***
    except InvalidURL:
        # We couldn't unquote the given URI, so let's try quoting it, but
        # there may be unquoted '%'s in the URI. We need to make sure they're
        # properly quoted so they do not cause issues elsewhere.
        return quote(uri, safe=safe_without_percent***REMOVED***


def address_in_network(ip, net***REMOVED***:
    ***REMOVED***This function allows you to check if on IP belongs to a network subnet

    Example: returns True if ip = 192.168.1.1 and net = 192.168.1.0/24
             returns False if ip = 192.168.1.1 and net = 192.168.100.0/24

    :rtype: bool
    ***REMOVED***
    ipaddr = struct.unpack('=L', socket.inet_aton(ip***REMOVED******REMOVED***[0***REMOVED***
    netaddr, bits = net.split('/'***REMOVED***
    netmask = struct.unpack('=L', socket.inet_aton(dotted_netmask(int(bits***REMOVED******REMOVED******REMOVED******REMOVED***[0***REMOVED***
    network = struct.unpack('=L', socket.inet_aton(netaddr***REMOVED******REMOVED***[0***REMOVED*** & netmask
    return (ipaddr & netmask***REMOVED*** == (network & netmask***REMOVED***


def dotted_netmask(mask***REMOVED***:
    ***REMOVED***Converts mask from /xx format to xxx.xxx.xxx.xxx

    Example: if mask is 24 function returns 255.255.255.0

    :rtype: str
    ***REMOVED***
    bits = 0xffffffff ^ (1 << 32 - mask***REMOVED*** - 1
    return socket.inet_ntoa(struct.pack('>I', bits***REMOVED******REMOVED***


def is_ipv4_address(string_ip***REMOVED***:
    ***REMOVED***
    :rtype: bool
    ***REMOVED***
    ***REMOVED***
        socket.inet_aton(string_ip***REMOVED***
    except socket.error:
        return False
    return True


def is_valid_cidr(string_network***REMOVED***:
    ***REMOVED***
    Very simple check of the cidr format in no_proxy variable.

    :rtype: bool
    ***REMOVED***
    if string_network.count('/'***REMOVED*** == 1:
        ***REMOVED***
            mask = int(string_network.split('/'***REMOVED***[1***REMOVED******REMOVED***
        except ValueError:
            return False

        if mask < 1 or mask > 32:
            return False

        ***REMOVED***
            socket.inet_aton(string_network.split('/'***REMOVED***[0***REMOVED******REMOVED***
        except socket.error:
            return False
    else:
        return False
    return True


def should_bypass_proxies(url***REMOVED***:
    ***REMOVED***
    Returns whether we should bypass proxies or not.

    :rtype: bool
    ***REMOVED***
    get_proxy = lambda k: os.environ.get(k***REMOVED*** or os.environ.get(k.upper(***REMOVED******REMOVED***

    # First check whether no_proxy is defined. If it is, check that the URL
    # we're getting isn't in the no_proxy list.
    no_proxy = get_proxy('no_proxy'***REMOVED***
    netloc = urlparse(url***REMOVED***.netloc

    if no_proxy:
        # We need to check whether we match here. We need to see if we match
        # the end of the netloc, both with and without the port.
        no_proxy = (
            host for host in no_proxy.replace(' ', ''***REMOVED***.split(','***REMOVED*** if host
        ***REMOVED***

        ip = netloc.split(':'***REMOVED***[0***REMOVED***
        if is_ipv4_address(ip***REMOVED***:
            for proxy_ip in no_proxy:
                if is_valid_cidr(proxy_ip***REMOVED***:
                    if address_in_network(ip, proxy_ip***REMOVED***:
                        return True
                elif ip == proxy_ip:
                    # If no_proxy ip was defined in plain IP notation instead of cidr notation &
                    # matches the IP of the index
                    return True
        else:
            for host in no_proxy:
                if netloc.endswith(host***REMOVED*** or netloc.split(':'***REMOVED***[0***REMOVED***.endswith(host***REMOVED***:
                    # The URL does match something in no_proxy, so we don't want
                    # to apply the proxies on this URL.
                    return True

    # If the system proxy settings indicate that this URL should be bypassed,
    # don't proxy.
    # The proxy_bypass function is incredibly buggy on OS X in early versions
    # of Python 2.6, so allow this call to fail. Only catch the specific
    # exceptions we've seen, though: this call failing in other ways can reveal
    # legitimate problems.
    ***REMOVED***
        bypass = proxy_bypass(netloc***REMOVED***
    except (TypeError, socket.gaierror***REMOVED***:
        bypass = False

    if bypass:
        return True

    return False


def get_environ_proxies(url***REMOVED***:
    ***REMOVED***
    Return a dict of environment proxies.

    :rtype: dict
    ***REMOVED***
    if should_bypass_proxies(url***REMOVED***:
        return {***REMOVED***
    else:
        return getproxies(***REMOVED***


def select_proxy(url, proxies***REMOVED***:
    ***REMOVED***Select a proxy for the url, if applicable.

    :param url: The url being for the request
    :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs
    ***REMOVED***
    proxies = proxies or {***REMOVED***
    urlparts = urlparse(url***REMOVED***
    if urlparts.hostname is None:
        return proxies.get('all', proxies.get(urlparts.scheme***REMOVED******REMOVED***

    proxy_keys = [
        'all://' + urlparts.hostname,
        'all',
        urlparts.scheme + '://' + urlparts.hostname,
        urlparts.scheme,
    ***REMOVED***
    proxy = None
    for proxy_key in proxy_keys:
        if proxy_key in proxies:
            proxy = proxies[proxy_key***REMOVED***
            break

    return proxy


def default_user_agent(name="python-requests"***REMOVED***:
    ***REMOVED***
    Return a string representing the default user agent.

    :rtype: str
    ***REMOVED***
    return '%s/%s' % (name, __version__***REMOVED***


def default_headers(***REMOVED***:
    ***REMOVED***
    :rtype: requests.structures.CaseInsensitiveDict
    ***REMOVED***
    return CaseInsensitiveDict({
        'User-Agent': default_user_agent(***REMOVED***,
        'Accept-Encoding': ', '.join(('gzip', 'deflate'***REMOVED******REMOVED***,
        'Accept': '*/*',
        'Connection': 'keep-alive',
***REMOVED******REMOVED***


def parse_header_links(value***REMOVED***:
    ***REMOVED***Return a dict of parsed link headers proxies.

    i.e. Link: <http:/.../front.jpeg>; rel=front; type="image/jpeg",<http://.../back.jpeg>; rel=back;type="image/jpeg"

    :rtype: list
    ***REMOVED***

    links = [***REMOVED***

    replace_chars = ' \'"'

    for val in re.split(', *<', value***REMOVED***:
        ***REMOVED***
            url, params = val.split(';', 1***REMOVED***
        except ValueError:
            url, params = val, ''

        link = {'url': url.strip('<> \'"'***REMOVED******REMOVED***

        for param in params.split(';'***REMOVED***:
            ***REMOVED***
                key, value = param.split('='***REMOVED***
            except ValueError:
                break

            link[key.strip(replace_chars***REMOVED******REMOVED*** = value.strip(replace_chars***REMOVED***

        links.append(link***REMOVED***

    return links


# Null bytes; no need to recreate these on each call to guess_json_utf
_null = '\x00'.encode('ascii'***REMOVED***  # encoding to ASCII for Python 3
_null2 = _null * 2
_null3 = _null * 3


def guess_json_utf(data***REMOVED***:
    ***REMOVED***
    :rtype: str
    ***REMOVED***
    # JSON always starts with two ASCII characters, so detection is as
    # easy as counting the nulls and from their location and count
    # determine the encoding. Also detect a BOM, if present.
    sample = data[:4***REMOVED***
    if sample in (codecs.BOM_UTF32_LE, codecs.BOM32_BE***REMOVED***:
        return 'utf-32'     # BOM included
    if sample[:3***REMOVED*** == codecs.BOM_UTF8:
        return 'utf-8-sig'  # BOM included, MS style (discouraged***REMOVED***
    if sample[:2***REMOVED*** in (codecs.BOM_UTF16_LE, codecs.BOM_UTF16_BE***REMOVED***:
        return 'utf-16'     # BOM included
    nullcount = sample.count(_null***REMOVED***
    if nullcount == 0:
        return 'utf-8'
    if nullcount == 2:
        if sample[::2***REMOVED*** == _null2:   # 1st and 3rd are null
            return 'utf-16-be'
        if sample[1::2***REMOVED*** == _null2:  # 2nd and 4th are null
            return 'utf-16-le'
        # Did not detect 2 valid UTF-16 ascii-range characters
    if nullcount == 3:
        if sample[:3***REMOVED*** == _null3:
            return 'utf-32-be'
        if sample[1:***REMOVED*** == _null3:
            return 'utf-32-le'
        # Did not detect a valid UTF-32 ascii-range character
    return None


def prepend_scheme_if_needed(url, new_scheme***REMOVED***:
    ***REMOVED***Given a URL that may or may not have a scheme, prepend the given scheme.
    Does not replace a present scheme with the one provided as an argument.

    :rtype: str
    ***REMOVED***
    scheme, netloc, path, params, query, fragment = urlparse(url, new_scheme***REMOVED***

    # urlparse is a finicky beast, and sometimes decides that there isn't a
    # netloc present. Assume that it's being over-cautious, and switch netloc
    # and path if urlparse decided there was no netloc.
    if not netloc:
        netloc, path = path, netloc

    return urlunparse((scheme, netloc, path, params, query, fragment***REMOVED******REMOVED***


def get_auth_from_url(url***REMOVED***:
    ***REMOVED***Given a url with authentication components, extract them into a tuple of
    username,password.

    :rtype: (str,str***REMOVED***
    ***REMOVED***
    parsed = urlparse(url***REMOVED***

    ***REMOVED***
        auth = (unquote(parsed.username***REMOVED***, unquote(parsed.password***REMOVED******REMOVED***
    except (AttributeError, TypeError***REMOVED***:
        auth = ('', ''***REMOVED***

    return auth


def to_native_string(string, encoding='ascii'***REMOVED***:
    ***REMOVED***Given a string object, regardless of type, returns a representation of
    that string in the native string type, encoding and decoding where
    necessary. This assumes ASCII unless told otherwise.
    ***REMOVED***
    if isinstance(string, builtin_str***REMOVED***:
        out = string
    else:
        if is_py2:
            out = string.encode(encoding***REMOVED***
        else:
            out = string.decode(encoding***REMOVED***

    return out


# Moved outside of function to avoid recompile every call
_CLEAN_HEADER_REGEX_BYTE = re.compile(b'^\\S[^\\r\\n***REMOVED****$|^$'***REMOVED***
_CLEAN_HEADER_REGEX_STR = re.compile(r'^\S[^\r\n***REMOVED****$|^$'***REMOVED***

def check_header_validity(header***REMOVED***:
    ***REMOVED***Verifies that header value is a string which doesn't contain
    leading whitespace or return characters. This prevents unintended
    header injection.

    :param header: tuple, in the format (name, value***REMOVED***.
    ***REMOVED***
    name, value = header

    if isinstance(value, bytes***REMOVED***:
        pat = _CLEAN_HEADER_REGEX_BYTE
    else:
        pat = _CLEAN_HEADER_REGEX_STR
    ***REMOVED***
        if not pat.match(value***REMOVED***:
            raise InvalidHeader("Invalid return character or leading space in header: %s" % name***REMOVED***
    except TypeError:
        raise InvalidHeader("Header value %s must be of type str or bytes, "
                            "not %s" % (value, type(value***REMOVED******REMOVED******REMOVED***


def urldefragauth(url***REMOVED***:
    ***REMOVED***
    Given a url remove the fragment and the authentication part.

    :rtype: str
    ***REMOVED***
    scheme, netloc, path, params, query, fragment = urlparse(url***REMOVED***

    # see func:`prepend_scheme_if_needed`
    if not netloc:
        netloc, path = path, netloc

    netloc = netloc.rsplit('@', 1***REMOVED***[-1***REMOVED***

    return urlunparse((scheme, netloc, path, params, query, ''***REMOVED******REMOVED***
